<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="it" xml:lang="it">
<head>
<!-- 2025-06-07 Sat 16:49 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Statistica</title>
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" href="https://cdn.simplecss.org/simple.min.css" onerror="this.onerror=null;this.href='local.css';" />
<script>
    window.MathJax = {
      tex: {
        ams: { multlineWidth: '85%' },
        {packages: {'[+]': ['mathtools']}},
        tags: 'ams',
        tagSide: 'right',
        tagIndent: '.8em'
      },
      chtml: {
        scale: 1.0,
        displayAlign: 'center',
        displayIndent: '0em'
      },
      svg: {
        scale: 1.0,
        displayAlign: 'center',
        displayIndent: '0em'
      },
      output: {
        font: 'mathjax-modern',
        displayOverflow: 'scale'
      },
      loader: {
        load: ['[tex]/mathtools']
      },
    };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Statistica</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgd65a0a0">Leggere e rappresentare dati su Python</a>
<ul>
<li><a href="#orgc08f01b">Tipi primitivi di collezioni</a></li>
<li><a href="#org7893313">NumPy</a></li>
<li><a href="#org3b17513">Pandas</a>
<ul>
<li><a href="#org174774b">Series</a>
<ul>
<li><a href="#orgb94cb44">Creazione</a></li>
<li><a href="#org5be234e">Accesso</a>
<ul>
<li><a href="#org987eed6">Accedere ad un valore specifico</a></li>
<li><a href="#orgd396d2f">Accedere ad una sottocollezione</a></li>
</ul>
</li>
<li><a href="#orga7a5000">Proprietà</a></li>
<li><a href="#orge324c00">Visualizzazione</a></li>
<li><a href="#orgea62f5a">Categorizzare dati</a>
<ul>
<li><a href="#org240e4b9">Raccogliere valori in bins</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orga0968b7">Dataframe</a>
<ul>
<li><a href="#org448df05">Creazione</a></li>
<li><a href="#orgc9e01a9">Accesso</a>
<ul>
<li><a href="#orgba4c936">Colonne</a></li>
<li><a href="#org10d56e5">Righe</a></li>
<li><a href="#orgc48681e">Elementi specifici</a></li>
</ul>
</li>
<li><a href="#orgbb6609a">Filtri e selezione</a></li>
<li><a href="#org056ffe5">Riordinamento</a>
<ul>
<li><a href="#org38841f7">Riordina indici (righe)</a></li>
<li><a href="#org488a726">Riordina colonne</a></li>
</ul>
</li>
<li><a href="#org7f82827">Conta numero di casi e colonne</a></li>
<li><a href="#orgc050509">Ritorna valori unici di un attributo</a></li>
<li><a href="#orga5cbf69">Capisci esistenza valori nulli</a></li>
</ul>
</li>
<li><a href="#org2b9ac2f">Lettura da CSV</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgb4f68ba">Statistica descrittiva (continuare con anki e revisione da qui)</a>
<ul>
<li><a href="#org1e5d71c">Descrivere i dati</a>
<ul>
<li><a href="#org28044a0">Dati quantitativi e qualitativi</a></li>
<li><a href="#orgcd936b4">Frequenze</a>
<ul>
<li><a href="#org6c873ff">Arrotondare il numero di cifre decimali</a></li>
<li><a href="#org743852a">Mostrare frequenza relativa in percentuali</a></li>
<li><a href="#orgfa65629">Rinormalizzare su un sottoinsieme delle osservazioni</a></li>
</ul>
</li>
<li><a href="#org53b88bb">Frequenze cumulate</a>
<ul>
<li><a href="#org96ad5bf"><span class="todo TODO">TODO</span> Funzione cumulativa empirica</a></li>
<li><a href="#org3398d1a">Diagrammi di Pareto</a></li>
</ul>
</li>
<li><a href="#orgf5ed488">Frequenze congiunte e marginali</a></li>
</ul>
</li>
<li><a href="#org63ba63a">Riassumere i dati</a>
<ul>
<li><a href="#org418119f">Indici di centralità</a>
<ul>
<li><a href="#orgc0ac8a0">Media campionaria</a></li>
<li><a href="#org6c4acd7">Mediana campionaria</a></li>
<li><a href="#orgd5248d4">Moda campionaria</a></li>
</ul>
</li>
<li><a href="#orgd279798">Indici di dispersione</a>
<ul>
<li><a href="#org6a7ed95">Varianza campionaria</a>
<ul>
<li><a href="#org2dc05a8">Perchè si usa il quadrato?</a></li>
</ul>
</li>
<li><a href="#org39dcb4a">Deviazione standard</a></li>
<li><a href="#org0cbcb88"><span class="todo WAIT">WAIT</span> Perchè si divide per \(n-1\)?</a></li>
</ul>
</li>
<li><a href="#org14a9103">Quantili</a>
<ul>
<li><a href="#org0403396">Box plot</a>
<ul>
<li><a href="#org8dc1c5f"><span class="todo TODO">TODO</span> Capire cosa ha detto il professore riguardo alla relazione fra box plot e distribuzione e capire cosa c'entra la media</a></li>
</ul>
</li>
<li><a href="#org5023b89">Range inter-quartile</a></li>
<li><a href="#orgf51e9d1">QQ plot</a>
<ul>
<li><a href="#org11d9f65"><span class="todo TODO">TODO</span> Come stampare?</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgf40875c">Distribuzione normale</a></li>
<li><a href="#org8a94430">Coefficiente di correlazione campionaria</a></li>
<li><a href="#org74d6d2e"><span class="todo TODO">TODO</span> Lorenz e Gini</a></li>
</ul>
</li>
<li><a href="#org1e7948c">Grafici</a>
<ul>
<li><a href="#org0d01e63">A bastoncini</a></li>
<li><a href="#org2aa18b4">A barre</a></li>
<li><a href="#org36f7236">Poligonale</a></li>
<li><a href="#org5ad55a6">A torta (aerogramma)</a></li>
<li><a href="#orgf6fae5f">Istogramma</a></li>
<li><a href="#org538f387">Grafici multipli</a></li>
<li><a href="#org5750333">Scatter plot</a></li>
<li><a href="#org4031975">Quando usarli</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgc4f9e77"><span class="todo TODO">TODO</span> Calcolo combinatorio</a>
<ul>
<li><a href="#orge7324dc">Teorema fondamentale del calcolo combinatorio</a></li>
<li><a href="#orgd3943c9">Permutazioni</a></li>
<li><a href="#org2db997b">Disposizioni</a></li>
<li><a href="#orgc72e4b4">Combinazioni</a></li>
</ul>
</li>
<li><a href="#org88aaa01">Probabilità</a></li>
<li><a href="#org9eea659">Variabili aleatorie</a>
<ul>
<li><a href="#org9dc9c96">Definizione e concetti di base</a></li>
<li><a href="#org4a34135">Funzione di ripartizione e funzione di massa di probabilità</a></li>
<li><a href="#org30aaf5e">Funzione indicatrice</a></li>
<li><a href="#org31227f4">Valore atteso e varianza</a></li>
<li><a href="#orgc7e5d71">Variabili aleatorie multivariate</a>
<ul>
<li><a href="#orgfc3764d">Massa di probabilità congiunta e marginale</a></li>
<li><a href="#orga8e1458">Indipendenza tra variabili aleatorie</a></li>
<li><a href="#org3f57426">Valore atteso di una somma di variabili aleatorie</a></li>
<li><a href="#org18e3960">Valore atteso del prodotto di variabili aleatorie</a></li>
<li><a href="#org573876c">Stima di una variabile aleatoria</a></li>
<li><a href="#org39d0753">Covarianza: definizione e proprietà</a></li>
<li><a href="#org6549c85">Varianza di somme di variabili aleatorie</a></li>
<li><a href="#org92f0dee">Covarianza di funzioni indicatrici</a></li>
<li><a href="#orgebfc0f0">Coefficiente di correlazione</a></li>
</ul>
</li>
<li><a href="#orgfe1ac5a">Variabili aleatorie continue</a></li>
<li><a href="#org89947d1">Disuguaglianze probabilistiche fondamentali</a>
<ul>
<li><a href="#org98ebc0b">Disuguaglianza di Markov</a></li>
<li><a href="#org16eba75">Disuguaglianza di Chebyshev</a></li>
<li><a href="#org88f7a28">Legge dei grandi numeri</a></li>
</ul>
</li>
<li><a href="#org74800df">Modelli di variabili aleatorie</a>
<ul>
<li><a href="#org57a32f3">Modello di Bernoulli</a></li>
<li><a href="#org0c4b5a7">Binomiale</a></li>
<li><a href="#org1e8f7e9">Geometrico (guardare anche dispensa)</a>
<ul>
<li><a href="#orgc76f5cc">Definizione e concetti base</a></li>
<li><a href="#orgf519898">Funzione di massa di probabilità</a></li>
<li><a href="#orga4968ae">Valore atteso</a></li>
<li><a href="#org732100c">Varianza</a></li>
<li><a href="#org316685e">Funzione di ripartizione</a></li>
<li><a href="#org698e5cf">Proprietà di assenza di memoria</a></li>
<li><a href="#org2cc6ff4">Relazione con altre distribuzioni</a></li>
<li><a href="#org0942342">Applicazioni</a></li>
<li><a href="#orgff8a2ba">Python</a></li>
</ul>
</li>
<li><a href="#org7036508">Uniforme discreto</a></li>
<li><a href="#orgb2fb813">Poisson</a></li>
<li><a href="#org6b15bd9">Ipergeometrica</a></li>
<li><a href="#orge9c02b0">Continuo uniforme</a></li>
<li><a href="#org59e2742">Esponenziale</a></li>
<li><a href="#org451db8e">Gaussiano</a></li>
<li><a href="#orgf895136">Considerazioni su riproducibilità (da integrare)</a></li>
</ul>
</li>
<li><a href="#orge68f6db">Tecniche python</a>
<ul>
<li><a href="#orgcd62758">Calcola funzione di ripartizione usando massa</a></li>
<li><a href="#orga16c0a3">Inversa della funzione di ripartizione</a></li>
<li><a href="#org429311b">(AGGIUNGERE CODICE) Stabilire se una popolazione può essere descritta da una variabile aleatoria</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgb225ee8"><span class="todo TODO">TODO</span> Creare una panoramica con tutti i grafici delle distribuzioni che abbiamo guardato fatte in python</a></li>
<li><a href="#org7b7f9ea"><span class="todo TODO">TODO</span> Creare boxplot di tutte le distribuzioni in python</a></li>
<li><a href="#org4e12c02"><span class="todo TODO">TODO</span> Creare ANKI usando copilot</a></li>
<li><a href="#org98baca8"><span class="todo TODO">TODO</span> grafici interattivi (usare possibilmente all'esame)</a></li>
<li><a href="#org9e2228d"><span class="todo TODO">TODO</span> mathcal per E</a></li>
</ul>
</div>
</div>
<div id="outline-container-orgd65a0a0" class="outline-2">
<h2 id="orgd65a0a0">Leggere e rappresentare dati su Python</h2>
<div class="outline-text-2" id="text-orgd65a0a0">
</div>
<div id="outline-container-orgc08f01b" class="outline-3">
<h3 id="orgc08f01b">Tipi primitivi di collezioni</h3>
<div class="outline-text-3" id="text-orgc08f01b">
<dl class="org-dl">
<dt>tupla</dt><dd><p>
sequenza <b>immutabile</b> e <b>fixed-length</b>, delimitata da <b>parentesi tonde</b>.
</p>
<ul class="org-ul">
<li>Non può essere modificata dopo la creazione</li>
<li>Efficiente in termini di memoria e velocità</li>
<li>Supporta l'unpacking: <code>a, b, c = (1, 2, 3)</code></li>
<li>Con asterisco: <code>a, *b = (1, 2, 3, 4)</code> → <code>a  =1, b = [2, 3, 4]</code></li>
<li>Può essere concatenata: <code>(1, 2) + (3, 4) → (1, 2, 3, 4)</code></li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">hero</span> = (<span style="color: #8b2252;">'Iron Man'</span>, <span style="color: #8b2252;">'Tony Stark'</span>, 1963, 191.85)
<span style="color: #a0522d;">name</span>, <span style="color: #a0522d;">identity</span>, <span style="color: #a0522d;">year</span>, <span style="color: #a0522d;">_</span> = hero  <span style="color: #b22222;"># </span><span style="color: #b22222;">unpacking</span>
</pre>
</div></dd>
<dt>lista</dt><dd><p>
semanticamente simile alla tupla, ma <b>mutabile</b>.
</p>
<ul class="org-ul">
<li>Modificabile con metodi come <code>append()</code>, <code>insert()</code>, <code>pop()</code>, <code>remove()</code></li>
<li>Supporta lo slicing: <code>lista[1:4], lista[-3:]</code></li>
<li>Può contenere elementi eterogenei</li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">heroes</span> = [<span style="color: #8b2252;">'Iron Man'</span>, <span style="color: #8b2252;">'Thor'</span>, <span style="color: #8b2252;">'Captain America'</span>]
heroes.append(<span style="color: #8b2252;">'Hulk'</span>)  <span style="color: #b22222;"># </span><span style="color: #b22222;">Aggiunge elemento</span>
heroes.pop(1)  <span style="color: #b22222;"># </span><span style="color: #b22222;">Rimuove Thor</span>
</pre>
</div></dd>
<dt>dizionario</dt><dd>memorizza associazioni <b>chiave-valore</b> ed è delimitato da <b>parentesi graffe</b>.
<ul class="org-ul">
<li>Accesso per chiave in tempo costante</li>
<li>Chiavi immutabili (stringhe, numeri, tuple)</li>
<li>Valori di qualsiasi tipo</li>
</ul></dd>
</dl>
</div>
</div>
<div id="outline-container-org7893313" class="outline-3">
<h3 id="org7893313">NumPy</h3>
<div class="outline-text-3" id="text-org7893313">
<p>
Array NumPy: Strutture multidimensionali e omogenee che offrono performance superiori per calcoli numerici.
</p>

<p>
Vantaggi:
</p>
<dl class="org-dl">
<dt>Operazioni vettorizzate (elemento per elemento)</dt><dd><code>array * 2</code>, <code>array1 + array2</code></dd>
<dt>Funzioni matematiche avanzate</dt><dd><code>np.sin()</code>, <code>np.mean()</code>, etc.</dd>
<dt>(no term)</dt><dd>Integrazione con matplotlib per visualizzazioni</dd>
</dl>

<p>
Uno svantaggio è che quando si creano grafici con NumPy, i valori di due array vengono accoppiati in base alla posizione/indice, non per valore logico.
</p>
</div>
</div>
<div id="outline-container-org3b17513" class="outline-3">
<h3 id="org3b17513">Pandas</h3>
<div class="outline-text-3" id="text-org3b17513">
<p>
La libreria <b>Pandas</b> fornisce due strutture dati fondamentali, <b>Series</b> e <b>DataFrame</b>.
</p>
</div>
<div id="outline-container-org174774b" class="outline-4">
<h4 id="org174774b">Series</h4>
<div class="outline-text-4" id="text-org174774b">
<p>
<b>Series</b> è un oggetto che associa un array mono-dimensionale di valori a un array di indici.
</p>

<p>
Caratteristiche:
</p>
<ul class="org-ul">
<li>Valori omogenei (stesso tipo)</li>
<li>Indici personalizzabili</li>
<li>Gestione automatica dei valori mancanti (NaN)</li>
<li>Comportamento ibrido tra array NumPy e dizionario</li>
</ul>


<p>
I due array sono accessibili mediante i metodi <code>.array</code>, che ritorna un <code>PandasArray</code>, tipicamente un wrapper di un array di NumPy, e <code>.index</code>.
</p>

<p>
Quando si disegna il grafico di due serie, i valori vengono associati in base all'indice.
</p>
</div>
<div id="outline-container-orgb94cb44" class="outline-5">
<h5 id="orgb94cb44">Creazione</h5>
<div class="outline-text-5" id="text-orgb94cb44">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> pandas <span style="color: #a020f0;">as</span> pd

<span style="color: #b22222;"># </span><span style="color: #b22222;">Con indici espliciti</span>
<span style="color: #a0522d;">first_appearance</span> = pd.Series([1963, 1962, 1941], 
                             index=[<span style="color: #8b2252;">'Iron Man'</span>, <span style="color: #8b2252;">'Hulk'</span>, <span style="color: #8b2252;">'Captain America'</span>])

<span style="color: #b22222;"># </span><span style="color: #b22222;">Da dizionario (le chiavi diventano indici)</span>
<span style="color: #a0522d;">heights</span> = pd.Series({<span style="color: #8b2252;">'Iron Man'</span>: 198.51, <span style="color: #8b2252;">'Hulk'</span>: 244.0, <span style="color: #8b2252;">'Thor'</span>: 198.0})
</pre>
</div>
</div>
</div>
<div id="outline-container-org5be234e" class="outline-5">
<h5 id="org5be234e">Accesso</h5>
<div class="outline-text-5" id="text-org5be234e">
</div>
<div id="outline-container-org987eed6" class="outline-6">
<h6 id="org987eed6">Accedere ad un valore specifico</h6>
<div class="outline-text-6" id="text-org987eed6">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">Per indice</span>
first_appearance[<span style="color: #8b2252;">'Iron Man'</span>]        <span style="color: #b22222;"># </span><span style="color: #b22222;">1963</span>
first_appearance.loc[<span style="color: #8b2252;">'Iron Man'</span>]    <span style="color: #b22222;"># </span><span style="color: #b22222;">1963 (pi&#249; esplicito)</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Per posizione</span>
first_appearance.iloc[0]            <span style="color: #b22222;"># </span><span style="color: #b22222;">1963 (primo elemento)</span>
</pre>
</div>
</div>
</div>
<div id="outline-container-orgd396d2f" class="outline-6">
<h6 id="orgd396d2f">Accedere ad una sottocollezione</h6>
<div class="outline-text-6" id="text-orgd396d2f">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">Slicing per indice (inclusivo dell'ultimo)</span>
first_appearance.loc[<span style="color: #8b2252;">'Hulk'</span>:<span style="color: #8b2252;">'Thor'</span>]

<span style="color: #b22222;"># </span><span style="color: #b22222;">Slicing per posizione (esclusivo dell'ultimo)</span>
first_appearance.iloc[0:2]  <span style="color: #b22222;"># </span><span style="color: #b22222;">primi due elementi</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Con lista di booleani (list comprehension)</span>
first_appearance[first_appearance &gt; 1960]

<span style="color: #b22222;"># </span><span style="color: #b22222;">Con espressione</span>
first_appearance[[1960 &lt;= y &lt; 1970 <span style="color: #a020f0;">for</span> y <span style="color: #a020f0;">in</span> first_appearance]]
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-orga7a5000" class="outline-5">
<h5 id="orga7a5000">Proprietà</h5>
<div class="outline-text-5" id="text-orga7a5000">
<div class="org-src-container">
<pre class="src src-python">first_appearance.array  <span style="color: #b22222;"># </span><span style="color: #b22222;">Accede all'array dei valori (PandasArray)</span>
first_appearance.index  <span style="color: #b22222;"># </span><span style="color: #b22222;">Accede all'array degli indici</span>
</pre>
</div>
</div>
</div>
<div id="outline-container-orge324c00" class="outline-5">
<h5 id="orge324c00">Visualizzazione</h5>
<div class="outline-text-5" id="text-orge324c00">
<div class="org-src-container">
<pre class="src src-python">first_appearance.plot.bar()  <span style="color: #b22222;"># </span><span style="color: #b22222;">Crea un grafico a barre</span>
<span style="color: #b22222;"># </span><span style="color: #b22222;">I valori vengono accoppiati per indice, non per posizione</span>
</pre>
</div>
</div>
</div>
<div id="outline-container-orgea62f5a" class="outline-5">
<h5 id="orgea62f5a">Categorizzare dati</h5>
<div class="outline-text-5" id="text-orgea62f5a">
</div>
<div id="outline-container-org240e4b9" class="outline-6">
<h6 id="org240e4b9">Raccogliere valori in bins</h6>
<div class="outline-text-6" id="text-org240e4b9">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">Esempio di categorizzazione delle altezze in bins</span>
<span style="color: #a0522d;">height_categories</span> = pd.cut(heroes_df[<span style="color: #8b2252;">'height'</span>], 
                          bins=[0, 170, 190, 250],
                          labels=[<span style="color: #8b2252;">'Basso'</span>, <span style="color: #8b2252;">'Medio'</span>, <span style="color: #8b2252;">'Alto'</span>],
                          right=<span style="color: #008b8b;">True</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo delle frequenze per categoria</span>
<span style="color: #a0522d;">frequency_table</span> = pd.crosstab(index=height_categories, columns=<span style="color: #8b2252;">'count'</span>)
</pre>
</div>
</div>
</div>
</div>
</div>
<div id="outline-container-orga0968b7" class="outline-4">
<h4 id="orga0968b7">Dataframe</h4>
<div class="outline-text-4" id="text-orga0968b7">
<p>
Un dataframe è una <b>tabella rettangolare</b> contenente una <b>collezione di Series</b>, con uno specifico nome e con lo stesso indice.
</p>

<p>
Può essere quindi pensata come un <b>dizionario di serie eterogenee</b>.
</p>

<p>
Gestisce automaticamente i dati mancanti (come?)
</p>

<p>
Per <b>accedere ad una colonna</b> si utilizza la sintassi dei dizionari, con il nome della colonna, es. <code>col = df['nome colonna']</code>. A più colonne con <code>rescol = df[['A', 'C']]</code>.
</p>

<p>
Per <b>accedere ad una riga</b> si utilizza <code>loc[]</code> e <code>iloc[]</code>.
</p>

<p>
Per <b>accedere ad un elemento</b> si usa <code>at[index, column]</code>.
</p>
</div>
<div id="outline-container-org448df05" class="outline-5">
<h5 id="org448df05">Creazione</h5>
<div class="outline-text-5" id="text-org448df05">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">Da dizionario di liste/array</span>
<span style="color: #a0522d;">heroes_df</span> = pd.DataFrame({
    <span style="color: #8b2252;">'name'</span>: [<span style="color: #8b2252;">'Iron Man'</span>, <span style="color: #8b2252;">'Hulk'</span>, <span style="color: #8b2252;">'Thor'</span>],
    <span style="color: #8b2252;">'height'</span>: [198.51, 244.0, 198.0],
    <span style="color: #8b2252;">'first_appearance'</span>: [1963, 1962, 1963]
})

<span style="color: #b22222;"># </span><span style="color: #b22222;">Da lista di dizionari</span>
<span style="color: #a0522d;">heroes_df</span> = pd.DataFrame([
    {<span style="color: #8b2252;">'name'</span>: <span style="color: #8b2252;">'Iron Man'</span>, <span style="color: #8b2252;">'height'</span>: 198.51, <span style="color: #8b2252;">'first_appearance'</span>: 1963},
    {<span style="color: #8b2252;">'name'</span>: <span style="color: #8b2252;">'Hulk'</span>, <span style="color: #8b2252;">'height'</span>: 244.0, <span style="color: #8b2252;">'first_appearance'</span>: 1962},
    {<span style="color: #8b2252;">'name'</span>: <span style="color: #8b2252;">'Thor'</span>, <span style="color: #8b2252;">'height'</span>: 198.0, <span style="color: #8b2252;">'first_appearance'</span>: 1963}
])
</pre>
</div>
</div>
</div>
<div id="outline-container-orgc9e01a9" class="outline-5">
<h5 id="orgc9e01a9">Accesso</h5>
<div class="outline-text-5" id="text-orgc9e01a9">
</div>
<div id="outline-container-orgba4c936" class="outline-6">
<h6 id="orgba4c936">Colonne</h6>
<div class="outline-text-6" id="text-orgba4c936">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">Singola colonna (restituisce Series)</span>
<span style="color: #a0522d;">heights</span> = heroes_df[<span style="color: #8b2252;">'height'</span>]

<span style="color: #b22222;"># </span><span style="color: #b22222;">Multiple colonne (restituisce DataFrame)</span>
<span style="color: #a0522d;">subset</span> = heroes_df[[<span style="color: #8b2252;">'name'</span>, <span style="color: #8b2252;">'height'</span>]]
</pre>
</div>
</div>
</div>
<div id="outline-container-org10d56e5" class="outline-6">
<h6 id="org10d56e5">Righe</h6>
<div class="outline-text-6" id="text-org10d56e5">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">Per indice</span>
heroes_df.loc[0]  <span style="color: #b22222;"># </span><span style="color: #b22222;">prima riga</span>
heroes_df.loc[<span style="color: #8b2252;">'Iron Man'</span>]  <span style="color: #b22222;"># </span><span style="color: #b22222;">se l'indice &#232; 'Iron Man'</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Per posizione</span>
heroes_df.iloc[1]  <span style="color: #b22222;"># </span><span style="color: #b22222;">seconda riga</span>
heroes_df.iloc[1:3]  <span style="color: #b22222;"># </span><span style="color: #b22222;">seconda e terza riga</span>
</pre>
</div>
</div>
</div>
<div id="outline-container-orgc48681e" class="outline-6">
<h6 id="orgc48681e">Elementi specifici</h6>
<div class="outline-text-6" id="text-orgc48681e">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">Per indice</span>
heroes_df.loc[0]  <span style="color: #b22222;"># </span><span style="color: #b22222;">prima riga</span>
heroes_df.loc[<span style="color: #8b2252;">'Iron Man'</span>]  <span style="color: #b22222;"># </span><span style="color: #b22222;">se l'indice &#232; 'Iron Man'</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Per posizione</span>
heroes_df.iloc[1]  <span style="color: #b22222;"># </span><span style="color: #b22222;">seconda riga</span>
heroes_df.iloc[1:3]  <span style="color: #b22222;"># </span><span style="color: #b22222;">seconda e terza riga</span>
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-orgbb6609a" class="outline-5">
<h5 id="orgbb6609a">Filtri e selezione</h5>
<div class="outline-text-5" id="text-orgbb6609a">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">Filtro con condizioni</span>
<span style="color: #a0522d;">tall_heroes</span> = heroes_df[heroes_df[<span style="color: #8b2252;">'height'</span>] &gt; 190]

<span style="color: #b22222;"># </span><span style="color: #b22222;">Filtri multipli</span>
<span style="color: #a0522d;">strong_tall</span> = heroes_df[(heroes_df[<span style="color: #8b2252;">'height'</span>] &gt; 190) &amp; 
                         (heroes_df[<span style="color: #8b2252;">'strength'</span>] &gt; 50)]

<span style="color: #b22222;"># </span><span style="color: #b22222;">Metodi di selezione</span>
heroes_df.query(<span style="color: #8b2252;">'height &gt; 190 and strength &gt; 50'</span>)
</pre>
</div>
</div>
</div>
<div id="outline-container-org056ffe5" class="outline-5">
<h5 id="org056ffe5">Riordinamento</h5>
<div class="outline-text-5" id="text-org056ffe5">
</div>
<div id="outline-container-org38841f7" class="outline-6">
<h6 id="org38841f7">Riordina indici (righe)</h6>
<div class="outline-text-6" id="text-org38841f7">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">Riordinare gli indici di un DataFrame</span>
<span style="color: #a0522d;">df_reordered</span> = dataframe.reindex([<span style="color: #8b2252;">'indice3'</span>, <span style="color: #8b2252;">'indice1'</span>, <span style="color: #8b2252;">'indice2'</span>])
<span style="color: #b22222;"># </span><span style="color: #b22222;">Specificare valori per righe mancanti</span>
<span style="color: #a0522d;">df_reordered</span> = dataframe.reindex([<span style="color: #8b2252;">'indice3'</span>, <span style="color: #8b2252;">'indice1'</span>, <span style="color: #8b2252;">'nuovo'</span>], fill_value=0)
</pre>
</div>
</div>
</div>
<div id="outline-container-org488a726" class="outline-6">
<h6 id="org488a726">Riordina colonne</h6>
<div class="outline-text-6" id="text-org488a726">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">Modo pi&#249; diretto per riordinare le colonne</span>
<span style="color: #a0522d;">df_reordered</span> = dataframe[[<span style="color: #8b2252;">'colonna3'</span>, <span style="color: #8b2252;">'colonna1'</span>, <span style="color: #8b2252;">'colonna2'</span>]]

<span style="color: #b22222;"># </span><span style="color: #b22222;">Esempio concreto</span>
<span style="color: #a0522d;">columns_order</span> = [<span style="color: #8b2252;">'name'</span>, <span style="color: #8b2252;">'strength'</span>, <span style="color: #8b2252;">'first_appearance'</span>]
<span style="color: #a0522d;">heroes_reordered</span> = heroes_df[columns_order]

<span style="color: #b22222;"># </span><span style="color: #b22222;">Metodo che usa loc inutilmente</span>
dataframe.loc[:,listaDiColonneRiordinate]
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-org7f82827" class="outline-5">
<h5 id="org7f82827">Conta numero di casi e colonne</h5>
<div class="outline-text-5" id="text-org7f82827">
<p>
<code>df.shape</code>, <code>[0]</code> righe, <code>[1]</code> colonne
</p>
</div>
</div>
<div id="outline-container-orgc050509" class="outline-5">
<h5 id="orgc050509">Ritorna valori unici di un attributo</h5>
<div class="outline-text-5" id="text-orgc050509">
<p>
<code>df['NomeColonna'].unique()</code>
</p>
</div>
</div>
<div id="outline-container-orga5cbf69" class="outline-5">
<h5 id="orga5cbf69">Capisci esistenza valori nulli</h5>
<div class="outline-text-5" id="text-orga5cbf69">
<p>
<code>pd.isnull(acqua['Oro']).any()</code> che resituisce True se esistono dei valori nulli
<code>pd.isnull(acqua['Oro']).sum()</code> che restituisce una serie con 1 al posto dei null, e poi ne conta le occorrenze
</p>
</div>
</div>
</div>
<div id="outline-container-org2b9ac2f" class="outline-4">
<h4 id="org2b9ac2f">Lettura da CSV</h4>
<div class="outline-text-4" id="text-org2b9ac2f">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">Lettura di base</span>
<span style="color: #a0522d;">df</span> = pd.read_csv(<span style="color: #8b2252;">'file.csv'</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Con opzioni personalizzate</span>
<span style="color: #a0522d;">heroes</span> = pd.read_csv(<span style="color: #8b2252;">'heroes.csv'</span>, 
                     delimiter=<span style="color: #8b2252;">';'</span>,  <span style="color: #b22222;"># </span><span style="color: #b22222;">Separatore personalizzato</span>
                     quotechar=<span style="color: #8b2252;">'"'</span>,  <span style="color: #b22222;"># </span><span style="color: #b22222;">Carattere di quotazione</span>
                     na_values=[<span style="color: #8b2252;">'NA'</span>, <span style="color: #8b2252;">''</span>],  <span style="color: #b22222;"># </span><span style="color: #b22222;">Valori da considerare come NA</span>
                     index_col=<span style="color: #8b2252;">'name'</span>)  <span style="color: #b22222;"># </span><span style="color: #b22222;">Colonna da usare come indice</span>
</pre>
</div>
</div>
</div>
</div>
</div>
<div id="outline-container-orgb4f68ba" class="outline-2">
<h2 id="orgb4f68ba">Statistica descrittiva (continuare con anki e revisione da qui)</h2>
<div class="outline-text-2" id="text-orgb4f68ba">
</div>
<div id="outline-container-org1e5d71c" class="outline-3">
<h3 id="org1e5d71c">Descrivere i dati</h3>
<div class="outline-text-3" id="text-org1e5d71c">
</div>
<div id="outline-container-org28044a0" class="outline-4">
<h4 id="org28044a0">Dati quantitativi e qualitativi</h4>
<div class="outline-text-4" id="text-org28044a0">
<dl class="org-dl">
<dt>Dati quantitativi</dt><dd>Misurati attraverso quantità numeriche
<ul class="org-ul">
<li><b><b>Discreti</b></b>: Ha senso considerare singoli valori specifici (es. anno di prima apparizione)</li>
<li><b><b>Continui</b></b>: Ha senso considerare intervalli di valori (es. altezza, peso)</li>
</ul></dd>

<dt>Dati qualitativi (categorici/nominali)</dt><dd>Misurati attraverso etichette selezionate da un insieme predefinito
<ul class="org-ul">
<li><b><b>Binari/booleani</b></b>: Due sole possibili etichette non confrontabili (es. Gender: M/F)</li>
<li><b><b>Nominali</b></b> (sconnessi): Multiple etichette non confrontabili tra loro (es. Name, Eye color)
<ul class="org-ul">
<li>È possibile solo stabilire relazioni di equivalenza (uguale/diverso)</li>
</ul></li>
<li><b><b>Ordinali</b></b>: Etichette tra cui è possibile stabilire una relazione d'ordine (es. Intelligence)</li>
</ul></dd>
</dl>

<p>
Certi dati temporali come gli anni sono formalmente numerici ma possono essere considerati qualitativi quando le operazioni aritmetiche perdono significato.
</p>

<p>
Nei dataset reali, va inoltre considerato che anche i dati continui vengono discretizzati quando memorizzati su computer, rendendo la distinzione talvolta sfumata.
</p>
</div>
</div>
<div id="outline-container-orgcd936b4" class="outline-4">
<h4 id="orgcd936b4">Frequenze</h4>
<div class="outline-text-4" id="text-orgcd936b4">
<p>
La <b>frequenza assoluta</b> è il conteggio del numero di volte che una data osservazione occorre in un campione.
</p>

<p>
La <b>frequenza relativa</b> è la frequenza del valore di un dato divisa per il numero totale di elementi in un insieme di dati, ovvero la frazione dei casi in cui quell'osservazione occorre.
</p>

<p>
In <code>pandas</code> si ottengono con il metodo <code>crosstab</code>, il cui parametro <code>normalize</code> è <code>True</code> per le frequenze relative, <code>False</code> altrimenti.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">tab_freq</span> = pd.crosstab(index = serie,
                       columns = <span style="color: #8b2252;">'Abs. frequence'</span>,
                       colnames=[<span style="color: #8b2252;">''</span>],
                       normalize = <span style="color: #008b8b;">True</span>)
</pre>
</div>
</div>
<div id="outline-container-org6c873ff" class="outline-5">
<h5 id="org6c873ff">Arrotondare il numero di cifre decimali</h5>
<div class="outline-text-5" id="text-org6c873ff">
<div class="org-src-container">
<pre class="src src-python">publisher_rel_freq.<span style="color: #483d8b;">apply</span>(<span style="color: #a020f0;">lambda</span> p: 100 * np.<span style="color: #483d8b;">round</span>(p, 3))
</pre>
</div>
</div>
</div>
<div id="outline-container-org743852a" class="outline-5">
<h5 id="org743852a">Mostrare frequenza relativa in percentuali</h5>
<div class="outline-text-5" id="text-org743852a">
<div class="org-src-container">
<pre class="src src-python">(publisher_rel_freq.<span style="color: #483d8b;">apply</span>(<span style="color: #a020f0;">lambda</span> p: np.<span style="color: #483d8b;">round</span>(100*p, 2))
                   .astype(<span style="color: #483d8b;">str</span>)
                   .<span style="color: #483d8b;">apply</span>(<span style="color: #a020f0;">lambda</span> s: s + <span style="color: #8b2252;">'%'</span>))
</pre>
</div>
</div>
</div>
<div id="outline-container-orgfa65629" class="outline-5">
<h5 id="orgfa65629">Rinormalizzare su un sottoinsieme delle osservazioni</h5>
<div class="outline-text-5" id="text-orgfa65629">
<p>
Quando si filtrano alcune osservazioni (ad esempio quelle con un valore inferiore ad un certo valore), la somma di tutte le frequenze relative non sarà più \(1\).
</p>

<p>
Per rinormalizzare le frequenze relative bisogna calcolare la somma delle osservazioni in considerazione e poi dividere ogni frequenza relativa per quel numero.
</p>

<p>
Il modo safe (utilizzando <code>.copy()</code> come best-practice per evitare ambiguità di modifica) su <b>dataframe</b> è il seguente:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">Filter the dataframe</span>
<span style="color: #a0522d;">filtered_df</span> = df[df[<span style="color: #8b2252;">'Rel. frequence'</span>] &gt; 0.04].copy()
<span style="color: #b22222;"># </span><span style="color: #b22222;">Renormalize just the 'Rel. frequence' column</span>
<span style="color: #a0522d;">filtered_df</span>[<span style="color: #8b2252;">'Rel. frequence'</span>] = filtered_df[<span style="color: #8b2252;">'Rel. frequence'</span>] / filtered_df[<span style="color: #8b2252;">'Rel. frequence'</span>].<span style="color: #483d8b;">sum</span>()
</pre>
</div>

<p>
Se invece si lavora su una <b>serie</b> (situazione solita), basta usare la notazione mediante un singolo operatore per applicare la trasformazione a tutti i valori della serie:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">Restringi serie</span>
<span style="color: #a0522d;">serie</span> = rel_freq[rel_freq &gt; 0.04]
<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcola somma serie ristretta</span>
<span style="color: #a0522d;">tsum</span> = serie.<span style="color: #483d8b;">sum</span>()
<span style="color: #b22222;"># </span><span style="color: #b22222;">Applica trasformazione con sintassi breve</span>
<span style="color: #a0522d;">serie_renormalizzata1</span> = serie/tsum
<span style="color: #b22222;"># </span><span style="color: #b22222;">Oppure applica trasformazione con apply, che &#232; inutile</span>
<span style="color: #b22222;">#</span><span style="color: #b22222;">serie_renormalizzata2 = serie.apply(lambda p: p/tsum)</span>
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-org53b88bb" class="outline-4">
<h4 id="org53b88bb">Frequenze cumulate</h4>
<div class="outline-text-4" id="text-org53b88bb">
<p>
La frequenza cumulata (assoluta o relativa), associata ad una modalità su cui è possibile definire una relazione d'ordine, è pari alla somma della sua frequenza (assoluta o relativa) e di quelle delle modalità che la precedono.
</p>

<p>
In <code>pandas</code> si ottiene con il metodo <code>.cumsum()</code> su un <code>dataframe</code>.
</p>
</div>
<div id="outline-container-org96ad5bf" class="outline-5">
<h5 id="org96ad5bf"><span class="todo TODO">TODO</span> Funzione cumulativa empirica</h5>
<div class="outline-text-5" id="text-org96ad5bf">
<p>
Una funzione che restituisce la proporzione di dati sotto un certo valore, aumentando di \(frac{1}{n}\) ad ogni osservazione di una certa modalità.
inserisci formula e fai anki
</p>
</div>
</div>
<div id="outline-container-org3398d1a" class="outline-5">
<h5 id="org3398d1a">Diagrammi di Pareto</h5>
<div class="outline-text-5" id="text-org3398d1a">
<p>
Il diagramma di Pareto di un campione unisce il grafico <b>a barre</b> delle <b>frequenze relative</b> ed il grafico <b>poligonale</b> delle <b>frequenze cumulate relative</b>.
</p>

<p>
Per generare il diagramma a partire da una serie, considerando solo i dati superiori ad un certo valore e scegliendo se renormalizzare o meno, si fa così:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">def</span> <span style="color: #0000ff;">my_pareto</span>(data, threshold=0.02, renormalize=<span style="color: #008b8b;">False</span>):
    <span style="color: #a0522d;">freq</span> = data.value_counts(normalize=<span style="color: #008b8b;">True</span>)
    <span style="color: #a0522d;">freq</span> = freq[freq &gt; threshold]
    <span style="color: #a020f0;">if</span> renormalize:
        <span style="color: #a0522d;">freq</span> = freq / <span style="color: #483d8b;">sum</span>(freq)
    freq.cumsum().plot()
    freq.plot.bar()

my_pareto(heroes[<span style="color: #8b2252;">'Eye color'</span>], threshold=0)
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-orgf5ed488" class="outline-4">
<h4 id="orgf5ed488">Frequenze congiunte e marginali</h4>
<div class="outline-text-4" id="text-orgf5ed488">
<p>
Le <b>frequenze congiunte*</b> contano il numero di osservazioni in cui due caratteri (variabili) assumono simultaneamente determinati valori. Rappresentano la distribuzione bidimensionale di due variabili.
</p>

<p>
Vengono stampate nelle <b>tabelle di contingenza</b>, in cui le righe sono una variabile, le colonne l'altra e i valori sono le frequenze congiunte.
</p>

<p>
Le <b>frequenze marginali</b> sono i totali di riga e colonna in una tabella di contingenza e rappresentano le frequenze di ciascuna variabile considerata singolarmente.
</p>

<p>
Le tabelle di contingenza relative possono essere <b>normalizzate</b> nei seguenti modi:
</p>
<dl class="org-dl">
<dt>Normalizzazione totale ('all')</dt><dd>Ogni cella rappresenta la proporzione rispetto al totale generale delle osservazioni. Tutte le celle sommate danno 1.</dd>
<dt>Normalizzazione per riga ('index')</dt><dd>Ogni cella rappresenta la proporzione rispetto al totale della riga. Ogni riga somma a 1, mostrando la distribuzione della variabile colonna all'interno di ciascun valore della variabile riga.</dd>
<dt>Normalizzazione per colonna ('columns')</dt><dd>Ogni cella rappresenta la proporzione rispetto al totale della colonna. Ogni colonna somma a 1, mostrando la distribuzione della variabile riga all'interno di ciascun valore della variabile colonna.</dd>
</dl>
<p>
Se \(x\) è sulle righe e \(y\) sulle colonne, si normalizza in base ad \(x\) quando si vogliono ottenere le frequenze relative dei valori di \(y\) per gli specifici valori di \(x\). 
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">Creazione di una tabella di frequenze congiunte tra intelligenza e genere</span>
<span style="color: #a0522d;">int_gender_freq</span> = pd.crosstab(index=heroes[<span style="color: #8b2252;">'Intelligence'</span>], 
                              columns=heroes[<span style="color: #8b2252;">'Gender'</span>])

<span style="color: #b22222;"># </span><span style="color: #b22222;">Riordinare le righe secondo un ordine logico anzich&#233; alfabetico</span>
<span style="color: #a0522d;">int_gender_freq</span> = int_gender_freq.reindex([<span style="color: #8b2252;">'low'</span>, <span style="color: #8b2252;">'moderate'</span>,
                                           <span style="color: #8b2252;">'average'</span>, <span style="color: #8b2252;">'good'</span>, <span style="color: #8b2252;">'high'</span>])

<span style="color: #b22222;"># </span><span style="color: #b22222;">Riordinare le colonne specificando l'ordine desiderato</span>
int_gender_freq.loc[:,[<span style="color: #8b2252;">'M'</span>, <span style="color: #8b2252;">'F'</span>]]

<span style="color: #b22222;"># </span><span style="color: #b22222;">Selezionare solo alcune righe della tabella</span>
<span style="color: #a0522d;">subset_freq</span> = int_gender_freq.loc[<span style="color: #8b2252;">'moderate'</span>:<span style="color: #8b2252;">'good'</span>, :]

<span style="color: #b22222;"># </span><span style="color: #b22222;">Visualizzazione grafica con barre affiancate</span>
int_gender_freq.plot.bar(color=[<span style="color: #8b2252;">'pink'</span>, <span style="color: #8b2252;">'blue'</span>])
plt.show()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Visualizzazione grafica con barre impilate</span>
int_gender_freq.plot.bar(color=[<span style="color: #8b2252;">'pink'</span>, <span style="color: #8b2252;">'blue'</span>], stacked=<span style="color: #008b8b;">True</span>)
plt.show()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Gestione di variabili quantitative mediante discretizzazione</span>
<span style="color: #b22222;"># </span><span style="color: #b22222;">Converte i pesi numerici in intervalli categorici</span>
<span style="color: #a0522d;">weight_categories</span> = pd.cut(heroes[<span style="color: #8b2252;">'Weight'</span>], 
                          bins=[30, 50, 80, 100, 200, 500, 1000])

<span style="color: #b22222;"># </span><span style="color: #b22222;">Creazione di tabella di contingenza con la variabile discretizzata</span>
<span style="color: #a0522d;">weight_gender_freq</span> = pd.crosstab(index=weight_categories,
                                columns=[heroes[<span style="color: #8b2252;">'Gender'</span>]])

<span style="color: #b22222;"># </span><span style="color: #b22222;">Modifica della direzione degli intervalli (aperti a sinistra)</span>
<span style="color: #a0522d;">weight_gender_freq_left</span> = pd.crosstab(index=pd.cut(heroes[<span style="color: #8b2252;">'Weight'</span>],
                                                 bins=[30, 50, 80, 100, 200, 500, 1000],
                                                 right=<span style="color: #008b8b;">False</span>),
                                     columns=[heroes[<span style="color: #8b2252;">'Gender'</span>]])

<span style="color: #b22222;"># </span><span style="color: #b22222;">Aggiunta delle frequenze marginali (totali di riga e colonna)</span>
<span style="color: #a0522d;">freq_with_margins</span> = pd.crosstab(index=heroes[<span style="color: #8b2252;">'Intelligence'</span>], 
                               columns=heroes[<span style="color: #8b2252;">'Gender'</span>], 
                               margins=<span style="color: #008b8b;">True</span>)  <span style="color: #b22222;"># </span><span style="color: #b22222;">Aggiunge riga e colonna 'All'</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo delle frequenze relative normalizzate sul totale</span>
<span style="color: #a0522d;">rel_freq_all</span> = pd.crosstab(index=heroes[<span style="color: #8b2252;">'Intelligence'</span>], 
                          columns=heroes[<span style="color: #8b2252;">'Gender'</span>],
                          margins=<span style="color: #008b8b;">True</span>,
                          normalize=<span style="color: #8b2252;">'all'</span>)  <span style="color: #b22222;"># </span><span style="color: #b22222;">Ogni cella divisa per il numero totale</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo delle frequenze relative normalizzate per riga</span>
<span style="color: #a0522d;">rel_freq_row</span> = pd.crosstab(index=heroes[<span style="color: #8b2252;">'Intelligence'</span>], 
                          columns=heroes[<span style="color: #8b2252;">'Gender'</span>],
                          margins=<span style="color: #008b8b;">True</span>,
                          normalize=<span style="color: #8b2252;">'index'</span>)  <span style="color: #b22222;"># </span><span style="color: #b22222;">Ogni riga somma a 1</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo delle frequenze relative normalizzate per colonna</span>
<span style="color: #a0522d;">rel_freq_col</span> = pd.crosstab(index=heroes[<span style="color: #8b2252;">'Intelligence'</span>], 
                          columns=heroes[<span style="color: #8b2252;">'Gender'</span>],
                          margins=<span style="color: #008b8b;">True</span>,
                          normalize=<span style="color: #8b2252;">'columns'</span>)  <span style="color: #b22222;"># </span><span style="color: #b22222;">Ogni colonna somma a 1</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Visualizzazione delle frequenze normalizzate per colonna con grafico a barre</span>
pd.crosstab(index=heroes[<span style="color: #8b2252;">'Strength'</span>],
           columns=[heroes[<span style="color: #8b2252;">'Gender'</span>]],
           normalize=<span style="color: #8b2252;">'columns'</span>).plot.bar(color=[<span style="color: #8b2252;">'pink'</span>, <span style="color: #8b2252;">'blue'</span>],
                                        stacked=<span style="color: #008b8b;">False</span>)
plt.show()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Creazione di un diagramma di dispersione (scatter plot)</span>
<span style="color: #b22222;"># </span><span style="color: #b22222;">per visualizzare la relazione tra altezza e peso</span>
heroes[heroes[<span style="color: #8b2252;">'Gender'</span>]==<span style="color: #8b2252;">'M'</span>].plot.scatter(<span style="color: #8b2252;">'Height'</span>, <span style="color: #8b2252;">'Weight'</span>)
plt.show()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Aggiunta di una linea di tendenza manuale</span>
heroes[heroes[<span style="color: #8b2252;">'Gender'</span>]==<span style="color: #8b2252;">'M'</span>].plot.scatter(<span style="color: #8b2252;">'Height'</span>, <span style="color: #8b2252;">'Weight'</span>)
<span style="color: #a0522d;">trend</span> = <span style="color: #a020f0;">lambda</span> x: -1200 + x * 7
<span style="color: #a0522d;">x_range</span> = [170, 300]
<span style="color: #a0522d;">line</span>, = plt.plot(x_range, <span style="color: #483d8b;">list</span>(<span style="color: #483d8b;">map</span>(trend, x_range)), color=<span style="color: #8b2252;">'black'</span>)
line.set_dashes([3, 2])  <span style="color: #b22222;"># </span><span style="color: #b22222;">Linea tratteggiata</span>
line.set_linewidth(2)    <span style="color: #b22222;"># </span><span style="color: #b22222;">Spessore della linea</span>
plt.show()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo e visualizzazione di una linea di tendenza con regressione lineare</span>
<span style="color: #a020f0;">from</span> sklearn <span style="color: #a020f0;">import</span> linear_model

<span style="color: #b22222;"># </span><span style="color: #b22222;">Creazione di un modello di regressione lineare</span>
<span style="color: #a0522d;">regr</span> = linear_model.LinearRegression()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Preparazione dei dati (rimozione dei valori mancanti)</span>
<span style="color: #a0522d;">heroes_with_data</span> = heroes[heroes[<span style="color: #8b2252;">'Gender'</span>]==<span style="color: #8b2252;">'M'</span>].copy().dropna()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Definizione delle variabili X (predittore) e Y (risposta)</span>
<span style="color: #a0522d;">X</span> = heroes_with_data.loc[:, [<span style="color: #8b2252;">'Height'</span>]]
<span style="color: #a0522d;">Y</span> = heroes_with_data[<span style="color: #8b2252;">'Weight'</span>]

<span style="color: #b22222;"># </span><span style="color: #b22222;">Addestramento del modello</span>
regr.fit(X, Y)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Visualizzazione del grafico con la linea di tendenza</span>
heroes[heroes[<span style="color: #8b2252;">'Gender'</span>]==<span style="color: #8b2252;">'M'</span>].plot.scatter(<span style="color: #8b2252;">'Height'</span>, <span style="color: #8b2252;">'Weight'</span>)
<span style="color: #a0522d;">line</span>, = plt.plot([0, 1000], regr.predict([[0], [1000]]), color=<span style="color: #8b2252;">'black'</span>)
line.set_dashes([3, 2])
line.set_linewidth(2)
plt.show()
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-org63ba63a" class="outline-3">
<h3 id="org63ba63a">Riassumere i dati</h3>
<div class="outline-text-3" id="text-org63ba63a">
</div>
<div id="outline-container-org418119f" class="outline-4">
<h4 id="org418119f">Indici di <a id="org6b0d8c3">centralità</a></h4>
<div class="outline-text-4" id="text-org418119f">
</div>
<div id="outline-container-orgc0ac8a0" class="outline-5">
<h5 id="orgc0ac8a0"><a id="org440f691">Media</a> campionaria</h5>
<div class="outline-text-5" id="text-orgc0ac8a0">
<p>
La <a href="#org440f691">media</a> campionaria è definita come:
</p>

<p>
\[\bar{x}=\frac{1}{n}\sum_{i=1}^{n}x_{i}\]
</p>

<p>
Ovvero è la somma di tutti i valori divisa per il numero di osservazioni.
</p>

<p>
È un po' come il <b>baricentro</b> del campione.
</p>

<p>
Dato che vanno eseguite operazioni matematiche sui valori, la <a href="#org440f691">media</a> può essere calcolata soltanto su dati <b>quantitativi</b> o <b>qualitativi</b> a cui è associato un <b>valore numerico</b>.
</p>

<p>
Dato un campione \(y = ax + b\), per \(a,b\) fissati, è facile derivare attraverso la formula che \(\bar{y} = a\bar{x}+y\), ovvero la <a href="#org440f691">media</a> di un campione trasformato è la <a href="#org440f691">media</a> del campione originale trasformata nello stesso modo.
</p>

<p>
Se di un campione si conoscono i \(k\) distinti valori quantitativi \(v\) del campione e le relative frequenze \(f\), allora la <a href="#org440f691">media</a> è definita come:
\[\bar{x}=\frac{1}{n}\sum_{j=1}^{k}v_{j}f_{j}\], che è anche la <b><a href="#org440f691">media</a> pesata</b> dei valori distinti, dove il peso di ogni valore è la sua frequenza. Questo ragionamento vale sia per le frequenze assolute che relative.
</p>

<p>
La <a href="#org440f691">media</a> campionaria <b>non è robusta</b>, ovvero è molto sensibile a <b>outlier</b> (valori fuori scala).
</p>

<p>
La somma di <b>tutti</b> gli scarti dei valori di un campione dalla <a href="#org440f691">media</a> è nulla, ovvero
\[\sum_{i=1}^{n} (x_i - \bar{x}) = 0\]
</p>
</div>
</div>
<div id="outline-container-org6c4acd7" class="outline-5">
<h5 id="org6c4acd7"><a id="org9e56fdf">Mediana</a> campionaria</h5>
<div class="outline-text-5" id="text-org6c4acd7">
<p>
La <a href="#org9e56fdf">mediana</a> campionaria è un altro indice di <a href="#org6b0d8c3">centralità</a>.
</p>

<p>
Per calcolarla, si ordinano i valori dell'insieme dal più piccolo al più grande.
</p>

<p>
Se il numero totale di elementi \(n\) è dispari, la <a href="#org9e56fdf">mediana</a> è il valore che occupa la posizione \((n + 1)/2\). Se n è pari, invece, la <a href="#org9e56fdf">mediana</a> è data dalla <a href="#org440f691">media</a> dei due valori centrali, ovvero quelli nelle posizioni \(n/2\) e \(n/2 + 1\).
</p>

<p>
A differenza della <a href="#org440f691">media</a>, la <a href="#org9e56fdf">mediana</a> può essere calcolata anche su dati <b>qualitativi</b>, purchè questi siano <b>ordinabili</b> e siano in numero <b>dispari</b>. Se fossero pari, sarebbe richiesto di fare la <a href="#org440f691">media</a> dei due valori centrali, che è impossibile.
</p>

<p>
La <a href="#org9e56fdf">mediana</a> è <b>robusta</b> rispetto ad eventuali outlier.
</p>
</div>
</div>
<div id="outline-container-orgd5248d4" class="outline-5">
<h5 id="orgd5248d4"><a id="orgd6a75da">Moda</a> campionaria</h5>
<div class="outline-text-5" id="text-orgd5248d4">
<p>
La <b><a href="#orgd6a75da">moda</a></b> è l'attributo che appare più spesso. Se esistono più attributi con frequenza massima, vengono chiamati <b>valori modali</b>. Può essere calcolata su qualunque tipo di dato.
</p>
</div>
</div>
</div>
<div id="outline-container-orgd279798" class="outline-4">
<h4 id="orgd279798">Indici di <a id="org22836a9">dispersione</a></h4>
<div class="outline-text-4" id="text-orgd279798">
<p>
Gli indici di <a href="#org22836a9">dispersione</a> sono utilizzati per capire la <b>variabilità</b> o lo <b>spread</b> di un campione.
</p>
</div>
<div id="outline-container-org6a7ed95" class="outline-5">
<h5 id="org6a7ed95"><a id="org41a8e1b">Varianza</a> campionaria</h5>
<div class="outline-text-5" id="text-org6a7ed95">
<p>
\[ s^2 = \frac{1}{n-1} \sum_{i=1}^{n} \left( x_i - \overline{x} \right)^2 \]
</p>

<p>
Il concetto su cui si basa la <b><a href="#org41a8e1b">varianza</a></b> campionaria è quello di accumulare la <b>distanza</b> fra ogni osservazione e la <b><a href="#org9e56fdf">mediana</a></b>.
</p>

<p>
Può essere calcolata <b>soltanto</b> su dati <b>quantitativi</b>.
</p>

<p>
Dalla definizione si ricava che la <a href="#org41a8e1b">varianza</a> di un campione trasformato linearmente è uguale alla <a href="#org41a8e1b">varianza</a> del campione originale <b>scalata</b> per <b>il quadrato del fattore</b>, mentre è <b>insensibile alla traslazione</b>.
</p>
</div>
<div id="outline-container-org2dc05a8" class="outline-6">
<h6 id="org2dc05a8">Perchè si usa il quadrato?</h6>
<div class="outline-text-6" id="text-org2dc05a8">
<p>
Per ottenere la somma delle differenze, occorre assicurarsi che le distanze calcolate siano sempre positive.
</p>

<p>
Infatti, le differenze per i valori superiori alla <a href="#org9e56fdf">mediana</a>, se lasciate invariate, produrrebbero valori negativi che ridurrebbero l'accumulazione totale.
</p>

<p>
Utilizzare la funzione del <b>valore assoluto</b> rappresenterebbe una scelta <b>subottimale</b>, poiché non si presta agevolmente a trasformazioni lineari.
</p>

<p>
Di conseguenza, l'approccio preferito consiste nel considerare il <b>quadrato</b> di ciascuna differenza.
</p>
</div>
</div>
</div>
<div id="outline-container-org39dcb4a" class="outline-5">
<h5 id="org39dcb4a"><a id="orged12c36">Deviazione standard</a></h5>
<div class="outline-text-5" id="text-org39dcb4a">
<p>
La <b><a href="#orged12c36">deviazione standard</a></b> è il quadrato della <a href="#org41a8e1b">varianza</a> campionaria. È utile perchè permette di esprimere la <a href="#org41a8e1b">varianza</a> nell'unità di misura originale.
</p>

<p>
Come la <a href="#org41a8e1b">varianza</a> è insensibile alla traslazione, mentre scala come il <b>valore assoluto del fattore</b> con cui è scalato il campione. Infatti, non avrebbe senso considerare una <a href="#org41a8e1b">varianza</a> negativa.
</p>
</div>
</div>
<div id="outline-container-org0cbcb88" class="outline-5">
<h5 id="org0cbcb88"><span class="todo WAIT">WAIT</span> Perchè si divide per \(n-1\)?</h5>
</div>
</div>
<div id="outline-container-org14a9103" class="outline-4">
<h4 id="org14a9103"><a id="orgbfbc9e0">Quantili</a></h4>
<div class="outline-text-4" id="text-org14a9103">
<p>
Un <b>quantile</b> di <b>livello</b> \(p \in [0,1]\) di un campione da \(n\) <b>osservazioni</b> è uno dei valori del campione tale per cui \(np\) valori del campione sono minori o uguali ad esso e \(n(1-p)\) valori ne sono invece maggiori o uguali. \(100p\) è la percentuale di valori minori o uguali a quello scelto e \(100(1-p)\) resto sono maggiori.
</p>

<p>
Se \(np\) è <b>intero</b>, allora si cerca quel valore che sia
</p>
<ul class="org-ul">
<li>maggiore o uguale a \(np\) elementi.
Lo trovo ordinando i valori del campione e scandendo dal più piccolo. L'elemento che incontro dopo \(n-p\) passi, ovvero in posizione \(np\), rispetta questa condizione.</li>
<li>minore o uguale a \(n-np\) elementi. Ordinando i valori e scandendo dal più grande, l'elemento \(n-np\) rispetta la condizione. Dato che al \(1°\) elemento scandito mi trovo in posizione \(n\), al \(2°\) in posizione \(n-1\), la posizione in cui mi trovo al passo \(k\) è \(n+1-k\). Quindi, l'elemento che ho trovato è quello in posizione \(n+1-(n-np)\), ovvero \(np+1\).</li>
</ul>

<p>
Di conseguenza, quando \(np\) è intero ci sono \(2\) elementi che rispettano entrambe le condizioni. Allora se ne prende la <b><a href="#org440f691">media</a></b>.
</p>

<p>
Dato che il numero di valori in un campione è intero, se \(np\) è <b>razionale</b>, allora si cerca il valore che sia minore uguale di \(\lceil np \rceil\) elementi e maggiore di \(\lceil n-np \rceil\) elementi. Quindi l'$np$-esimo elemento a partire dal più piccolo non basta e quindi devo andare in posizione \(np+1\), mentre, come nel caso intero, prendo l'$n-np$-esimo elemento partendo dal più grande, ovvero quello in posizione \(np+1\).
</p>

<p>
Quindi, quando \(np\) è razionale esiste <b>un solo</b> valore che rispetta entrambe le condizioni, ovvero quello in posizione \(np+1\) del campione ordinato ed è proprio quello il quantile di livello \(p\).
</p>

<p>
I <a href="#orgbfbc9e0">quantili</a> dove \(p\) ha al massimo due cifre decimali sono chiamati <b>percentili</b>, mentre quelli con \(p\) corrispondente ai multipli interi di \(0.25\) sono chiamati <b>quartili</b>.
</p>
</div>
<div id="outline-container-org0403396" class="outline-5">
<h5 id="org0403396">Box plot</h5>
<div class="outline-text-5" id="text-org0403396">
<p>
Il box plot è un grafico che riassume le seguenti informazioni: <b>minimo e massimo</b> del campione come <b>estremi dei baffi</b>, <b>primo e terzo</b> quartile come <b>estremi della scatola</b> e <b><a href="#org9e56fdf">mediana</a></b> (o secondo quartile) come <b>linea nella scatola</b>.
</p>
</div>
<div id="outline-container-org8dc1c5f" class="outline-6">
<h6 id="org8dc1c5f"><span class="todo TODO">TODO</span> Capire cosa ha detto il professore riguardo alla relazione fra box plot e distribuzione e capire cosa c'entra la <a href="#org440f691">media</a></h6>
</div>
</div>
<div id="outline-container-org5023b89" class="outline-5">
<h5 id="org5023b89">Range inter-quartile</h5>
<div class="outline-text-5" id="text-org5023b89">
<p>
Il <b>range inter-quartile</b> è un <b>indice di <a href="#org22836a9">dispersione</a></b> ed è calcolato come \(Q_3 - Q_1\), ovvero l'ampiezza della fascia di valori che circonda la <a href="#org9e56fdf">mediana</a> senza entrare nei quartili adiacenti. Corrisponde alla lunghezza del box nel box plot.
</p>

<p>
Quanto più questo range è grande, quanto più i valori sono lontani dalla <a href="#org9e56fdf">mediana</a>.
</p>
</div>
</div>
<div id="outline-container-orgf51e9d1" class="outline-5">
<h5 id="orgf51e9d1">QQ plot</h5>
<div class="outline-text-5" id="text-orgf51e9d1">
<p>
I grafici <b>quantile-quantile</b> sono usati per confrontare le distribuzioni di due campioni.
</p>

<p>
L'idea è quella di introdurre un sistema di riferimento cartesiano sulle cui assi si trovano i corrispondenti <a href="#orgbfbc9e0">quantili</a> dei due campioni e i punti si trovano in corrispondenza del valore dei due <a href="#orgbfbc9e0">quantili</a>.
</p>

<p>
Se i valori si allineano sulla funzione identità \(y=x\), ovvero sulla bisettrice del primo e terzo quadrante, allora le distribuzioni sono simili. Se si allineano su una retta diversa da quella precedente, c'è una relazione lineare fra i due campioni.
</p>

<p>
Il QQ plot è uno metodo grafico e <b>qualitativo</b>, usato soltanto per refutare o validare ipotesi su campioni.
</p>
</div>
<div id="outline-container-org11d9f65" class="outline-6">
<h6 id="org11d9f65"><span class="todo TODO">TODO</span> Come stampare?</h6>
</div>
</div>
</div>
<div id="outline-container-orgf40875c" class="outline-4">
<h4 id="orgf40875c">Distribuzione normale</h4>
<div class="outline-text-4" id="text-orgf40875c">
<p>
Un campione le frequenza dei valori presi in ordine cresce fino a raggiungere il picco in corrispondenza della <a href="#org9e56fdf">mediana</a> e poi decresce con la stessa pendenza con cui era cresciuta viene chiamata <b>normale</b>.
</p>

<p>
Questi campioni seguono una <b>regola empirica</b> per la quale (dove \(\bar{x}\) è la <a href="#org440f691">media</a> e \(s\) è la <a href="#orged12c36">deviazione standard</a>:
</p>
<ul class="org-ul">
<li>il \(68\%\) delle osservazioni hanno valore compreso entro \(\bar{x} \pm s\)</li>
<li>il \(95\%\) delle osservazioni hanno valore compreso entro \(\bar{x} \pm 2s\)</li>
<li>il \(99.7\%\) delle osservazioni hanno valore compreso entro \(\bar{x} \pm 3s\)</li>
</ul>
</div>
</div>
<div id="outline-container-org8a94430" class="outline-4">
<h4 id="org8a94430">Coefficiente di correlazione campionaria</h4>
<div class="outline-text-4" id="text-org8a94430">
<p>
Lo scatter plot di due osservazioni in coppia può presentare una <b>tendenza lineare</b>. Ovvero, i valori sulle ascisse e sulle ordinate sono fra loro proporzionali.
</p>

<p>
Date le <b>medie campionarie</b> \(\bar x, \bar y\), consideriamo gli <b>scarti</b> fra ogni valore di un campione e la propria <a href="#org440f691">media</a>. Quando lo scarto di un valore è <b>non negativo</b>, esso è più grande della propria <a href="#org440f691">media</a>, ovvero \(x-\bar x \geq 0\). Quando è <b>non positivo</b>, esso è minore e quindi \(x-\bar x \leq 0\).
</p>

<p>
Allora, data una coppia di valori \((x_i, y_i)\), se essi sono entrambi maggiori o uguali della propria <a href="#org440f691">media</a>, oppure sono entrambi minori o uguali, il <b>prodotto degli scarti</b> sarà <b>non negativo</b>, ovvero \((x-\bar x)(y-\bar y)\geq 0\). Quando invece un elemento della coppia è piccolo, mentre l'altro è grande, il prodotto dei loro scarti sarà <b>non positivo</b>.
</p>

<p>
Sommiamo quindi <b>tutti gli scarti</b>, ottenendo \(\sum_{i=1}^{n} (x-\bar x)(y-\bar y)\). Quanti più casi di coppie in cui gli entrambi gli elementi sono grandi o piccoli allo stesso momento, quanto maggiore sarà il risultato della sommatoria, mentre ogni caso di elementi l'uno grande e l'altro piccolo diminuisce il suo valore. <b>Standardizziamo</b> la somma dividendo per \((n-1)\). Questo indice si chiama <b>covarianza campionaria</b>:
</p>

<p>
\[ s_{xy} = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{n-1}\]
</p>

<p>
La covarianza campionaria è illimitata e la sua unità di misura è il prodotto delle unità di misura dei campioni.
</p>

<p>
Dato che i valori della covarianza sono nell'intervallo \([s_xs_y]\) (non dimostrato), normalizziamo dividendo per il <b>prodotto delle due deviazioni standard</b>. Il risultato è quindi quello di <b>scalare</b> il risultato della sommatoria limitandolo all'intervallo \([-1,1]\).
</p>

<p>
Si ottiene il <b>coefficiente di correlazione campionaria</b>
\[r_{xy} = \frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{(n-1)s_x s_y}\]
</p>

<p>
Quando \(r_{xy} > 0\), i dati sono <b>correlati positivamente</b>, o <b>direttamente</b>. Quando \(r_{xy} < 0\), i dati sono <b>correlati negativamente</b>, o <b>inversamente</b>. Quando \(r_{xy}\) è nullo, non c'è correlazione.
</p>

<p>
Inoltre, quanto più il valore di \(r_{xy}\) è alto, quanto più la correlazione è forte.
</p>

<p>
Se un dato è una trasformazione lineare dell'altro, allora:
</p>
<ul class="org-ul">
<li>\(r = 1\) se il coefficiente moltiplicativo è positivo;</li>
<li>\(r = -1\) se è negativo.</li>
</ul>

<p>
Se entrambi i dati sono trasformazioni lineari dello stesso dato, \(r = 1\) se i coefficienti sono concordi e l'opposto se discordi.
</p>
</div>
</div>
<div id="outline-container-org74d6d2e" class="outline-4">
<h4 id="org74d6d2e"><span class="todo TODO">TODO</span> Lorenz e Gini</h4>
<div class="outline-text-4" id="text-org74d6d2e">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">def</span> <span style="color: #0000ff;">gini</span>(series):
    <span style="color: #a020f0;">return</span> 1 - <span style="color: #483d8b;">sum</span>(series.value_counts(normalize=<span style="color: #008b8b;">True</span>)
                         .<span style="color: #483d8b;">map</span>(<span style="color: #a020f0;">lambda</span> f: f**2))

<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">normalized_gini</span>(series):
    <span style="color: #a0522d;">s</span> = <span style="color: #483d8b;">len</span>(series.unique())
    <span style="color: #a020f0;">return</span> s * gini(series) / (s-1)
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-org1e7948c" class="outline-3">
<h3 id="org1e7948c">Grafici</h3>
<div class="outline-text-3" id="text-org1e7948c">
<p>
<a href="https://matplotlib.org/stable/users/explain/quick_start.html">https://matplotlib.org/stable/users/explain/quick_start.html</a>
vedi xlim e ylim e xticks
</p>
</div>
<div id="outline-container-org0d01e63" class="outline-4">
<h4 id="org0d01e63">A bastoncini</h4>
<div class="outline-text-4" id="text-org0d01e63">
<div class="org-src-container">
<pre class="src src-python">plt.vlines(selected_freq.index, 0, selected_freq.values)
<span style="color: #b22222;">#</span><span style="color: #b22222;">aggiungi puntino sull'estremit&#224; superiore</span>
plt.plot(selected_freq.index, selected_freq.values, <span style="color: #8b2252;">'o'</span>)
plt.show()
</pre>
</div>
<p>
oppure
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">fig</span>, <span style="color: #a0522d;">ax</span> = plt.subplots()
ax.vlines(dim_freq.index, 0, dim_freq.values)
<span style="color: #b22222;">#</span><span style="color: #b22222;">aggiungi puntino sull'estremit&#224; superiore</span>
ax.plot(dim_freq.index, dim_freq.values, <span style="color: #8b2252;">'o'</span>)
plt.show()
</pre>
</div>
</div>
</div>
<div id="outline-container-org2aa18b4" class="outline-4">
<h4 id="org2aa18b4">A barre</h4>
<div class="outline-text-4" id="text-org2aa18b4">
<p>
Metodo definito su dataframe
</p>
<div class="org-src-container">
<pre class="src src-python">dataframe.plot.bar(legend=<span style="color: #008b8b;">False</span>)
plt.show
</pre>
</div>
<p>
Metodo OOP
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">fig</span>, <span style="color: #a0522d;">ax</span> = plt.subplots()
ax.bar(dim_freq.index, dim_freq[<span style="color: #8b2252;">'Abs. frequence'</span>])
plt.show()
</pre>
</div>
<p>
Parametro <code>alpha</code> minore di 1 per sovrapporre colori diversi
</p>
</div>
</div>
<div id="outline-container-org36f7236" class="outline-4">
<h4 id="org36f7236">Poligonale</h4>
<div class="outline-text-4" id="text-org36f7236">
<p>
Metodo definito su dataframe, <code>marker</code> per puntino
</p>
<div class="org-src-container">
<pre class="src src-python">dataframe.plot(marker=<span style="color: #8b2252;">'o'</span>, color=<span style="color: #8b2252;">'blue'</span>, legend=<span style="color: #008b8b;">False</span>)
plt.show
</pre>
</div>
<p>
Metodo pyplot
</p>
<div class="org-src-container">
<pre class="src src-python">plt.plot(dim_freq, label = <span style="color: #8b2252;">'norm'</span>)
<span style="color: #a0522d;">new</span> = dim_freq * 2
plt.plot(new)
plt.legend()
</pre>
</div>
<p>
Usare invece step per step discreti
</p>
</div>
</div>
<div id="outline-container-org5ad55a6" class="outline-4">
<h4 id="org5ad55a6">A torta (aerogramma)</h4>
<div class="outline-text-4" id="text-org5ad55a6">
<p>
Sul dataframe
</p>
<div class="org-src-container">
<pre class="src src-python">dataframe.plot.pie(y=<span style="color: #8b2252;">'Abs. frequence'</span>, colors=[<span style="color: #8b2252;">'pink'</span>, <span style="color: #8b2252;">'blue'</span>])
plt.show()
</pre>
</div>
</div>
</div>
<div id="outline-container-orgf6fae5f" class="outline-4">
<h4 id="orgf6fae5f">Istogramma</h4>
<div class="outline-text-4" id="text-orgf6fae5f">
<div class="org-src-container">
<pre class="src src-python">series.hist(bins=50)
plt.show()
</pre>
</div>
<p>
oppure con diverse dimensioni dei bin
</p>
<div class="org-src-container">
<pre class="src src-python">heroes[<span style="color: #8b2252;">'Weight'</span>].hist(bins=np.hstack((np.arange(0, 200, 20),
                                      np.arange(200, 500, 50),
                                      np.arange(500, 1000, 100))))
</pre>
</div>
</div>
</div>
<div id="outline-container-org538f387" class="outline-4">
<h4 id="org538f387">Grafici multipli</h4>
<div class="outline-text-4" id="text-org538f387">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">fig</span>, <span style="color: #a0522d;">axs</span> = plt.subplots(2)
axs[0].vlines(dim_freq.index, 0, dim_freq[<span style="color: #8b2252;">'Abs. frequence'</span>])
axs[0].set_title(<span style="color: #8b2252;">'bastoncini'</span>)
axs[1].plot(dim_freq)
axs[1].set_title(<span style="color: #8b2252;">'poligono'</span>)
fig.tight_layout()
</pre>
</div>
</div>
</div>
<div id="outline-container-org5750333" class="outline-4">
<h4 id="org5750333">Scatter plot</h4>
<div class="outline-text-4" id="text-org5750333">
<p>
Si stampa con
<code>heroes[heroes['Gender'] =</code> 'M'].plot.scatter('Height', 'Weight')=
</p>
</div>
</div>
<div id="outline-container-org4031975" class="outline-4">
<h4 id="org4031975">Quando usarli</h4>
<div class="outline-text-4" id="text-org4031975">
<dl class="org-dl">
<dt>frequenze</dt><dd>A barre non va bene, perchè essendo l'attributo di tipo numerico, si corre il rischio che ogni barra venga percepita come associata più a un intervallo di valori piuttosto che a un unico numero. Per evitare questo fraintendimento è più opportuno generare un grafico a bastoncini.</dd>
</dl>
</div>
</div>
</div>
</div>
<div id="outline-container-orgc4f9e77" class="outline-2">
<h2 id="orgc4f9e77"><span class="todo TODO">TODO</span> Calcolo combinatorio</h2>
<div class="outline-text-2" id="text-orgc4f9e77">
</div>
<div id="outline-container-orge7324dc" class="outline-3">
<h3 id="orge7324dc">Teorema fondamentale del calcolo combinatorio</h3>
</div>
<div id="outline-container-orgd3943c9" class="outline-3">
<h3 id="orgd3943c9">Permutazioni</h3>
</div>
<div id="outline-container-org2db997b" class="outline-3">
<h3 id="org2db997b">Disposizioni</h3>
</div>
<div id="outline-container-orgc72e4b4" class="outline-3">
<h3 id="orgc72e4b4">Combinazioni</h3>
</div>
</div>
<div id="outline-container-org88aaa01" class="outline-2">
<h2 id="org88aaa01">Probabilità</h2>
<div class="outline-text-2" id="text-org88aaa01">
<p>
Lo <b>spazio campionario (sample space)</b> è l'insieme dei possibili esiti e si indica con la lettera \(\Omega\).
</p>

<p>
Gli elementi \(\omega\) dello spazio campionario si chiamano <b>esiti o eventi elementari (outcomes)</b> e sono <b>concetti primitivi</b> (non definiti).
</p>

<p>
L'<b>evento (event)</b> è un sottoinsieme \(A\subseteq \Omega\). Gli eventi possibili sono anche \(\Omega\) stesso, quindi <b>evento certo</b>, e \(\emptyset\), ovvero <b>evento impossibile</b>.
</p>

<p>
Gli eventi sono insiemi e vi si possono applicare le tipiche operazioni insiemistiche, con le solite proprietà quali commutatività, associatività, distribuzione dell'unione sull'intersezione e viceversa e leggi di DeMorgan.
</p>

<p>
Una famiglia di eventi \(\mathcal{A}\) (ovvero un insieme di sottoinsiemi di \(\Omega\)) è un <b>algebra</b> secondo l'<b>algebra degli insiemi</b> se:
</p>
<ul class="org-ul">
<li>\(\Omega \in \mathcal{A}\), ovvero l'evento certo fa parte dell'algebra,</li>
<li>\(A \in \mathcal{A} \Rightarrow A^c  \in \mathcal{A}\), ovvero se un evento fa parte dell'algebra, deve farne parte anche il suo complemento,</li>
<li>\(A_i \in \mathcal{A} \; \forall i \in \mathcal{N} \Rightarrow \bigcup_{i = 1}^{\infty} {A_i } \in \mathcal{A}\), ovvero l'unione di ogni evento è presente nell'algebra.</li>
</ul>

<p>
Per estendere questo concetto agli insiemi infiniti, si introduce il concetto di \(\sigma\) -algebra
</p>

<p>
Se prendo in considerazione i singoletti di ogni evento allora l'insieme \(\mathcal{A}\) è l'<b>insieme delle parti</b> di \(\Omega\).
</p>

<p>
La teoria della probabilità di <b>Kolmogorov</b> si basa su questi tre assiomi:
</p>
<ol class="org-ol">
<li>\(P(E)\in\mathbb{R}, P(E) \geq 0 \qquad \forall E \in F\), ovvero la probabilità di un evento è un numero reale non negativo.</li>
<li>\(P(\Omega) = 1\), ovvero la probabilità dello spazio campione è \(1\).</li>
<li>La probabilità dell'unione di due eventi mutualmente esclusivi è la somma delle loro probabilità.</li>
</ol>

<p>
Da questi assiomi seguono le seguenti proposizioni (facilmente dimostrabili):
</p>
<ul class="org-ul">
<li>\(P(A \cup B) = P(A) + P(B) - P(A \cap B)\), ovvero la probabilità dell'unione di eventi non disgiunti è la somma delle probabilità singole meno la probabilità dell'intersezione</li>
<li>\[P\left(A^{c}\right) = P(\Omega\setminus A) = 1 - P(A)\]
ovvero la probabilità dell'evento complementare è il complemento a 1 della probabilità di un evento. Si dimostra così:
\[1 = P(S) = P(E \cup E^C) = P(E)+P(E^C)\]</li>
</ul>

<p>
Altri corollari sono che ogni probabilità è minore di \(1\) e che un evento sottoinsieme di un altro ha probabilità minore dell'altro. (Vedere eventualmente dimostrazione)
</p>

<p>
Per ottenere la probabilità dell'unione di più eventi, si può procedere in questo modo:
\[P(A_1 \cup A_2 \cup ... \cup A_n) = 1 - P((A_1 \cup A_2 \cup ... \cup A_n)^c) = 1-P(A_1^cA_2^c ...A_n^c)\]
</p>

<p>
Uno spazio è detto <b>equiprobabile</b> se ogni evento ha la stessa probabiltità di verificarsi.
</p>

<p>
La <b>probabilità condizionata</b> di un evento \(E\) dato che l'evento \(F\) è avvenuto è indicata con \(P(E|F)\). Si parla dei casi in cui sia \(E\) che \(F\) accadono, ma restringendo lo spazio campionario ai soli casi in cui \(F\) è accaduto, quindi:
\[P(E|F)=\frac{P(EF)}{P(F)}\]
</p>

<p>
Registra il concetto di "come cambia la certezza conoscendo un informazione parziale sull'esito dell'esperimento casuale".
</p>

<p>
Dato che un evento \(E\) può essere scritto come \(EF \cup EF^C\) e dato che dalla definizione di probabilità condizionata si ha che \(P(EF) = P(E|F)P(F)\), si ottiene la formula
\[P(E) = P(E|F)P(F) + P(E|F^C)[1-P(F)]\]
</p>

<p>
Questa formula mi permette di calcolare la probabilità di un evento \(E\) in base ad un altro evento \(F\), conoscendo la probabilità \(F\) e quella di \(E\) condizionato ad \(F\).
(Keep going from example 3.7.d included)
</p>

<p>
La formula può essere estesa ad \(n\) eventi \(F_i\) tali per cui essi sono tutti mutualmente esclusivi e \(\bigcup_{i=1}^n F_i = \Omega\). Dato che gli eventi sono mutualmente esclusivi fra loro, ottengo la formula
\[P(E) = \sum_{i=1}^n P(EF_i) = \sum_{i=1}^n P(E|F_i)P(F_i)\].
</p>

<p>
Questa formula mi dice che \(P(E)\) è la <a href="#org440f691">media</a> degli \(P(E|F_i)\) pesata in base ad \(F_i\).
</p>

<p>
Ho un evento \(E\). Non ne conosco la probabilità. Conosco però la probabilità di un gruppo di \(n\) eventi disgiunti e "totali" \(F_i\) e so come l'esito di un certo esperimento influenza \(E\). Dato che un evento \(E\) posso scriverlo come \(\bigcup_{i=1}^n EF_i\), allora riesco a ricavare \(P(E)\) soltanto in funzione di \(P(F_i)\) e \(P(E|F_i)\).
</p>

<p>
La <b>formula di Bayes</b> è la seguente:
\[P(F_j|E) = \frac{P(F_jE)}{P(E)} = \frac{P(E|F_j)P(F_j)}{\sum_{i=1}^n P(E|F_i)P(F_i)}\]
si usa per incorporare la probabilità di un evento \(E\) (evidence) nell'evento \(F_j\) (hypotesis).
</p>

<p>
Ho un'insieme di ipotesi disgiunte \(F_i\) che unite coprono l'intero spazio delle probabilità. Voglio capire come l'indizio \(E\) influenzi una delle ipotesi. Allora considero la probabilità che l'indizio sia "corretto" per ogni ipotesi e lo peso in base alla probabilità che quell'ipotesi fosse vera in partenza (vedi aggiunta sotto). Prese tutte le ipotesi così aggiornate, l'ipotesi per cui mi sto interrogando va divisa per tutte le ipotesi aggiornate.
</p>

<p>
Dato \(P(H|E)\), ovvero quanto è probabile l'ipotesi considerando il dato nuovo, allora la quantità \(P(E|H)\) è come chiedersi quanto il nuovo dato "predica" l'ipotesi. Se \(P(E|H)\) è molto alto, allora c'è una forte "correlazione" fra gli eventi.
</p>

<p>
Due eventi sono detti <b>indipendenti</b> quando \(P(E|F) = P(E)\), in base alla definizione di probabilità condizionata. Questa definizione non è molto apprezzata perchè "tratta \(E\) ed \(F\) in modo diverso" (probabilmente si intende il fatto che la probabilità condizionata a sinistra non è commutativa).
</p>

<p>
Allora sostituisco alla probabilità condizionata la sua definizione e manipolando ottengo \(P(EF) = P(E)P(F)\), ovvero la probabilità dell'intersezione fattorizza come il prodotto dei singoli eventi. Questa seconda formula è chiaramente simmetrica e anche l'indipendenza è una relazione simmetrica.
</p>

<p>
L'indipendenza cattura il concetto che i due eventi non sono influenzati, ovvero il fatto che un evento sia accaduto non cambia la probabilità che l'altro accada. La probabilità di un evento \(E\) l'altro \(F\) è la stessa probabilità dell'evento \(E\) da solo, che è come dire che la proporzione dei casi in cui gli eventi \(E\) ed \(F\) avvengono entrambi rispetto ai casi in cui avviene \(F\) è la stessa proporzione con cui \(E\) avviene nella popolazione generale.
</p>

<p>
Il fatto che \(F\) sia accaduto o meno non cambia l'informazione riguardo \(E\). Infatti, se provo ad incorporare un evidenza indipendente nella mia ipotesi (caso d'uso della formula di Bayes), ottengo \(P(H|E) = \frac{P(EF)}{P(E)} = \frac{P(E)P(F)}{P(E)} = P(H)\)
</p>

<pre class="example" id="org863f21e">
Immagina una popolazione in cui:
la metà sono uomini, P(U)=0.5
un decimo sono mancini, P(M)=0.1

Se gli eventi sono indipendenti, allora la probabilità di trovare un mancino fra gli uomini è P(M|U) = P(M), perchè un uomo può essere mancino tanto quanto la popolazione generale.

Allo stesso modo, ci si aspetta che la metà dei mancini sia uomo e quindi P(U|M)=P(U).

Quindi ci sono P(U) uomini fra i P(M) mancini, quindi il 50% dei mancini, che sono il 10%, per un totale di 5%.

Allo stesso modo c'è il 10% di mancini fra gli uomini, che sono il 50%, per un totale di 5%.

Si può considerare che data una persona fra 100, la probabilità che sia un uomo mancino è come pescare una palla da un urna di 100, dove 50 sono uomo e 50 donna, e poi pescare una palla da un'altra urna, sempre da 100, dove 10 sono mancino e 90 no. Esistono 50×10 casi in cui la persona sia un uomo mancino e 10000 casi in totale.
</pre>

<p>
Se \(E\) e \(F\) sono indipendenti, lo sono anche \(E\) e \(F^c\). La dimostrazione è una semplice manipolazione algebrica di \(P(E) = P(EF)+P(EF^c)\) (3 assioma) usando che \(P(EF) = P(E)P(F)\) per ipotesi.
</p>

<p>
Se \(E\) e \(F\) sono disgiunti e le probabilità di entrambi sono maggiori di \(0\), allora non sono indipendenti. L'intuizione è che se gli eventi sono disgiunti e gli eventi sono entrambi maggiori di \(0\), allora \(F^c\) è un sovrainsieme di \(E\) e quindi \(P(E|F^c)\) è necessariamente minore di $P(E)
</p>

<p>
Più di due eventi \(E\) &isin; \(I\) sono indipendenti gli eventi di ogni sottoinsieme di \(I\) sono indipendenti fra loro.
</p>

<p>
<b><b>Sistemi in serie</b></b>
</p>

<ul class="org-ul">
<li>\[ P(\text{funziona}) = P(A_1) P(A_2) \dots P(A_n) \]</li>

<li>\[ P(\text{non funziona}) = 1 - P(A_1) P(A_2) \dots P(A_n) \]</li>
</ul>

<p>
&#x2014;
</p>

<p>
<b><b>Sistemi in parallelo</b></b>
</p>

<ul class="org-ul">
<li>\[ P(\text{funziona}) = 1 - P(A_1^c) P(A_2^c) \dots P(A_n^c) \]</li>

<li>\[ P(\text{non funziona}) = P(A_1^c) P(A_2^c) \dots P(A_n^c) \]</li>
</ul>
</div>
</div>
<div id="outline-container-org9eea659" class="outline-2">
<h2 id="org9eea659">Variabili aleatorie</h2>
<div class="outline-text-2" id="text-org9eea659">
</div>
<div id="outline-container-org9dc9c96" class="outline-3">
<h3 id="org9dc9c96">Definizione e concetti di base</h3>
<div class="outline-text-3" id="text-org9dc9c96">
<p>
Le <b>variabili aleatorie</b> si usano per poter codificare degli esiti di un esperimento aleatorio in termini numerici, permettendo più operazioni matematiche sui risultati.
</p>

<p>
Una variabile aleatoria si "costruisce" associando ad ogni esito \(\omega \in \Omega\) un valore mediante la funzione \(X:\Omega\rightarrow\mathbb{R}\) e poi ragionando sui valori assunti dalla variabile e non più sugli esiti dell'esperimento.
</p>

<p>
I valori assunti dalla variabile (specificazioni) sono definiti in base agli esiti dell'esperimento su cui è definita:
\[\{ X=\alpha \} = \{\omega\in\Omega:X(\omega)=\alpha\}\]
ovvero, la variabile assume il valore \(\alpha\) quando l'immagine dell'esito \(\omega\) è \(\alpha\).
</p>
</div>
</div>
<div id="outline-container-org4a34135" class="outline-3">
<h3 id="org4a34135">Funzione di ripartizione e funzione di massa di probabilità</h3>
<div class="outline-text-3" id="text-org4a34135">
<p>
<b>Definizione (Funzione di ripartizione):</b> La funzione di ripartizione di X, indicata con \(F_X(x)\), è definita come la probabilità che la variabile aleatoria X assuma valore minore o uguale a x:
\[F_X(x) = P(X \leq x)\]
</p>

<p>
Per variabili aleatorie su supporti discreti, vale il ragionamento per cui:
\[F_X(b) - F_X(a) = P(a < X \leq b)\]
</p>

<p>
<b>Dimostrazione:</b>
</p>
\begin{align}
F_X(b) - F_X(a) &= P(X \leq b) - P(X \leq a)\\
&= P((X \leq a) \cup (a < X \leq b)) - P(X \leq a)\\
&= P(X \leq a) + P(a < X \leq b) - P(X \leq a)\\
&= P(a < X \leq b)
\end{align}

<p>
La funzione di ripartizione \(F_X(x)\) ha le seguenti proprietà:
</p>
<ol class="org-ol">
<li>È sempre \(F_X(x) \geq 0\) (la probabilità non è mai negativa)</li>
<li>Il suo limite a infinito è \(\lim_{x \to \infty} F_X(x) = 1\) (certezza)</li>
<li>È continua da destra (per costruzione)</li>
<li>È non decrescente (se \(a < b\), allora \(F_X(a) \leq F_X(b)\))</li>
</ol>

<p>
<b>Definizione (Funzione di massa di probabilità):</b> La funzione di massa di probabilità, indicata con \(p_X(x)\), è definita come la probabilità che la variabile aleatoria X assuma esattamente il valore x:
\[p_X(x) = P(X = x)\]
</p>

<p>
Questa funzione ha le seguenti proprietà:
</p>
<ol class="org-ol">
<li>È sempre \(p_X(x) \geq 0\) (la probabilità non è mai negativa)</li>
<li>La somma su ogni valore della variabile è \(\sum_x p_X(x) = 1\) (certezza)</li>
</ol>

<p>
Sia la funzione di ripartizione \(F_X(x)\) che la funzione di massa di probabilità \(p_X(x)\) da sole bastano per descrivere una variabile aleatoria e si può passare dall'una all'altra considerando che \(F_X(x)\) è una funzione costante a tratti e i salti fra i tratti sono uguali al valore di \(p_X(x)\) in quel punto. I passaggi sono questi:
\[F_X(x) = P(X \leq x) = P\left(\bigcup_{a \leq x} \{X = a\}\right) = \sum_{a \leq x} p_X(a)\]
</p>

<p>
E viceversa:
\[p_X(x) = F_X(x) - \lim_{y \to x^-} F_X(y)\]
</p>
</div>
</div>
<div id="outline-container-org30aaf5e" class="outline-3">
<h3 id="org30aaf5e">Funzione indicatrice</h3>
<div class="outline-text-3" id="text-org30aaf5e">
<p>
Una <b>funzione indicatrice</b> di un evento A, indicata con \(I_A(x)\), è una funzione che restituisce \(1\) se è avvenuto l'evento e \(0\) altrimenti:
\[I_A(x) = \begin{cases}
1 & \text{se } x \in A \\
0 & \text{se } x \notin A
\end{cases}\]
</p>

<p>
Le funzioni indicatrici sono utili per rappresentare eventi in forma matematica e per calcolare probabilità attraverso il valore atteso.
</p>
</div>
</div>
<div id="outline-container-org31227f4" class="outline-3">
<h3 id="org31227f4">Valore atteso e <a href="#org41a8e1b">varianza</a></h3>
<div class="outline-text-3" id="text-org31227f4">
<p>
<b>Definizione (Valore atteso):</b> Per variabili aleatorie discrete, il valore atteso (o <a href="#org440f691">media</a>), indicato con \(E[X]\), è definito come:
\[E[X] = \sum_x x \cdot P(X = x) = \sum_x x \cdot p_X(x)\]
</p>

<p>
ovvero la <a href="#org440f691">media</a> ponderata in cui i pesi sono le probabilità.
</p>

<p>
Quando si tratta di insiemi enumerabili infiniti, è necessario verificare la convergenza della serie. Nei casi che tratteremo, assumeremo sempre che la serie converga.
</p>

<p>
Il valore atteso di una funzione indicatrice è:
\[E[I_A] = 1 \cdot P(I_A = 1) + 0 \cdot P(I_A = 0) = P(A)\]
</p>

<p>
<b>Proprietà (Area e valore atteso):</b> Per variabili aleatorie non negative, il valore atteso può essere interpretato geometricamente come l'area sotto la curva della funzione di sopravvivenza \(1-F_X(x)\):
\[E[X] = \int_{0}^{+\infty} (1-F_X(x)) \, dx\]
</p>

<p>
Per variabili aleatorie discrete positive, questa proprietà si può verificare dividendo l'area in rettangoli di altezza \(P(X > k)\) e larghezza 1, ottenendo:
\[E[X] = \sum_{k=0}^{\infty} P(X > k) = \sum_{k=1}^{\infty} k \cdot P(X = k)\]
</p>

<p>
<b>Definizione (<a href="#org41a8e1b">Varianza</a>):</b> La <a href="#org41a8e1b">varianza</a>, indicata con \(\text{Var}(X)\), si calcola con:
\[\text{Var}(X) = E[(X - E[X])^2]\]
</p>

<p>
che può essere espanso per ottenere una formula più pratica:
\[\text{Var}(X) = E[X^2] - (E[X])^2\]
</p>

<p>
Da questo consegue che la <a href="#org41a8e1b">varianza</a> di una variabile aleatoria che descrive \(n\) valori equiprobabili da 1 a \(n\) ha la forma:
\[\text{Var}(X) = \frac{n^2-1}{12}\]
</p>

<p>
La <a href="#org41a8e1b">varianza</a> della funzione indicatrice, considerando che questa è idempotente (ovvero \(I_A(x)^2 = I_A(x)\)), è:
\[\text{Var}(I_A) = E[I_A] - (E[I_A])^2 = P(A) - P(A)^2 = P(A) \cdot (1 - P(A)) = P(A) \cdot P(A^c)\]
</p>
</div>
</div>
<div id="outline-container-orgc7e5d71" class="outline-3">
<h3 id="orgc7e5d71">Variabili aleatorie multivariate</h3>
<div class="outline-text-3" id="text-orgc7e5d71">
<p>
<b>Definizione (Funzione di ripartizione congiunta):</b> La funzione di ripartizione congiunta di due variabili aleatorie X e Y, indicata con \(F_{X,Y}(x,y)\), è definita come:
\[F_{X,Y}(x,y) = P(X \leq x, Y \leq y)\]
</p>

<p>
Quando consideriamo \(\lim_{y \to \infty} F_{X,Y}(x,y)\) otteniamo \(F_X(x)\), che viene chiamata <b>funzione di ripartizione marginale</b>.
</p>
</div>
<div id="outline-container-orgfc3764d" class="outline-4">
<h4 id="orgfc3764d">Massa di probabilità congiunta e marginale</h4>
<div class="outline-text-4" id="text-orgfc3764d">
<p>
<b>Definizione (Massa di probabilità congiunta):</b> La funzione di massa di probabilità congiunta di due variabili aleatorie discrete \(X\) e \(Y\), indicata con \(p_{X,Y}(x,y)\), è definita come:
\[p_{X,Y}(x,y) = P(X = x, Y = y)\]
</p>

<p>
<b>Definizione (Massa di probabilità marginale):</b> La funzione di massa di probabilità marginale di \(X\), indicata con \(p_X(x)\), si ottiene sommando la massa di probabilità congiunta su tutti i possibili valori di \(Y\):
\[p_X(x) = \sum_y p_{X,Y}(x,y)\]
</p>

<p>
Analogamente, per la variabile aleatoria \(Y\):
\[p_Y(y) = \sum_x p_{X,Y}(x,y)\]
</p>

<p>
La massa di probabilità marginale si ottiene considerando tutte le associazioni della prima variabile con la seconda, fissando la prima e poi sommandole, poiché gli eventi \(\{X=x, Y=y\}\) per valori diversi di \(y\) sono disgiunti.
</p>
</div>
</div>
<div id="outline-container-orga8e1458" class="outline-4">
<h4 id="orga8e1458">Indipendenza tra variabili aleatorie</h4>
<div class="outline-text-4" id="text-orga8e1458">
<p>
<b>Definizione (Indipendenza di variabili aleatorie):</b> Due variabili aleatorie \(X\) e \(Y\) si dicono indipendenti se ogni evento della specificazione dell'una è indipendente da ogni evento dell'altra. 
</p>

<p>
Formalmente, \(X\) e \(Y\) sono indipendenti se e solo se:
</p>

<ol class="org-ol">
<li>Per la funzione di ripartizione congiunta:
\[F_{X,Y}(x,y) = F_X(x) \cdot F_Y(y) \quad \text{per ogni } x, y \in \mathbb{R}\]</li>

<li>Per la funzione di massa di probabilità congiunta (nel caso discreto):
\[p_{X,Y}(x,y) = p_X(x) \cdot p_Y(y) \quad \text{per ogni } x, y\]</li>
</ol>
</div>
</div>
<div id="outline-container-org3f57426" class="outline-4">
<h4 id="org3f57426">Valore atteso di una somma di variabili aleatorie</h4>
<div class="outline-text-4" id="text-org3f57426">
<p>
Considerando un vettore multivariato, ovvero un insieme di variabili aleatorie \(X_1, X_2, \ldots, X_n\), possiamo definire una nuova variabile aleatoria \(S = X_1 + X_2 + \ldots + X_n\).
</p>

<p>
Il valore atteso di \(S\) può essere calcolato come:
\[E[S] = E[X_1 + X_2 + \ldots + X_n]\]
</p>

<p>
Dalla definizione di valore atteso:
\[E[S] = \sum_{x_1} \sum_{x_2} \ldots \sum_{x_n} (x_1 + x_2 + \ldots + x_n) \cdot p_{X_1,X_2,\ldots,X_n}(x_1,x_2,\ldots,x_n)\]
</p>

<p>
Riarrangiando i termini:
\[E[S] = \sum_{x_1} x_1 \sum_{x_2} \ldots \sum_{x_n} p_{X_1,X_2,\ldots,X_n}(x_1,x_2,\ldots,x_n) + \ldots\]
</p>

<p>
Dove le sommatorie interne corrispondono alle probabilità marginali di ciascuna variabile. Quindi:
\[E[S] = \sum_{x_1} x_1 \cdot p_{X_1}(x_1) + \sum_{x_2} x_2 \cdot p_{X_2}(x_2) + \ldots + \sum_{x_n} x_n \cdot p_{X_n}(x_n)\]
</p>

<p>
\[E[S] = E[X_1] + E[X_2] + \ldots + E[X_n]\]
</p>

<p>
Questo dimostra la linearità del valore atteso: \(E[X+Y] = E[X] + E[Y]\).
</p>
</div>
</div>
<div id="outline-container-org18e3960" class="outline-4">
<h4 id="org18e3960">Valore atteso del prodotto di variabili aleatorie</h4>
<div class="outline-text-4" id="text-org18e3960">
<p>
Per definizione di valore atteso:
\[E(XY)=\sum_i \sum_j x_i \cdot y_j \cdot P(X=x_i,Y=y_j)\]
</p>

<p>
Per definizione di indipendenza, vale che:
\[P(X=x_i,Y=y_j) = P(X=x_i) \cdot P(Y=y_j)\]
</p>

<p>
Quindi otteniamo:
\[E(XY)=\sum_i \sum_j x_i \cdot y_j \cdot P(X=x_i) \cdot P(Y=y_j)\]
</p>

<p>
Che è separabile in:
\[E(XY) = \left(\sum_i x_i \cdot P(X=x_i)\right) \cdot \left(\sum_j y_j \cdot P(Y=y_j)\right) = E(X) \cdot E(Y)\]
</p>

<p>
Da questo deriva che se \(X\) e \(Y\) sono indipendenti, allora \(E(XY) = E(X) \cdot E(Y)\).
</p>
</div>
</div>
<div id="outline-container-org573876c" class="outline-4">
<h4 id="org573876c">Stima di una variabile aleatoria</h4>
<div class="outline-text-4" id="text-org573876c">
<p>
Se vogliamo stimare il valore di una variabile aleatoria \(X\) usando un valore costante \(c\), possiamo calcolare il valore atteso dell'errore quadratico:
\[E[(X-c)^2]\]
</p>

<p>
Riscrivendo questo, aggiungendo e sottraendo il valore atteso di \(X\), indicato con \(\mu\):
\[E[(X-c)^2] = E[(X-\mu+\mu-c)^2]\]
</p>

<p>
Espandendo il binomio:
\[E[(X-c)^2] = E[(X-\mu)^2 + 2(X-\mu)(\mu-c) + (\mu-c)^2]\]
</p>

<p>
\[E[(X-c)^2] = E[(X-\mu)^2] + 2(\mu-c)E[X-\mu] + (\mu-c)^2\]
</p>

<p>
Poiché \(E[X-\mu] = E[X] - \mu = 0\):
\[E[(X-c)^2] = E[(X-\mu)^2] + (\mu-c)^2\]
</p>

<p>
\[E[(X-c)^2] = \text{Var}(X) + (\mu-c)^2\]
</p>

<p>
Questo è sempre maggiore o uguale alla <a href="#org41a8e1b">varianza</a> di \(X\), poiché \((\mu-c)^2 \geq 0\). L'errore minimo si ottiene quando \(c = \mu = E[X]\), e in quel caso l'errore quadratico medio è esattamente uguale alla <a href="#org41a8e1b">varianza</a>.
</p>
</div>
</div>
<div id="outline-container-org39d0753" class="outline-4">
<h4 id="org39d0753">Covarianza: definizione e proprietà</h4>
<div class="outline-text-4" id="text-org39d0753">
<p>
<b>Definizione (Covarianza):</b> La covarianza tra due variabili aleatorie \(X\) e \(Y\), indicata con \(\text{Cov}(X,Y)\), è definita come:
\[\text{Cov}(X,Y) = E[(X-\mu_X)(Y-\mu_Y)]\]
dove \(\mu_X = E[X]\) e \(\mu_Y = E[Y]\).
</p>

<p>
Usando la linearità del valore atteso, si può dimostrare che:
\[\text{Cov}(X,Y) = E[XY] - E[X] \cdot E[Y]\]
</p>

<p>
<b>Proprietà della covarianza:</b>
</p>

<ol class="org-ol">
<li>\(\text{Cov}(aX,Y) = a \cdot \text{Cov}(X,Y)\) per ogni costante \(a\)</li>
<li>\(\text{Cov}(X+Y,Z) = \text{Cov}(X,Z) + \text{Cov}(Y,Z)\)</li>
<li>Più in generale: \(\text{Cov}\left(\sum_{i} X_i, \sum_{j} Y_j\right) = \sum_{i} \sum_{j} \text{Cov}(X_i, Y_j)\)</li>
<li>\(\text{Cov}(X,X) = \text{Var}(X)\)</li>
</ol>
</div>
</div>
<div id="outline-container-org6549c85" class="outline-4">
<h4 id="org6549c85"><a href="#org41a8e1b">Varianza</a> di somme di variabili aleatorie</h4>
<div class="outline-text-4" id="text-org6549c85">
<p>
Per la <a href="#org41a8e1b">varianza</a> di una somma di variabili aleatorie \(X\) e \(Y\), vale:
\[\text{Var}(X+Y) = E[(X+Y)^2] - (E[X+Y])^2\]
</p>

<p>
Usando la linearità del valore atteso e sviluppando i quadrati:
\[\text{Var}(X+Y) = E[X^2 + 2XY + Y^2] - (E[X] + E[Y])^2\]
\[\text{Var}(X+Y) = E[X^2] + 2E[XY] + E[Y^2] - E[X]^2 - 2E[X]E[Y] - E[Y]^2\]
\[\text{Var}(X+Y) = \text{Var}(X) + \text{Var}(Y) + 2(E[XY] - E[X]E[Y])\]
\[\text{Var}(X+Y) = \text{Var}(X) + \text{Var}(Y) + 2\text{Cov}(X,Y)\]
</p>

<p>
In generale, per una somma di \(n\) variabili aleatorie:
\[\text{Var}\left(\sum_{i=1}^n X_i\right) = \sum_{i=1}^n \text{Var}(X_i) + 2\sum_{i<j} \text{Cov}(X_i, X_j)\]
</p>

<p>
Quando \(X\) e \(Y\) sono indipendenti, la loro covarianza è 0 e quindi:
\[\text{Var}(X+Y) = \text{Var}(X) + \text{Var}(Y)\]
</p>

<p>
Quindi la <a href="#org41a8e1b">varianza</a> è additiva solo quando le variabili sono indipendenti.
</p>
</div>
</div>
<div id="outline-container-org92f0dee" class="outline-4">
<h4 id="org92f0dee">Covarianza di funzioni indicatrici</h4>
<div class="outline-text-4" id="text-org92f0dee">
<p>
Se consideriamo due variabili aleatorie uguali alle funzioni indicatrici di due insiemi, quindi \(X=I_A\) e \(Y=I_B\), allora:
</p>

<p>
\[E(X) = P(X=1) = P(A)\]
\[E(Y) = P(Y=1) = P(B)\]
\[E(XY) = P(XY=1) = P(X=1, Y=1) = P(A \cap B)\]
</p>

<p>
La covarianza diventa:
\[\text{Cov}(X,Y) = E(XY) - E(X)E(Y) = P(A \cap B) - P(A)P(B)\]
</p>

<p>
Osservazione: \(\text{Cov}(X,Y) > 0\) quando \(P(A \cap B) > P(A)P(B)\), ovvero quando:
\[\frac{P(A \cap B)}{P(B)} > P(A)\]
</p>

<p>
Notando che \(\frac{P(A \cap B)}{P(B)} = P(A|B)\), la covarianza è positiva quando \(P(A|B) > P(A)\), cioè quando sapere che \(B\) è vero aumenta la probabilità che \(A\) sia vero. Questo indica che gli eventi sono positivamente correlati.
</p>
</div>
</div>
<div id="outline-container-orgebfc0f0" class="outline-4">
<h4 id="orgebfc0f0">Coefficiente di correlazione</h4>
<div class="outline-text-4" id="text-orgebfc0f0">
<p>
Poiché il valore della covarianza dipende fortemente dalle unità di misura delle variabili, viene introdotto il coefficiente di correlazione che "normalizza" la covarianza:
</p>

<p>
\[\rho_{X,Y} = \frac{\text{Cov}(X,Y)}{\sqrt{\text{Var}(X) \cdot \text{Var}(Y)}} = \frac{\text{Cov}(X,Y)}{\sigma_X \cdot \sigma_Y}\]
</p>

<p>
Il coefficiente di correlazione \(\rho_{X,Y}\) è sempre compreso tra -1 e 1:
</p>
<ul class="org-ul">
<li>\(\rho_{X,Y} = 1\): correlazione positiva perfetta</li>
<li>\(\rho_{X,Y} = -1\): correlazione negativa perfetta</li>
<li>\(\rho_{X,Y} = 0\): nessuna correlazione lineare</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgfe1ac5a" class="outline-3">
<h3 id="orgfe1ac5a">Variabili aleatorie continue</h3>
<div class="outline-text-3" id="text-orgfe1ac5a">
<p>
Nelle variabili aleatorie a supporto continuo, al posto della funzione di massa di probabilità si ha la funzione di densità di probabilità, \(f_X: \mathbb{R} \to \mathbb{R}^+\) tale che per ogni insieme misurabile \(B \subset \mathbb{R}\):
\[P(X \in B) = \int_B f_X(x) \, dx\]
</p>

<p>
ovvero per ogni intervallo \(B\), si considera la probabilità che la variabile assuma valori in quell'intervallo.
</p>

<p>
Proprietà fondamentali della densità di probabilità:
</p>

<ol class="org-ol">
<li>\(f_X(x) \geq 0\) per ogni \(x \in \mathbb{R}\)</li>
<li>\(\int_{-\infty}^{+\infty} f_X(x) \, dx = 1\)</li>
<li>La probabilità che una singola specificazione sia ottenuta è 0, quindi \(P(X = x) = 0\)
per qualsiasi \(x\). Per questo motivo non importa se gli intervalli siano aperti o chiusi.</li>
</ol>

<p>
La funzione di ripartizione per variabili continue è:
\[F_X(a) = P(X \leq a) = \int_{-\infty}^{a} f_X(x) \, dx\]
</p>

<p>
Per il teorema fondamentale del calcolo integrale, vale che:
\[f_X(x) = \frac{d}{dx}F_X(x)\]
</p>

<p>
Questo ha senso intuitivamente perché la densità rappresenta il tasso di variazione della funzione di ripartizione.
</p>
</div>
</div>
<div id="outline-container-org89947d1" class="outline-3">
<h3 id="org89947d1">Disuguaglianze probabilistiche fondamentali</h3>
<div class="outline-text-3" id="text-org89947d1">
</div>
<div id="outline-container-org98ebc0b" class="outline-4">
<h4 id="org98ebc0b">Disuguaglianza di Markov</h4>
<div class="outline-text-4" id="text-org98ebc0b">
<p>
<b>Teorema (Disuguaglianza di Markov):</b> Se \(X\) è una variabile aleatoria non negativa (cioè \(X \geq 0\)), allora per ogni costante \(a > 0\):
\[P(X \geq a) \leq \frac{E[X]}{a}\]
</p>

<p>
<b>Dimostrazione:</b> 
Partiamo dalla definizione del valore atteso:
\[E[X] = \int_{0}^{\infty} x \cdot f_X(x) \, dx\]
</p>

<p>
Possiamo dividere l'integrale in due parti:
\[E[X] = \int_{0}^{a} x \cdot f_X(x) \, dx + \int_{a}^{\infty} x \cdot f_X(x) \, dx\]
</p>

<p>
Dato che \(X\) è non negativa, entrambi gli addendi sono non negativi, quindi:
\[E[X] \geq \int_{a}^{\infty} x \cdot f_X(x) \, dx\]
</p>

<p>
Nell'intervallo \([a, \infty)\), ogni valore di \(x\) è maggiore o uguale ad \(a\), quindi:
\[\int_{a}^{\infty} x \cdot f_X(x) \, dx \geq \int_{a}^{\infty} a \cdot f_X(x) \, dx = a \cdot \int_{a}^{\infty} f_X(x) \, dx = a \cdot P(X \geq a)\]
</p>

<p>
Combinando queste disuguaglianze:
\[E[X] \geq a \cdot P(X \geq a)\]
</p>

<p>
Da cui:
\[P(X \geq a) \leq \frac{E[X]}{a}\]
</p>

<p>
<b>Interpretazione:</b> La disuguaglianza di Markov fornisce un limite superiore alla probabilità che una variabile aleatoria non negativa superi un certo valore. Ad esempio, per una variabile con valore atteso 10, la probabilità che superi 100 è al massimo 10/100 = 0.1 o 10%.
</p>
</div>
</div>
<div id="outline-container-org16eba75" class="outline-4">
<h4 id="org16eba75">Disuguaglianza di Chebyshev</h4>
<div class="outline-text-4" id="text-org16eba75">
<p>
<b>Teorema (Disuguaglianza di Chebyshev):</b> Data una variabile aleatoria \(X\) con valore atteso \(E[X] = \mu\) e <a href="#org41a8e1b">varianza</a> \(Var(X) = \sigma^2\), allora per ogni \(r > 0\):
\[P(|X-\mu| \geq r) \leq \frac{\sigma^2}{r^2}\]
</p>

<p>
<b>Dimostrazione:</b>
Osserviamo che:
\[|X-\mu| \geq r \Rightarrow (X-\mu)^2 \geq r^2\]
</p>

<p>
Poiché \(r > 0\), vale anche l'implicazione contraria, quindi:
\[P(|X-\mu| \geq r) = P((X-\mu)^2 \geq r^2)\]
</p>

<p>
Definiamo \(Y := (X-\mu)^2\). Notiamo che \(Y\) è una variabile aleatoria non negativa con \(E[Y] = Var(X) = \sigma^2\). 
</p>

<p>
Applicando la disuguaglianza di Markov a \(Y\):
\[P(Y \geq r^2) \leq \frac{E[Y]}{r^2} = \frac{Var(X)}{r^2} = \frac{\sigma^2}{r^2}\]
</p>

<p>
Sostituendo:
\[P(|X-\mu| \geq r) \leq \frac{\sigma^2}{r^2}\]
</p>

<p>
<b>Caso particolare:</b> Se consideriamo \(r = k\sigma\) (cioè misuriamo la distanza in termini di deviazioni standard), otteniamo:
\[P(|X-\mu| \geq k\sigma) \leq \frac{1}{k^2}\]
</p>

<p>
<b>Interpretazione:</b> La disuguaglianza di Chebyshev fornisce un limite superiore alla probabilità che una variabile aleatoria si discosti dal suo valore atteso di una certa quantità. Ad esempio:
</p>
<ul class="org-ul">
<li>La probabilità di deviare di almeno 2 deviazioni standard è al massimo 1/4 = 25%</li>
<li>La probabilità di deviare di almeno 3 deviazioni standard è al massimo 1/9 ≈ 11.1%</li>
<li>La probabilità di deviare di almeno 4 deviazioni standard è al massimo 1/16 = 6.25%</li>
</ul>

<p>
Questo ci fornisce un nuovo modo di interpretare la <a href="#org41a8e1b">varianza</a>: non solo come misura di <a href="#org22836a9">dispersione</a>, ma anche come unità di misura per la distanza dal valore atteso. La probabilità che un valore si discosti di \(k\) deviazioni standard dal valore atteso decresce almeno proporzionalmente a \(1/k^2\).
</p>
</div>
</div>
<div id="outline-container-org88f7a28" class="outline-4">
<h4 id="org88f7a28">Legge dei grandi numeri</h4>
<div class="outline-text-4" id="text-org88f7a28">
<p>
<b>Teorema (Legge debole dei grandi numeri):</b> Sia \(X_1, X_2, ..., X_n\) una sequenza di variabili aleatorie indipendenti e identicamente distribuite, ciascuna con valore atteso \(\mu\) e <a href="#org41a8e1b">varianza</a> \(\sigma^2\). Definiamo la <a href="#org440f691">media</a> campionaria \(\overline{X}_n = \frac{1}{n}\sum_{i=1}^n X_i\). Allora, per ogni \(\varepsilon > 0\):
</p>

<p>
\[\lim_{n \to \infty} P(|\overline{X}_n - \mu| < \varepsilon) = 1\]
</p>

<p>
Ovvero, la <a href="#org440f691">media</a> campionaria converge in probabilità al valore atteso \(\mu\).
</p>

<p>
<b>Dimostrazione (usando Chebyshev):</b>
Poiché le variabili sono indipendenti e identicamente distribuite:
</p>
<ul class="org-ul">
<li>\(E[\overline{X}_n] = \mu\)</li>
<li>\(Var(\overline{X}_n) = \frac{\sigma^2}{n}\)</li>
</ul>

<p>
Applicando la disuguaglianza di Chebyshev:
\[P(|\overline{X}_n - \mu| \geq \varepsilon) \leq \frac{Var(\overline{X}_n)}{\varepsilon^2} = \frac{\sigma^2}{n\varepsilon^2}\]
</p>

<p>
Quando \(n \to \infty\), questo limite tende a 0, quindi:
\[\lim_{n \to \infty} P(|\overline{X}_n - \mu| \geq \varepsilon) = 0\]
</p>

<p>
Quindi:
\[\lim_{n \to \infty} P(|\overline{X}_n - \mu| < \varepsilon) = 1\]
</p>

<p>
<b>Interpretazione:</b> La legge dei grandi numeri afferma che, con un numero sufficientemente grande di osservazioni, la <a href="#org440f691">media</a> campionaria di variabili aleatorie indipendenti e identicamente distribuite si avvicina al loro valore atteso. Ad esempio, lanciando una moneta equilibrata molte volte, la proporzione di teste si avvicinerà a 0.5.
</p>
</div>
</div>
</div>
<div id="outline-container-org74800df" class="outline-3">
<h3 id="org74800df">Modelli di variabili aleatorie</h3>
<div class="outline-text-3" id="text-org74800df">
</div>
<div id="outline-container-org57a32f3" class="outline-4">
<h4 id="org57a32f3">Modello di Bernoulli</h4>
<div class="outline-text-4" id="text-org57a32f3">
<p>
<b>Definizione (Distribuzione di Bernoulli):</b> Una variabile aleatoria \(X\) segue una distribuzione di Bernoulli con parametro \(p\) (scriviamo \(X \sim \text{Bern}(p)\)) se rappresenta un esperimento con esito binario, dove:
</p>
<ul class="org-ul">
<li>\(X = 1\) rappresenta il "successo" con probabilità \(p\)</li>
<li>\(X = 0\) rappresenta il "fallimento" con probabilità \(1-p\)</li>
</ul>

<p>
La funzione di massa di probabilità è:
\[p_X(x) = p^x (1-p)^{1-x} \cdot I_{\{0,1\}}(x)\]
</p>

<p>
dove \(I_{\{0,1\}}(x)\) è la funzione indicatrice dell'insieme \(\{0,1\}\).
</p>

<p>
<b>Proprietà principali:</b>
</p>
<ul class="org-ul">
<li><b>Valore atteso:</b> \(E[X] = p\)</li>
<li><b><a href="#org41a8e1b">Varianza</a>:</b> \(Var(X) = p(1-p)\)</li>
</ul>

<p>
<b>Dimostrazione della <a href="#org41a8e1b">varianza</a>:</b>
</p>
\begin{align}
Var(X) &= E[X^2] - E[X]^2 \\
&= E[X] - E[X]^2 \quad \text{(usando l'idempotenza: } X^2 = X \text{ per } X \in \{0,1\}\text{)} \\
&= p - p^2 \\
&= p(1-p)
\end{align}

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
<span style="color: #a020f0;">import</span> matplotlib.pyplot <span style="color: #a020f0;">as</span> plt
<span style="color: #a020f0;">import</span> scipy.stats <span style="color: #a020f0;">as</span> st

<span style="color: #b22222;"># </span><span style="color: #b22222;">Creazione di una distribuzione di Bernoulli</span>
<span style="color: #a0522d;">p</span> = 0.3  <span style="color: #b22222;"># </span><span style="color: #b22222;">parametro della distribuzione</span>
<span style="color: #a0522d;">bernoulli</span> = st.bernoulli(p)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo della funzione di massa di probabilit&#224;</span>
<span style="color: #a0522d;">x</span> = np.array([0, 1])
<span style="color: #a0522d;">pmf_values</span> = bernoulli.pmf(x)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X=0) = </span>{pmf_values[0]}<span style="color: #8b2252;">"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X=1) = </span>{pmf_values[1]}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo della funzione di ripartizione</span>
<span style="color: #a0522d;">cdf_values</span> = bernoulli.cdf(x)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X&#8804;0) = </span>{cdf_values[0]}<span style="color: #8b2252;">"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X&#8804;1) = </span>{cdf_values[1]}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Generazione di variabili aleatorie</span>
<span style="color: #a0522d;">random_sample</span> = bernoulli.rvs(size=1000)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Media campionaria: </span>{random_sample.mean()}<span style="color: #8b2252;">"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Varianza campionaria: </span>{random_sample.var()}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo dei momenti</span>
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Valore atteso teorico: </span>{bernoulli.mean()}<span style="color: #8b2252;">"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Varianza teorica: </span>{bernoulli.var()}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Visualizzazione della distribuzione</span>
plt.figure(figsize=(8, 4))
plt.bar(x, pmf_values, width=0.2)
plt.xlabel(<span style="color: #8b2252;">'x'</span>)
plt.ylabel(<span style="color: #8b2252;">'P(X=x)'</span>)
plt.title(f<span style="color: #8b2252;">'Distribuzione di Bernoulli con p=</span>{p}<span style="color: #8b2252;">'</span>)
plt.xticks([0, 1])
plt.grid(alpha=0.3)
plt.show()
</pre>
</div>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
<span style="color: #a020f0;">import</span> matplotlib.pyplot <span style="color: #a020f0;">as</span> plt
<span style="color: #a020f0;">import</span> scipy.stats <span style="color: #a020f0;">as</span> st

<span style="color: #b22222;"># </span><span style="color: #b22222;">Creazione di una distribuzione di Bernoulli</span>
<span style="color: #a0522d;">p</span> = 0.3  <span style="color: #b22222;"># </span><span style="color: #b22222;">parametro della distribuzione</span>
<span style="color: #a0522d;">bernoulli</span> = st.bernoulli(p)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo della funzione di massa di probabilit&#224;</span>
<span style="color: #a0522d;">x</span> = np.array([0, 1])
<span style="color: #a0522d;">pmf_values</span> = bernoulli.pmf(x)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X=0) = </span>{pmf_values[0]}<span style="color: #8b2252;">"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X=1) = </span>{pmf_values[1]}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo della funzione di ripartizione</span>
<span style="color: #a0522d;">cdf_values</span> = bernoulli.cdf(x)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X&#8804;0) = </span>{cdf_values[0]}<span style="color: #8b2252;">"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X&#8804;1) = </span>{cdf_values[1]}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Generazione di variabili aleatorie</span>
<span style="color: #a0522d;">random_sample</span> = bernoulli.rvs(size=1000)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Media campionaria: </span>{random_sample.mean()}<span style="color: #8b2252;">"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Varianza campionaria: </span>{random_sample.var()}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo dei momenti</span>
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Valore atteso teorico: </span>{bernoulli.mean()}<span style="color: #8b2252;">"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Varianza teorica: </span>{bernoulli.var()}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Visualizzazione della distribuzione e della funzione di ripartizione</span>
plt.figure(figsize=(12, 5))

<span style="color: #b22222;"># </span><span style="color: #b22222;">Subplot per la PMF</span>
plt.subplot(1, 2, 1)
plt.bar(x, pmf_values, width=0.2)
plt.xlabel(<span style="color: #8b2252;">'x'</span>)
plt.ylabel(<span style="color: #8b2252;">'P(X=x)'</span>)
plt.title(f<span style="color: #8b2252;">'PMF - Bernoulli con p=</span>{p}<span style="color: #8b2252;">'</span>)
plt.xticks([0, 1])
plt.grid(alpha=0.3)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Subplot per la CDF</span>
plt.subplot(1, 2, 2)
<span style="color: #b22222;"># </span><span style="color: #b22222;">Definiamo i punti per la CDF</span>
<span style="color: #a0522d;">x_step</span> = [-0.5, 0, 1, 1.5]  <span style="color: #b22222;"># </span><span style="color: #b22222;">Punti x: prima di 0, a 0, a 1, dopo 1</span>
<span style="color: #a0522d;">y_step</span> = [0, 1-p, 1, 1]     <span style="color: #b22222;"># </span><span style="color: #b22222;">Valori CDF corrispondenti</span>
plt.step(x_step, y_step, <span style="color: #8b2252;">'r-'</span>, where=<span style="color: #8b2252;">'post'</span>, lw=2, label=<span style="color: #8b2252;">'CDF'</span>)
<span style="color: #b22222;"># </span><span style="color: #b22222;">Aggiungiamo i punti per evidenziare le discontinuit&#224;</span>
plt.plot([0, 0], [0, 1-p], <span style="color: #8b2252;">'k--'</span>, alpha=0.3)  <span style="color: #b22222;"># </span><span style="color: #b22222;">Linea verticale a x=0</span>
plt.plot([1, 1], [1-p, 1], <span style="color: #8b2252;">'k--'</span>, alpha=0.3)  <span style="color: #b22222;"># </span><span style="color: #b22222;">Linea verticale a x=1</span>
plt.scatter([0, 1], [1-p, 1], color=<span style="color: #8b2252;">'red'</span>, s=50, zorder=3)  <span style="color: #b22222;"># </span><span style="color: #b22222;">Punti inclusi</span>
plt.scatter([0, 1], [0, 1-p], facecolors=<span style="color: #8b2252;">'white'</span>, edgecolors=<span style="color: #8b2252;">'red'</span>, s=50, zorder=3)  <span style="color: #b22222;"># </span><span style="color: #b22222;">Punti esclusi</span>
plt.xlabel(<span style="color: #8b2252;">'x'</span>)
plt.ylabel(<span style="color: #8b2252;">'P(X&#8804;x)'</span>)
plt.title(f<span style="color: #8b2252;">'CDF - Bernoulli con p=</span>{p}<span style="color: #8b2252;">'</span>)
plt.xticks([-0.5, 0, 1, 1.5])
plt.yticks([0, 1-p, 1])
plt.grid(alpha=0.3)
plt.legend()

plt.tight_layout()
plt.show()
</pre>
</div>
</div>
</div>
<div id="outline-container-org0c4b5a7" class="outline-4">
<h4 id="org0c4b5a7">Binomiale</h4>
<div class="outline-text-4" id="text-org0c4b5a7">
<p>
Supporto discreto.
Modella un esperimento che consiste in \(n\) sottoesperimenti bernoulliani di parametro \(p\) indipendenti fra loro.
Indicato con \(X \sim B(n,p)\)
</p>

<p>
Riguardo la funzione di massa di probabilità, si consideri che la probabilità che una singola serie di \(n\) esperimenti abbia \(i\) successi e \(n-i\) insuccessi è \(p^i(1-p)^{(n-i)}\). In totale, ci sono \(\binom{n}{i}\) combinazioni con \(i\) successi e quindi ottengo:
\[p_X(i) = \binom{n}{i} p^i (1-p)^{n-i} \mathbb{I}_{\{0,1,...,n\}}(i)\]
</p>

<p>
Dimostrazione che la somma è 1:
\[\sum_{i=0}^n \binom{n}{i} p^i (1-p)^{n-i} = (p+(1-p))^n = 1^n = 1\]
nel secondo step uso il binomio di Newton:
\[\sum_{i=0}^n \binom{n}{i} a^i b^{n-i} = (a+b)^n\]
</p>

<p>
Per il valore atteso, se applico la definizione esce una sommatoria difficile da calcolare.
Invece, considero le variabili bernoulliane \(X_i\) che compongono la binomiale e uso la linearità del valore atteso, quindi:
\[E(X) = E\left(\sum X_i\right) = \sum E(X_i) = \sum p = np\]
</p>

<p>
Stesso per la <a href="#org41a8e1b">varianza</a>, sfruttando l'indipendenza per usare la linearità della <a href="#org41a8e1b">varianza</a>, quindi il risultato è:
\[\text{Var}(X) = np(1-p)\]
</p>

<p>
La funzione di ripartizione è:
\[P(X \leq x) = \sum_{i=0}^{\lfloor x \rfloor} \binom{n}{i} p^i (1-p)^{n-i} \mathbb{I}_{[0,n]}(x) + \mathbb{I}_{(n,\infty)}(x)\]
</p>

<p>
Se \(X_1 \sim B(n,p)\) e \(X_2 \sim B(m,p)\) sono binomiali e indipendenti, allora:
\[(X_1 + X_2) \sim B(n+m,p)\]
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
<span style="color: #a020f0;">import</span> matplotlib.pyplot <span style="color: #a020f0;">as</span> plt
<span style="color: #a020f0;">import</span> scipy.stats <span style="color: #a020f0;">as</span> st

<span style="color: #b22222;"># </span><span style="color: #b22222;">Parametri della distribuzione binomiale</span>
<span style="color: #a0522d;">n</span> = 10  <span style="color: #b22222;"># </span><span style="color: #b22222;">numero di prove</span>
<span style="color: #a0522d;">p</span> = 0.3  <span style="color: #b22222;"># </span><span style="color: #b22222;">probabilit&#224; di successo</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Creazione dell'oggetto distribuzione binomiale</span>
<span style="color: #a0522d;">binom</span> = st.binom(n, p)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo della funzione di massa di probabilit&#224;</span>
<span style="color: #a0522d;">x</span> = np.arange(0, n+1)
<span style="color: #a0522d;">pmf_values</span> = binom.pmf(x)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Funzione personalizzata per calcolare la PMF</span>
<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">p_binom</span>(k, n, p):
    <span style="color: #8b2252;">"""Calcola la PMF di una binomiale"""</span>
    <span style="color: #a020f0;">from</span> scipy.special <span style="color: #a020f0;">import</span> comb
    <span style="color: #a020f0;">if</span> k &lt; 0 <span style="color: #a020f0;">or</span> k &gt; n:
        <span style="color: #a020f0;">return</span> 0
    <span style="color: #a020f0;">return</span> comb(n, k) * p**k * (1-p)**(n-k)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Verifica</span>
<span style="color: #483d8b;">print</span>(<span style="color: #8b2252;">"PMF calcolata manualmente:"</span>)
<span style="color: #a020f0;">for</span> k <span style="color: #a020f0;">in</span> <span style="color: #483d8b;">range</span>(n+1):
    <span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X=</span>{k}<span style="color: #8b2252;">) = </span>{p_binom(k, n, p)}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo della funzione di ripartizione</span>
<span style="color: #a0522d;">cdf_values</span> = binom.cdf(x)
<span style="color: #483d8b;">print</span>(<span style="color: #8b2252;">"</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">Valori della funzione di ripartizione:"</span>)
<span style="color: #a020f0;">for</span> k <span style="color: #a020f0;">in</span> <span style="color: #483d8b;">range</span>(n+1):
    <span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X&#8804;</span>{k}<span style="color: #8b2252;">) = </span>{cdf_values[k]}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo dei momenti</span>
<span style="color: #a0522d;">mean</span> = binom.mean()
<span style="color: #a0522d;">var</span> = binom.var()
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">Valore atteso: </span>{mean}<span style="color: #8b2252;"> (teoricamente: </span>{n*p}<span style="color: #8b2252;">)"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Varianza: </span>{var}<span style="color: #8b2252;"> (teoricamente: </span>{n*p*(1-p)}<span style="color: #8b2252;">)"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Generazione di un campione di variabili aleatorie binomiali</span>
<span style="color: #a0522d;">random_sample</span> = binom.rvs(size=1000)
<span style="color: #a0522d;">sample_mean</span> = random_sample.mean()
<span style="color: #a0522d;">sample_var</span> = random_sample.var()
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">Media campionaria: </span>{sample_mean}<span style="color: #8b2252;">"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Varianza campionaria: </span>{sample_var}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Visualizzazione della distribuzione</span>
plt.figure(figsize=(10, 5))
plt.bar(x, pmf_values, width=0.4)
plt.plot(x, cdf_values, <span style="color: #8b2252;">'r-o'</span>, alpha=0.5, label=<span style="color: #8b2252;">'Funzione di ripartizione'</span>)
plt.xlabel(<span style="color: #8b2252;">'Numero di successi'</span>)
plt.ylabel(<span style="color: #8b2252;">'Probabilit&#224;'</span>)
plt.title(f<span style="color: #8b2252;">'Distribuzione Binomiale B(</span>{n}<span style="color: #8b2252;">, </span>{p}<span style="color: #8b2252;">)'</span>)
plt.grid(alpha=0.3)
plt.legend()
plt.show()
</pre>
</div>
</div>
</div>
<div id="outline-container-org1e8f7e9" class="outline-4">
<h4 id="org1e8f7e9">Geometrico (guardare anche dispensa)</h4>
<div class="outline-text-4" id="text-org1e8f7e9">
<p>
Modello discreto che parte da bernoulli.
Conta quante ripetizioni servono prima che avvenga un successo.
</p>
</div>
<div id="outline-container-orgc76f5cc" class="outline-5">
<h5 id="orgc76f5cc">Definizione e concetti base</h5>
<div class="outline-text-5" id="text-orgc76f5cc">
<p>
La distribuzione geometrica descrive il numero di insuccessi necessari affinché si verifichi il primo successo in una successione di esperimenti bernoulliani indipendenti e identicamente distribuiti.
</p>

<ul class="org-ul">
<li>Parametro: \(p\) = probabilità di successo in un singolo esperimento</li>
<li>Variabile aleatoria: \(X\) = numero di insuccessi prima del primo successo</li>
<li>Supporto: \(X \in \mathbb{N} \cup \{0\}\)</li>
</ul>

<p>
<b>Nota:</b> Esiste una definizione alternativa che conta il numero totale di esperimenti fino al primo successo, in quel caso \(Y = X + 1\) e \(Y \in \mathbb{N}^+\).
</p>
</div>
</div>
<div id="outline-container-orgf519898" class="outline-5">
<h5 id="orgf519898">Funzione di massa di probabilità</h5>
<div class="outline-text-5" id="text-orgf519898">
<p>
La funzione di massa di probabilità è data da:
</p>

<p>
\[f_X(i; p) = p(1-p)^i \mathbb{I}_{\mathbb{N} \cup \{0\}}(i)\]
</p>

<p>
Implementazione in Python:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">def</span> <span style="color: #0000ff;">geom_pdf</span>(x, p):
    <span style="color: #a020f0;">assert</span> p &gt; 0 <span style="color: #a020f0;">and</span> p &lt;= 1, <span style="color: #8b2252;">'{} is not a valid parameter for the geometric distribution.'</span>.<span style="color: #483d8b;">format</span>(p)
    <span style="color: #a020f0;">return</span> p * (1 - p)**x <span style="color: #a020f0;">if</span> x==<span style="color: #483d8b;">int</span>(x) <span style="color: #a020f0;">and</span> x &gt;= 0 <span style="color: #a020f0;">else</span> 0
</pre>
</div>
</div>
</div>
<div id="outline-container-orga4968ae" class="outline-5">
<h5 id="orga4968ae">Valore atteso</h5>
<div class="outline-text-5" id="text-orga4968ae">
<p>
Il valore atteso della distribuzione geometrica è:
</p>

<p>
\[E[X] = \frac{1-p}{p}\]
</p>

<p>
<b>Dimostrazione:</b> Utilizzando la serie geometrica \(\sum_{i=0}^{+\infty} \alpha^i = \frac{1}{1-\alpha}\) per \(|\alpha| < 1\) e le sue derivate.
</p>

\begin{align}
E[X] &= \sum_{i=0}^{+\infty} i \cdot p(1-p)^i \\
&= p \sum_{i=1}^{+\infty} i \cdot (1-p)^i \\
&= p(1-p) \sum_{i=1}^{+\infty} i \cdot (1-p)^{i-1} \\
&= p(1-p) \frac{d}{d(1-p)} \sum_{i=0}^{+\infty} (1-p)^i \\
&= p(1-p) \frac{d}{d(1-p)} \frac{1}{1-(1-p)} \\
&= p(1-p) \frac{d}{d(1-p)} \frac{1}{p} \\
&= p(1-p) \cdot \frac{1}{p^2} \\
&= \frac{1-p}{p}
\end{align}
</div>
</div>
<div id="outline-container-org732100c" class="outline-5">
<h5 id="org732100c"><a href="#org41a8e1b">Varianza</a></h5>
<div class="outline-text-5" id="text-org732100c">
<p>
La <a href="#org41a8e1b">varianza</a> della distribuzione geometrica è:
</p>

<p>
\[\text{Var}(X) = \frac{1-p}{p^2}\]
</p>

<p>
<b>Nota:</b> All'aumentare di \(p\), sia il valore atteso che la <a href="#org41a8e1b">varianza</a> diminuiscono, coerentemente con l'interpretazione probabilistica (maggiore probabilità di successo = minor numero atteso di insuccessi).
</p>
</div>
</div>
<div id="outline-container-org316685e" class="outline-5">
<h5 id="org316685e">Funzione di ripartizione</h5>
<div class="outline-text-5" id="text-org316685e">
<p>
La funzione di ripartizione ha una forma analitica calcolabile:
</p>

<p>
\[F_X(x; p) = \left(1 - (1-p)^{\lfloor x \rfloor + 1}\right) \mathbb{I}_{[0, +\infty]}(x)\]
</p>

<p>
Implementazione in Python:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">def</span> <span style="color: #0000ff;">geom_cdf</span>(x, p):
    <span style="color: #a020f0;">assert</span> p &gt; 0 <span style="color: #a020f0;">and</span> p &lt;= 1, <span style="color: #8b2252;">'{} is not a valid parameter for the geometric distribution.'</span>.<span style="color: #483d8b;">format</span>(p)
    <span style="color: #a020f0;">return</span> 1 - (1-p)**(<span style="color: #483d8b;">int</span>(x) + 1) <span style="color: #a020f0;">if</span> x &gt;= 0 <span style="color: #a020f0;">else</span> 0
</pre>
</div>
</div>
</div>
<div id="outline-container-org698e5cf" class="outline-5">
<h5 id="org698e5cf">Proprietà di assenza di memoria</h5>
<div class="outline-text-5" id="text-org698e5cf">
<p>
Una caratteristica fondamentale della distribuzione geometrica è l'assenza di memoria:
</p>

<p>
\[P(X \geq x+y | X \geq x) = P(X \geq y)\]
</p>

<p>
Questo significa che la probabilità di dover attendere altri \(y\) insuccessi non dipende da quanti insuccessi si sono già verificati.
</p>

<p>
<b>Dimostrazione:</b>
</p>
\begin{align}
P(X \geq x+y | X \geq x) &= \frac{P(X \geq x+y \cap X \geq x)}{P(X \geq x)} \\
&= \frac{P(X \geq x+y)}{P(X \geq x)} \\
&= \frac{(1-p)^{x+y}}{(1-p)^x} \\
&= (1-p)^y \\
&= P(X \geq y)
\end{align}
</div>
</div>
<div id="outline-container-org2cc6ff4" class="outline-5">
<h5 id="org2cc6ff4">Relazione con altre distribuzioni</h5>
<div class="outline-text-5" id="text-org2cc6ff4">
<ul class="org-ul">
<li>La distribuzione geometrica è un caso particolare della distribuzione binomiale negativa</li>
<li>Se \(X \sim \text{Geom}(p)\), allora \(Y = X + 1\) segue la distribuzione del numero di prove fino al primo successo</li>
<li>La somma di \(n\) variabili geometriche indipendenti con lo stesso parametro \(p\) segue una distribuzione binomiale negativa</li>
</ul>
</div>
</div>
<div id="outline-container-org0942342" class="outline-5">
<h5 id="org0942342">Applicazioni</h5>
<div class="outline-text-5" id="text-org0942342">
<p>
La distribuzione geometrica modella efficacemente:
</p>
<ul class="org-ul">
<li>Il numero di tentativi prima di ottenere un successo</li>
<li>Il tempo di attesa (discreto) fino alla prima occorrenza di un evento raro</li>
<li>Fenomeni di affidabilità in cui si attende il primo guasto</li>
</ul>

<p>
<b>Esempio:</b> Se la probabilità di vincere alla lotteria è \(p = 0.001\), il numero atteso di biglietti da acquistare prima di vincere è \(E[X+1] = 1/p = 1000\).
</p>
</div>
</div>
<div id="outline-container-orgff8a2ba" class="outline-5">
<h5 id="orgff8a2ba">Python</h5>
<div class="outline-text-5" id="text-orgff8a2ba">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
<span style="color: #a020f0;">import</span> matplotlib.pyplot <span style="color: #a020f0;">as</span> plt
<span style="color: #a020f0;">import</span> scipy.stats <span style="color: #a020f0;">as</span> st
<span style="color: #a020f0;">from</span> ipywidgets <span style="color: #a020f0;">import</span> interact, FloatSlider
<span style="color: #a020f0;">import</span> matplotlib.patches <span style="color: #a020f0;">as</span> patches

<span style="color: #b22222;"># </span><span style="color: #b22222;">Funzione di massa di probabilit&#224; per la distribuzione geometrica</span>
<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">geom_pdf</span>(x, p):
    <span style="color: #a020f0;">assert</span> p &gt; 0 <span style="color: #a020f0;">and</span> p &lt;= 1, <span style="color: #8b2252;">'{} non &#232; un parametro valido per la distribuzione geometrica'</span>.<span style="color: #483d8b;">format</span>(p)
    <span style="color: #a020f0;">return</span> p * (1 - p)**x <span style="color: #a020f0;">if</span> x==<span style="color: #483d8b;">int</span>(x) <span style="color: #a020f0;">and</span> x &gt;= 0 <span style="color: #a020f0;">else</span> 0

<span style="color: #b22222;"># </span><span style="color: #b22222;">Funzione di ripartizione per la distribuzione geometrica</span>
<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">geom_cdf</span>(x, p):
    <span style="color: #a020f0;">assert</span> p &gt; 0 <span style="color: #a020f0;">and</span> p &lt;= 1, <span style="color: #8b2252;">'{} non &#232; un parametro valido per la distribuzione geometrica'</span>.<span style="color: #483d8b;">format</span>(p)
    <span style="color: #a020f0;">return</span> 1 - (1-p)**(<span style="color: #483d8b;">int</span>(x) + 1) <span style="color: #a020f0;">if</span> x &gt;= 0 <span style="color: #a020f0;">else</span> 0

<span style="color: #b22222;"># </span><span style="color: #b22222;">Creazione di una distribuzione geometrica usando scipy.stats</span>
<span style="color: #a0522d;">p</span> = 0.3
<span style="color: #a0522d;">geom_dist</span> = st.geom(p)  <span style="color: #b22222;"># </span><span style="color: #b22222;">Nota: scipy.stats usa la definizione alternativa (Y = X + 1)</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo valori PMF e CDF usando funzioni personalizzate e scipy</span>
<span style="color: #a0522d;">x_values</span> = np.arange(0, 10)
<span style="color: #a0522d;">pdf_custom</span> = [geom_pdf(x, p) <span style="color: #a020f0;">for</span> x <span style="color: #a020f0;">in</span> x_values]
<span style="color: #a0522d;">cdf_custom</span> = [geom_cdf(x, p) <span style="color: #a020f0;">for</span> x <span style="color: #a020f0;">in</span> x_values]

<span style="color: #b22222;"># </span><span style="color: #b22222;">Nota: Per confrontare con scipy, dobbiamo modificare i valori poich&#233;</span>
<span style="color: #b22222;"># </span><span style="color: #b22222;">scipy.stats.geom usa la definizione Y = X + 1</span>
<span style="color: #a0522d;">pdf_scipy</span> = geom_dist.pmf(x_values + 1)
<span style="color: #a0522d;">cdf_scipy</span> = geom_dist.cdf(x_values + 1) - geom_dist.pmf(0)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Verifica che le funzioni siano corrette</span>
<span style="color: #483d8b;">print</span>(<span style="color: #8b2252;">"Confronto tra PMF personalizzata e scipy.stats:"</span>)
<span style="color: #a020f0;">for</span> i, x <span style="color: #a020f0;">in</span> <span style="color: #483d8b;">enumerate</span>(x_values):
    <span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X=</span>{x}<span style="color: #8b2252;">) = </span>{pdf_custom[i]:.6f}<span style="color: #8b2252;"> (custom) vs </span>{pdf_scipy[i]:.6f}<span style="color: #8b2252;"> (scipy)"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Simulazione di valori di una geometrica</span>
<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">geom_sim</span>(p, size=1):
    <span style="color: #8b2252;">"""Simula valori da una distribuzione geometrica"""</span>
    <span style="color: #a0522d;">result</span> = []
    <span style="color: #a020f0;">for</span> _ <span style="color: #a020f0;">in</span> <span style="color: #483d8b;">range</span>(size):
        <span style="color: #a0522d;">count</span> = 0
        <span style="color: #a020f0;">while</span> np.random.random() &gt;= p:
            <span style="color: #a0522d;">count</span> += 1
        result.append(count)
    <span style="color: #a020f0;">return</span> np.array(result)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Simulazione usando la funzione personalizzata</span>
<span style="color: #a0522d;">sim_sample_custom</span> = geom_sim(p, 1000)
<span style="color: #a0522d;">sim_mean_custom</span> = sim_sample_custom.mean()
<span style="color: #a0522d;">sim_var_custom</span> = sim_sample_custom.var()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Simulazione usando scipy.stats (correggiamo per la definizione)</span>
<span style="color: #a0522d;">sim_sample_scipy</span> = geom_dist.rvs(size=1000) - 1
<span style="color: #a0522d;">sim_mean_scipy</span> = sim_sample_scipy.mean()
<span style="color: #a0522d;">sim_var_scipy</span> = sim_sample_scipy.var()

<span style="color: #483d8b;">print</span>(<span style="color: #8b2252;">"</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">Statistiche dalle simulazioni:"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Media (custom): </span>{sim_mean_custom:.4f}<span style="color: #8b2252;"> vs Media teorica: </span>{(1-p)/p:.4f}<span style="color: #8b2252;">"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Varianza (custom): </span>{sim_var_custom:.4f}<span style="color: #8b2252;"> vs Varianza teorica: </span>{(1-p)/p**2:.4f}<span style="color: #8b2252;">"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Media (scipy): </span>{sim_mean_scipy:.4f}<span style="color: #8b2252;">"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Varianza (scipy): </span>{sim_var_scipy:.4f}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Funzione per visualizzare la distribuzione con parametro variabile</span>
<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">visualizza_geom_pdf</span>(p=0.5):
    <span style="color: #a0522d;">x</span> = np.arange(0, 10, 1)
    <span style="color: #a0522d;">avg</span> = (1 - p) / p
    <span style="color: #a0522d;">stdev</span> = np.sqrt((1-p)/p**2)

    plt.figure(figsize=(10, 6))

    <span style="color: #b22222;"># </span><span style="color: #b22222;">Aggiungi intervallo di confidenza</span>
    plt.gca().add_patch(patches.Rectangle(
        (avg-stdev, 0.95), 2*stdev, 0.05, edgecolor=<span style="color: #8b2252;">'None'</span>, facecolor=<span style="color: #8b2252;">'green'</span>
    ))
    plt.plot([avg, avg], [0.9, 1], color=<span style="color: #8b2252;">'green'</span>, label=f<span style="color: #8b2252;">'Media: </span>{avg:.2f}<span style="color: #8b2252;">'</span>)

    <span style="color: #b22222;"># </span><span style="color: #b22222;">Grafico a bastoncini della PMF</span>
    plt.vlines(x, [0]*<span style="color: #483d8b;">len</span>(x), [geom_pdf(_, p) <span style="color: #a020f0;">for</span> _ <span style="color: #a020f0;">in</span> x], color=<span style="color: #8b2252;">'blue'</span>)
    plt.plot(x, [geom_pdf(_, p) <span style="color: #a020f0;">for</span> _ <span style="color: #a020f0;">in</span> x], <span style="color: #8b2252;">'o'</span>, color=<span style="color: #8b2252;">'blue'</span>, label=<span style="color: #8b2252;">'PMF'</span>)

    <span style="color: #b22222;"># </span><span style="color: #b22222;">Aggiungi CDF</span>
    plt.plot(x, [geom_cdf(_, p) <span style="color: #a020f0;">for</span> _ <span style="color: #a020f0;">in</span> x], <span style="color: #8b2252;">'s--'</span>, color=<span style="color: #8b2252;">'red'</span>, alpha=0.7, label=<span style="color: #8b2252;">'CDF'</span>)

    plt.ylim(ymax=1, ymin=0)
    plt.xlim(xmax=11, xmin=-1)
    plt.grid(alpha=0.3)
    plt.title(f<span style="color: #8b2252;">'Distribuzione Geometrica con p=</span>{p}<span style="color: #8b2252;">'</span>)
    plt.xlabel(<span style="color: #8b2252;">'x (numero di insuccessi)'</span>)
    plt.ylabel(<span style="color: #8b2252;">'Probabilit&#224;'</span>)
    plt.legend()
    plt.show()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Visualizza la distribuzione con un controllo interattivo per p</span>
interact(visualizza_geom_pdf, p=FloatSlider(<span style="color: #483d8b;">min</span>=0.1, <span style="color: #483d8b;">max</span>=1.0, step=0.1, value=0.5))
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-org7036508" class="outline-4">
<h4 id="org7036508">Uniforme discreto</h4>
<div class="outline-text-4" id="text-org7036508">
<p>
Si immagini di avere \(n\in \mathbb{N}\) esiti equiprobabili numerati. Allora \(X\sim U(n)\) e 
\[p_X(i) = P(X=i)=\frac{1}{n}\mathbb{I}_{\{1,...,n\}}(i)\]
</p>

<p>
\[F_X(x)=P(X\leq x)=\sum_{i=1}^{\lfloor x \rfloor} P(X=i)\]
che non dipende da \(i\) e quindi 
\[F_X(x) = \frac{\lfloor x \rfloor}{n}\mathbb{I}_{[1,n[}(x) + \mathbb{I}_{[n,\infty)}(x)\]
</p>

<p>
\[E(X) = \frac{n+1}{2}\]
\[E(X^2) = \frac{(n+1)(2n+1)}{6}\]
\[Var(X) = E(X^2)-E(X)^2 = \frac{n^2-1}{12}\]
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
<span style="color: #a020f0;">import</span> matplotlib.pyplot <span style="color: #a020f0;">as</span> plt
<span style="color: #a020f0;">import</span> scipy.stats <span style="color: #a020f0;">as</span> st

<span style="color: #b22222;"># </span><span style="color: #b22222;">Parametri della distribuzione uniforme discreta</span>
<span style="color: #a0522d;">n</span> = 6  <span style="color: #b22222;"># </span><span style="color: #b22222;">come un dado a sei facce</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Creazione dell'oggetto distribuzione uniforme discreta</span>
<span style="color: #a0522d;">unif_disc</span> = st.randint(1, n+1)  <span style="color: #b22222;"># </span><span style="color: #b22222;">scipy.stats.randint usa [low, high)</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo della funzione di massa di probabilit&#224;</span>
<span style="color: #a0522d;">x</span> = np.arange(1, n+1)
<span style="color: #a0522d;">pmf_values</span> = unif_disc.pmf(x)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Funzione personalizzata per calcolare la PMF</span>
<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">p_unif_disc</span>(k, n):
    <span style="color: #8b2252;">"""Calcola la PMF di una uniforme discreta"""</span>
    <span style="color: #a020f0;">if</span> 1 &lt;= k &lt;= n <span style="color: #a020f0;">and</span> k == <span style="color: #483d8b;">int</span>(k):
        <span style="color: #a020f0;">return</span> 1/n
    <span style="color: #a020f0;">return</span> 0

<span style="color: #b22222;"># </span><span style="color: #b22222;">Verifica</span>
<span style="color: #483d8b;">print</span>(<span style="color: #8b2252;">"PMF calcolata manualmente:"</span>)
<span style="color: #a020f0;">for</span> k <span style="color: #a020f0;">in</span> <span style="color: #483d8b;">range</span>(1, n+1):
    <span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X=</span>{k}<span style="color: #8b2252;">) = </span>{p_unif_disc(k, n)}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo della funzione di ripartizione</span>
<span style="color: #a0522d;">cdf_values</span> = unif_disc.cdf(x)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Funzione personalizzata per calcolare la CDF</span>
<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">F_unif_disc</span>(x, n):
    <span style="color: #8b2252;">"""Calcola la CDF di una uniforme discreta"""</span>
    <span style="color: #a020f0;">if</span> x &lt; 1:
        <span style="color: #a020f0;">return</span> 0
    <span style="color: #a020f0;">elif</span> x &gt;= n:
        <span style="color: #a020f0;">return</span> 1
    <span style="color: #a020f0;">else</span>:
        <span style="color: #a020f0;">return</span> np.floor(x)/n

<span style="color: #b22222;"># </span><span style="color: #b22222;">Verifica della CDF</span>
<span style="color: #483d8b;">print</span>(<span style="color: #8b2252;">"</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">CDF calcolata manualmente:"</span>)
<span style="color: #a0522d;">check_points</span> = [0.5, 1, 1.5, 2, 4.7, 6, 7]
<span style="color: #a020f0;">for</span> x <span style="color: #a020f0;">in</span> check_points:
    <span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X&#8804;</span>{x}<span style="color: #8b2252;">) = </span>{F_unif_disc(x, n)}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo dei momenti</span>
<span style="color: #a0522d;">mean</span> = unif_disc.mean()
<span style="color: #a0522d;">var</span> = unif_disc.var()
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">Valore atteso: </span>{mean}<span style="color: #8b2252;"> (teoricamente: </span>{(n+1)/2}<span style="color: #8b2252;">)"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Varianza: </span>{var}<span style="color: #8b2252;"> (teoricamente: </span>{(n**2-1)/12}<span style="color: #8b2252;">)"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Generazione di un campione di variabili aleatorie uniformi discrete</span>
<span style="color: #a0522d;">random_sample</span> = unif_disc.rvs(size=1000)
<span style="color: #a0522d;">sample_mean</span> = random_sample.mean()
<span style="color: #a0522d;">sample_var</span> = random_sample.var()
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">Media campionaria: </span>{sample_mean}<span style="color: #8b2252;">"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Varianza campionaria: </span>{sample_var}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Visualizzazione della distribuzione</span>
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.bar(x, pmf_values, width=0.4)
plt.xlabel(<span style="color: #8b2252;">'x'</span>)
plt.ylabel(<span style="color: #8b2252;">'P(X=x)'</span>)
plt.title(f<span style="color: #8b2252;">'PMF - Uniforme Discreta su </span>{1, 2, ..., n}<span style="color: #8b2252;">'</span>)
plt.grid(alpha=0.3)

plt.subplot(1, 2, 2)
<span style="color: #b22222;"># </span><span style="color: #b22222;">Per la CDF, aggiungiamo punti extra per mostrare i salti</span>
<span style="color: #a0522d;">x_minus</span> = np.array([val - 0.001 <span style="color: #a020f0;">for</span> val <span style="color: #a020f0;">in</span> x])
<span style="color: #a0522d;">x_plus</span> = np.array([val + 0.001 <span style="color: #a020f0;">for</span> val <span style="color: #a020f0;">in</span> x])
<span style="color: #a0522d;">x_cdf</span> = np.sort(np.concatenate([np.arange(0.8, n+1.2, 0.01), x_minus, x_plus]))
<span style="color: #a0522d;">cdf_custom</span> = [F_unif_disc(t, n) <span style="color: #a020f0;">for</span> t <span style="color: #a020f0;">in</span> x_cdf]
plt.plot(x_cdf, cdf_custom)
plt.xlabel(<span style="color: #8b2252;">'x'</span>)
plt.ylabel(<span style="color: #8b2252;">'P(X&#8804;x)'</span>)
plt.title(<span style="color: #8b2252;">'CDF - Uniforme Discreta'</span>)
plt.grid(alpha=0.3)

plt.tight_layout()
plt.show()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Istogramma di un campione grande per verificare la distribuzione</span>
plt.figure(figsize=(8, 5))
plt.hist(random_sample, bins=np.linspace(0.5, n+0.5, n+1), density=<span style="color: #008b8b;">True</span>)
plt.xlabel(<span style="color: #8b2252;">'Valore'</span>)
plt.ylabel(<span style="color: #8b2252;">'Frequenza relativa'</span>)
plt.title(<span style="color: #8b2252;">'Istogramma di 1000 campioni da Uniforme Discreta'</span>)
plt.xticks(<span style="color: #483d8b;">range</span>(1, n+1))
plt.grid(alpha=0.3)
plt.show()
</pre>
</div>
<p>
NON FUNZIONA!
</p>
</div>
</div>
<div id="outline-container-orgb2fb813" class="outline-4">
<h4 id="orgb2fb813">Poisson</h4>
<div class="outline-text-4" id="text-orgb2fb813">
<p>
<b>Definizione (Distribuzione di Poisson):</b> Una variabile aleatoria \(X\) segue una distribuzione di Poisson con parametro \(\lambda > 0\) (scriviamo \(X \sim \text{Pois}(\lambda)\)) se rappresenta il numero di eventi che si verificano in un intervallo fisso di tempo o spazio, quando questi eventi si verificano con un tasso medio costante e indipendentemente dall'istante dell'ultimo evento.
</p>

<p>
La funzione di massa di probabilità è:
\[p_X(i) = \frac{e^{-\lambda}\lambda^i}{i!} \mathbb{I}_{\mathbb{N} \cup \{0\}}(i)\]
</p>

<p>
<b>Proprietà principali:</b>
</p>
<ul class="org-ul">
<li><b>Supporto:</b> \(X \in \{0, 1, 2, ...\}\)</li>
<li><b>Valore atteso:</b> \(E[X] = \lambda\)</li>
<li><b><a href="#org41a8e1b">Varianza</a>:</b> \(Var(X) = \lambda\)</li>
</ul>

<p>
<b>Dimostrazione del valore atteso:</b>
</p>
\begin{align}
E[X] &= \sum_{i=0}^{\infty} i \cdot \frac{e^{-\lambda}\lambda^i}{i!} \\
&= \sum_{i=1}^{\infty} i \cdot \frac{e^{-\lambda}\lambda^i}{i!} \\
&= \lambda e^{-\lambda} \sum_{i=1}^{\infty} \frac{\lambda^{i-1}}{(i-1)!} \\
&= \lambda e^{-\lambda} \sum_{j=0}^{\infty} \frac{\lambda^j}{j!} \\
&= \lambda e^{-\lambda} \cdot e^{\lambda} \\
&= \lambda
\end{align}

<p>
La funzione di ripartizione è:
\[F_X(x) = P(X \leq x) = \sum_{i=0}^{\lfloor x \rfloor} \frac{e^{-\lambda}\lambda^i}{i!} \mathbb{I}_{[0, \infty)}(x)\]
</p>

<p>
<b>Proprietà aggiuntive:</b>
</p>
<ul class="org-ul">
<li>Se \(X_1 \sim \text{Pois}(\lambda_1)\) e \(X_2 \sim \text{Pois}(\lambda_2)\) sono indipendenti, allora \(X_1 + X_2 \sim \text{Pois}(\lambda_1 + \lambda_2)\)</li>
<li>La distribuzione di Poisson può essere ottenuta come limite della distribuzione binomiale quando \(n \to \infty\), \(p \to 0\) e \(np = \lambda\) rimane costante</li>
</ul>

<p>
Un vantaggio significativo della distribuzione di Poisson rispetto a quella binomiale è di natura computazionale: quando si modellano eventi rari in campioni grandi, la binomiale richiederebbe il calcolo di un numero elevato di coefficienti binomiali e potenze, mentre la Poisson fornisce una buona approssimazione con una formula più semplice da calcolare, risparmiando così un considerevole numero di addendi e semplificando i calcoli.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
<span style="color: #a020f0;">import</span> matplotlib.pyplot <span style="color: #a020f0;">as</span> plt
<span style="color: #a020f0;">import</span> scipy.stats <span style="color: #a020f0;">as</span> st
<span style="color: #a020f0;">import</span> math
<span style="color: #a020f0;">from</span> ipywidgets <span style="color: #a020f0;">import</span> interact, FloatSlider

<span style="color: #b22222;"># </span><span style="color: #b22222;">Definizione di una funzione per calcolare la PMF della distribuzione di Poisson</span>
<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">p_poisson</span>(k, lambda_val):
    <span style="color: #8b2252;">"""Calcola la PMF della distribuzione di Poisson P(X=k)"""</span>
    <span style="color: #a020f0;">if</span> k &lt; 0 <span style="color: #a020f0;">or</span> k != <span style="color: #483d8b;">int</span>(k):
        <span style="color: #a020f0;">return</span> 0
    <span style="color: #a020f0;">return</span> np.exp(-lambda_val) * (lambda_val**k) / math.factorial(k)

<span style="color: #b22222;">#</span><span style="color: #b22222;">F</span>
<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">F_Pois</span>(l, k):
    <span style="color: #a020f0;">if</span> k &lt; 0:  <span style="color: #b22222;"># </span><span style="color: #b22222;">CDF is 0 for negative values</span>
        <span style="color: #a020f0;">return</span> 0
    <span style="color: #a0522d;">total</span> = 0
    <span style="color: #a020f0;">for</span> i <span style="color: #a020f0;">in</span> <span style="color: #483d8b;">range</span>(k + 1):  <span style="color: #b22222;"># </span><span style="color: #b22222;">Include k in the calculation</span>
        <span style="color: #a0522d;">total</span> += p_poisson(l, i)
    <span style="color: #a020f0;">return</span> total  <span style="color: #b22222;"># </span><span style="color: #b22222;">Return the calculated CDF value</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Parametro della distribuzione di Poisson</span>
<span style="color: #a0522d;">lambda_val</span> = 5

<span style="color: #b22222;"># </span><span style="color: #b22222;">Creazione dell'oggetto distribuzione di Poisson usando scipy.stats</span>
<span style="color: #a0522d;">poisson_dist</span> = st.poisson(lambda_val)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo della PMF per diversi valori</span>
<span style="color: #a0522d;">x_range</span> = np.arange(0, 15)
<span style="color: #a0522d;">pmf_custom</span> = [p_poisson(x, lambda_val) <span style="color: #a020f0;">for</span> x <span style="color: #a020f0;">in</span> x_range]
<span style="color: #a0522d;">pmf_scipy</span> = poisson_dist.pmf(x_range)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Verifica che le due implementazioni diano risultati identici</span>
<span style="color: #483d8b;">print</span>(<span style="color: #8b2252;">"Confronto tra implementazione personalizzata e scipy.stats:"</span>)
<span style="color: #a020f0;">for</span> i, x <span style="color: #a020f0;">in</span> <span style="color: #483d8b;">enumerate</span>(x_range):
    <span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X=</span>{x}<span style="color: #8b2252;">) = </span>{pmf_custom[i]:.6f}<span style="color: #8b2252;"> (custom) vs </span>{pmf_scipy[i]:.6f}<span style="color: #8b2252;"> (scipy)"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo della CDF</span>
<span style="color: #a0522d;">cdf_values</span> = poisson_dist.cdf(x_range)
<span style="color: #483d8b;">print</span>(<span style="color: #8b2252;">"</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">Valori della CDF:"</span>)
<span style="color: #a020f0;">for</span> i, x <span style="color: #a020f0;">in</span> <span style="color: #483d8b;">enumerate</span>(x_range):
    <span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X&#8804;</span>{x}<span style="color: #8b2252;">) = </span>{cdf_values[i]:.6f}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo di momenti e statistiche</span>
<span style="color: #a0522d;">mean</span> = poisson_dist.mean()
<span style="color: #a0522d;">var</span> = poisson_dist.var()
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">Valore atteso: </span>{mean}<span style="color: #8b2252;"> (teoricamente: </span>{lambda_val}<span style="color: #8b2252;">)"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Varianza: </span>{var}<span style="color: #8b2252;"> (teoricamente: </span>{lambda_val}<span style="color: #8b2252;">)"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Generazione di variabili aleatorie con distribuzione di Poisson</span>
<span style="color: #a0522d;">sample</span> = poisson_dist.rvs(size=1000)
<span style="color: #a0522d;">sample_mean</span> = sample.mean()
<span style="color: #a0522d;">sample_var</span> = sample.var()
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">Media campionaria: </span>{sample_mean}<span style="color: #8b2252;">"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Varianza campionaria: </span>{sample_var}<span style="color: #8b2252;">"</span>)

<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">visualizza_poisson</span>(lambda_val=5):
    <span style="color: #8b2252;">"""Funzione per visualizzare la distribuzione di Poisson con un dato parametro"""</span>
    <span style="color: #a0522d;">poisson_dist</span> = st.poisson(lambda_val)
    <span style="color: #a0522d;">x_range</span> = np.arange(0, <span style="color: #483d8b;">max</span>(20, <span style="color: #483d8b;">int</span>(lambda_val*3)))
    <span style="color: #a0522d;">pmf_values</span> = poisson_dist.pmf(x_range)
    <span style="color: #a0522d;">cdf_values</span> = poisson_dist.cdf(x_range)

    plt.figure(figsize=(12, 5))

    <span style="color: #b22222;"># </span><span style="color: #b22222;">Grafico della PMF con sticks invece che barre</span>
    plt.subplot(1, 2, 1)
    plt.vlines(x_range, [0]*<span style="color: #483d8b;">len</span>(x_range), pmf_values, colors=<span style="color: #8b2252;">'blue'</span>)  <span style="color: #b22222;"># </span><span style="color: #b22222;">Sticks verticali</span>
    plt.plot(x_range, pmf_values, <span style="color: #8b2252;">'o'</span>, color=<span style="color: #8b2252;">'blue'</span>)  <span style="color: #b22222;"># </span><span style="color: #b22222;">Punti alle estremit&#224;</span>
    plt.axvline(x=lambda_val, color=<span style="color: #8b2252;">'r'</span>, linestyle=<span style="color: #8b2252;">'--'</span>, 
                label=f<span style="color: #8b2252;">'Media = Varianza = </span>{lambda_val}<span style="color: #8b2252;">'</span>)
    plt.xlabel(<span style="color: #8b2252;">'k'</span>)
    plt.ylabel(<span style="color: #8b2252;">'P(X=k)'</span>)
    plt.title(f<span style="color: #8b2252;">'PMF - Poisson(&#955;=</span>{lambda_val}<span style="color: #8b2252;">)'</span>)
    plt.grid(alpha=0.3)
    plt.legend()

    <span style="color: #b22222;"># </span><span style="color: #b22222;">Grafico della CDF</span>
    plt.subplot(1, 2, 2)
    plt.step(x_range, cdf_values, where=<span style="color: #8b2252;">'post'</span>, color=<span style="color: #8b2252;">'green'</span>)
    plt.axvline(x=lambda_val, color=<span style="color: #8b2252;">'r'</span>, linestyle=<span style="color: #8b2252;">'--'</span>, 
                label=f<span style="color: #8b2252;">'Media = Varianza = </span>{lambda_val}<span style="color: #8b2252;">'</span>)
    plt.xlabel(<span style="color: #8b2252;">'x'</span>)
    plt.ylabel(<span style="color: #8b2252;">'P(X&#8804;x)'</span>)
    plt.title(f<span style="color: #8b2252;">'CDF - Poisson(&#955;=</span>{lambda_val}<span style="color: #8b2252;">)'</span>)
    plt.grid(alpha=0.3)
    plt.legend()

    plt.tight_layout()
    plt.show()

    <span style="color: #b22222;"># </span><span style="color: #b22222;">Istogramma di un campione grande</span>
    <span style="color: #a0522d;">sample</span> = poisson_dist.rvs(size=2000)
    plt.figure(figsize=(10, 4))
    plt.hist(sample, bins=np.arange(-0.5, <span style="color: #483d8b;">max</span>(sample)+1.5, 1), 
             density=<span style="color: #008b8b;">True</span>, alpha=0.7, label=<span style="color: #8b2252;">'Campione'</span>)
    plt.vlines(x_range, [0]*<span style="color: #483d8b;">len</span>(x_range), pmf_values, colors=<span style="color: #8b2252;">'red'</span>, alpha=0.7)
    plt.plot(x_range, pmf_values, <span style="color: #8b2252;">'o'</span>, color=<span style="color: #8b2252;">'red'</span>, label=<span style="color: #8b2252;">'PMF teorica'</span>)

    plt.axvline(x=lambda_val, color=<span style="color: #8b2252;">'k'</span>, linestyle=<span style="color: #8b2252;">'--'</span>, 
                label=f<span style="color: #8b2252;">'Media = Varianza = </span>{lambda_val}<span style="color: #8b2252;">'</span>)
    plt.xlabel(<span style="color: #8b2252;">'Valore'</span>)
    plt.ylabel(<span style="color: #8b2252;">'Frequenza relativa'</span>)
    plt.title(f<span style="color: #8b2252;">'Istogramma di 2000 campioni da Poisson(&#955;=</span>{lambda_val}<span style="color: #8b2252;">)'</span>)
    plt.grid(alpha=0.3)
    plt.legend()
    plt.show()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Eseguire per il valore di default</span>
visualizza_poisson(lambda_val)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Oppure creare un controllo interattivo</span>
<span style="color: #b22222;"># </span><span style="color: #b22222;">interact(visualizza_poisson, lambda_val=FloatSlider(min=0.5, max=15, step=0.5, value=5))</span>
</pre>
</div>
</div>
</div>
<div id="outline-container-org6b15bd9" class="outline-4">
<h4 id="org6b15bd9">Ipergeometrica</h4>
<div class="outline-text-4" id="text-org6b15bd9">
<p>
<b>Definizione (Distribuzione Ipergeometrica):</b> Una variabile aleatoria \(X\) segue una distribuzione ipergeometrica con parametri \(M\) (numero di oggetti funzionanti), \(N\) (numero di oggetti guasti) e \(n\) (numero di estrazioni), scriviamo \(X \sim \text{HGeom}(M,N,n)\), se rappresenta il numero di oggetti funzionanti in \(n\) estrazioni senza rimpiazzo da una popolazione di \(M+N\) elementi totali.
</p>

<p>
La funzione di massa di probabilità è:
\[p_X(i) = \frac{\binom{M}{i} \binom{N}{n-i}}{\binom{M+N}{n}} \mathbb{I}_{\{max(0,n-N), \ldots, min(n,M)\}}(i)\]
</p>

<p>
<b>Proprietà principali:</b>
</p>
<ul class="org-ul">
<li><b>Supporto:</b> \(X \in \{max(0,n-N), \ldots, min(n,M)\}\)</li>
</ul>

<p>
<b><b>Calcolo del valore atteso:</b></b>
</p>

<p>
Per determinare il valore atteso della variabile ipergeometrica, possiamo scomporla in variabili più semplici. Definiamo \(n\) variabili indicatrici \(X_1, X_2, \ldots, X_n\) dove:
\[X_i = \begin{cases}
1 & \text{se l'$i$-esimo oggetto estratto è funzionante} \\
0 & \text{altrimenti}
\end{cases}\]
</p>

<p>
Ciascuna \(X_i\) è una variabile bernoulliana, ma non con lo stesso parametro per ogni estrazione a causa dell'assenza di rimpiazzo. Tuttavia, per simmetria, la probabilità marginale che un singolo oggetto estratto sia funzionante è \(\frac{M}{M+N}\).
</p>

<p>
La variabile aleatoria ipergeometrica \(X\) rappresenta il numero totale di oggetti funzionanti estratti, quindi:
\[X = \sum_{i=1}^{n} X_i\]
</p>

<p>
Applicando la linearità del valore atteso:
\[E[X] = E\left[\sum_{i=1}^{n} X_i\right] = \sum_{i=1}^{n} E[X_i] = \sum_{i=1}^{n} \frac{M}{M+N} = n \cdot \frac{M}{M+N}\]
</p>

<p>
<b><b>Analogia con la distribuzione binomiale:</b></b>
</p>

<p>
La distribuzione ipergeometrica è simile alla distribuzione binomiale \(B(n, p)\) con \(p = \frac{M}{M+N}\), in quanto entrambe rappresentano il numero di "successi" in \(n\) prove. La differenza fondamentale è che:
</p>
<ul class="org-ul">
<li>Nella distribuzione binomiale, le prove sono indipendenti (con rimpiazzo)</li>
<li>Nella distribuzione ipergeometrica, le prove non sono indipendenti (senza rimpiazzo)</li>
</ul>

<p>
Quando la popolazione è molto grande rispetto al numero di estrazioni (cioè \(M+N \gg n\)), la probabilità di estrarre un oggetto funzionante rimane quasi costante durante il processo di estrazione, rendendo le prove quasi indipendenti. In questo caso, la distribuzione ipergeometrica può essere ben approssimata dalla distribuzione binomiale \(B(n, \frac{M}{M+N})\).
</p>

<ul class="org-ul">
<li><b>Valore atteso:</b> \(E[X] = n \cdot \frac{M}{M+N}\)</li>
<li><b><a href="#org41a8e1b">Varianza</a>:</b> \(Var(X) = n \cdot \frac{M}{M+N} \cdot (1-\frac{M}{M+N}) \cdot \frac{M+N-n}{M+N-1}\)</li>
</ul>

<p>
La funzione di ripartizione è:
\[F_X(x) = P(X \leq x) = \sum_{i=max(0,n-N)}^{min(\lfloor x \rfloor, min(n,M))} \frac{\binom{M}{i} \binom{N}{n-i}}{\binom{M+N}{n}}\]
</p>

<p>
<b>Relazione con altre distribuzioni:</b>
</p>
<ul class="org-ul">
<li>Se \(n\) è piccolo rispetto a \(M+N\), la distribuzione ipergeometrica può essere approssimata da una distribuzione binomiale \(B(n, \frac{M}{M+N})\)</li>
<li>A differenza della binomiale, le estrazioni non sono indipendenti (senza rimpiazzo)</li>
<li>Il fattore \(\frac{M+N-n}{M+N-1}\) nella <a href="#org41a8e1b">varianza</a> è chiamato "fattore di correzione per popolazione finita"n (non visto a lezione)</li>
</ul>

<p>
<b><b>Domande per approfondire:</b></b>
</p>
<ol class="org-ol">
<li><p>
Come si comporta la <a href="#org41a8e1b">varianza</a> della distribuzione ipergeometrica quando \(n\) si avvicina a \(M+N\)?
</p>

<p>
La <a href="#org41a8e1b">varianza</a> tende a 0, infatti nel caso limite di \(n=M+N\), il numero di oggetti funzionanti sarà sempre \(M\) e quindi tale probabilità è \(1\) e tutto il resto \(0\).
</p></li>
<li>Perché il fattore di correzione per popolazione finita è sempre minore o uguale a 1?</li>
<li><p>
Come cambierebbe il modello se fosse possibile rimettere gli oggetti nella popolazione dopo ogni estrazione?
</p>

<div class="org-src-container">
<pre class="src src-python"> <span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
<span style="color: #a020f0;">import</span> matplotlib.pyplot <span style="color: #a020f0;">as</span> plt
<span style="color: #a020f0;">import</span> scipy.stats <span style="color: #a020f0;">as</span> st
<span style="color: #a020f0;">from</span> scipy.special <span style="color: #a020f0;">import</span> comb

<span style="color: #b22222;"># </span><span style="color: #b22222;">Parametri della distribuzione ipergeometrica</span>
<span style="color: #a0522d;">M</span> = 20  <span style="color: #b22222;"># </span><span style="color: #b22222;">numero di oggetti funzionanti</span>
<span style="color: #a0522d;">N</span> = 30  <span style="color: #b22222;"># </span><span style="color: #b22222;">numero di oggetti guasti</span>
<span style="color: #a0522d;">n</span> = 10  <span style="color: #b22222;"># </span><span style="color: #b22222;">numero di estrazioni</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Creazione dell'oggetto distribuzione ipergeometrica</span>
<span style="color: #b22222;"># </span><span style="color: #b22222;">scipy.stats.hypergeom usa parametri (M+N, M, n)</span>
<span style="color: #a0522d;">hypergeom_dist</span> = st.hypergeom(M=M+N, n=M, N=n)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo del supporto</span>
<span style="color: #a0522d;">lower_bound</span> = <span style="color: #483d8b;">max</span>(0, n-N)
<span style="color: #a0522d;">upper_bound</span> = <span style="color: #483d8b;">min</span>(n, M)
<span style="color: #a0522d;">support</span> = np.arange(lower_bound, upper_bound+1)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Funzione personalizzata per calcolare la PMF</span>
<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">p_hypergeom</span>(k, M, N, n):
    <span style="color: #8b2252;">"""Calcola la PMF di una distribuzione ipergeometrica"""</span>
    <span style="color: #a020f0;">if</span> k &lt; <span style="color: #483d8b;">max</span>(0, n-N) <span style="color: #a020f0;">or</span> k &gt; <span style="color: #483d8b;">min</span>(n, M):
        <span style="color: #a020f0;">return</span> 0
    <span style="color: #a020f0;">return</span> (comb(M, k) * comb(N, n-k)) / comb(M+N, n)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo della PMF</span>
<span style="color: #a0522d;">pmf_custom</span> = [p_hypergeom(k, M, N, n) <span style="color: #a020f0;">for</span> k <span style="color: #a020f0;">in</span> support]
<span style="color: #a0522d;">pmf_scipy</span> = hypergeom_dist.pmf(support)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Verifica che le due implementazioni diano risultati simili</span>
<span style="color: #483d8b;">print</span>(<span style="color: #8b2252;">"Confronto tra implementazione personalizzata e scipy.stats:"</span>)
<span style="color: #a020f0;">for</span> i, k <span style="color: #a020f0;">in</span> <span style="color: #483d8b;">enumerate</span>(support):
    <span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X=</span>{k}<span style="color: #8b2252;">) = </span>{pmf_custom[i]:.6f}<span style="color: #8b2252;"> (custom) vs </span>{pmf_scipy[i]:.6f}<span style="color: #8b2252;"> (scipy)"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo della CDF</span>
<span style="color: #a0522d;">cdf_values</span> = hypergeom_dist.cdf(support)
<span style="color: #483d8b;">print</span>(<span style="color: #8b2252;">"</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">Valori della CDF:"</span>)
<span style="color: #a020f0;">for</span> i, k <span style="color: #a020f0;">in</span> <span style="color: #483d8b;">enumerate</span>(support):
    <span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X&#8804;</span>{k}<span style="color: #8b2252;">) = </span>{cdf_values[i]:.6f}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo dei momenti</span>
<span style="color: #a0522d;">mean</span> = hypergeom_dist.mean()
<span style="color: #a0522d;">var</span> = hypergeom_dist.var()
<span style="color: #a0522d;">theo_mean</span> = n * M / (M+N)
<span style="color: #a0522d;">theo_var</span> = n * (M/(M+N)) * (1-M/(M+N)) * ((M+N-n)/(M+N-1))

<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">Valore atteso: </span>{mean:.6f}<span style="color: #8b2252;"> (teoricamente: </span>{theo_mean:.6f}<span style="color: #8b2252;">)"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Varianza: </span>{var:.6f}<span style="color: #8b2252;"> (teoricamente: </span>{theo_var:.6f}<span style="color: #8b2252;">)"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Generazione di variabili aleatorie con distribuzione ipergeometrica</span>
<span style="color: #a0522d;">sample</span> = hypergeom_dist.rvs(size=1000)
<span style="color: #a0522d;">sample_mean</span> = sample.mean()
<span style="color: #a0522d;">sample_var</span> = sample.var()
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">Media campionaria: </span>{sample_mean:.6f}<span style="color: #8b2252;">"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Varianza campionaria: </span>{sample_var:.6f}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Visualizzazione della distribuzione ipergeometrica</span>
plt.figure(figsize=(10, 5))

<span style="color: #b22222;"># </span><span style="color: #b22222;">Grafico della PMF</span>
plt.subplot(1, 2, 1)
plt.bar(support, pmf_scipy, width=0.4, alpha=0.7)
plt.axvline(x=mean, color=<span style="color: #8b2252;">'r'</span>, linestyle=<span style="color: #8b2252;">'--'</span>, 
            label=f<span style="color: #8b2252;">'Media = </span>{mean:.2f}<span style="color: #8b2252;">'</span>)
plt.xlabel(<span style="color: #8b2252;">'Numero di oggetti funzionanti estratti'</span>)
plt.ylabel(<span style="color: #8b2252;">'Probabilit&#224;'</span>)
plt.title(f<span style="color: #8b2252;">'PMF - Ipergeometrica(M=</span>{M}<span style="color: #8b2252;">, N=</span>{N}<span style="color: #8b2252;">, n=</span>{n}<span style="color: #8b2252;">)'</span>)
plt.grid(alpha=0.3)
plt.legend()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Grafico della CDF</span>
plt.subplot(1, 2, 2)
plt.step(support, cdf_values, where=<span style="color: #8b2252;">'post'</span>, color=<span style="color: #8b2252;">'green'</span>)
plt.axvline(x=mean, color=<span style="color: #8b2252;">'r'</span>, linestyle=<span style="color: #8b2252;">'--'</span>, 
            label=f<span style="color: #8b2252;">'Media = </span>{mean:.2f}<span style="color: #8b2252;">'</span>)
plt.xlabel(<span style="color: #8b2252;">'x'</span>)
plt.ylabel(<span style="color: #8b2252;">'P(X&#8804;x)'</span>)
plt.title(<span style="color: #8b2252;">'CDF - Ipergeometrica'</span>)
plt.grid(alpha=0.3)
plt.legend()

plt.tight_layout()
plt.show()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Confronto con la distribuzione binomiale approssimante</span>
<span style="color: #a0522d;">p_approx</span> = M / (M+N)
<span style="color: #a0522d;">binom_approx</span> = st.binom(n, p_approx)
<span style="color: #a0522d;">x_binom</span> = np.arange(0, n+1)
<span style="color: #a0522d;">pmf_binom</span> = binom_approx.pmf(x_binom)

plt.figure(figsize=(12, 5))
plt.bar(support, pmf_scipy, width=0.4, alpha=0.7, label=<span style="color: #8b2252;">'Ipergeometrica'</span>)
plt.plot(x_binom, pmf_binom, <span style="color: #8b2252;">'ro-'</span>, label=<span style="color: #8b2252;">'Binomiale approssimante'</span>)
plt.axvline(x=mean, color=<span style="color: #8b2252;">'k'</span>, linestyle=<span style="color: #8b2252;">'--'</span>, 
            label=f<span style="color: #8b2252;">'Media = </span>{mean:.2f}<span style="color: #8b2252;">'</span>)
plt.xlabel(<span style="color: #8b2252;">'Numero di oggetti funzionanti estratti'</span>)
plt.ylabel(<span style="color: #8b2252;">'Probabilit&#224;'</span>)
plt.title(<span style="color: #8b2252;">'Confronto tra Ipergeometrica e Binomiale approssimante'</span>)
plt.grid(alpha=0.3)
plt.legend()
plt.show()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Visualizzazione dell'effetto del rapporto n/(M+N) sull'approssimazione binomiale</span>
<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">compare_hypergeom_binom</span>(M, N, n):
    <span style="color: #8b2252;">"""Confronta la distribuzione ipergeometrica con la sua approssimazione binomiale"""</span>
    <span style="color: #a0522d;">hypergeom_dist</span> = st.hypergeom(M=M+N, n=M, N=n)
    <span style="color: #a0522d;">p_approx</span> = M / (M+N)
    <span style="color: #a0522d;">binom_approx</span> = st.binom(n, p_approx)

    <span style="color: #a0522d;">support</span> = np.arange(<span style="color: #483d8b;">max</span>(0, n-N), <span style="color: #483d8b;">min</span>(n, M)+1)
    <span style="color: #a0522d;">pmf_hypergeom</span> = hypergeom_dist.pmf(support)

    <span style="color: #a0522d;">x_binom</span> = np.arange(0, n+1)
    <span style="color: #a0522d;">pmf_binom</span> = binom_approx.pmf(x_binom)

    plt.figure(figsize=(10, 5))
    plt.bar(support, pmf_hypergeom, width=0.4, alpha=0.7, label=<span style="color: #8b2252;">'Ipergeometrica'</span>)
    plt.plot(x_binom, pmf_binom, <span style="color: #8b2252;">'ro-'</span>, label=<span style="color: #8b2252;">'Binomiale approssimante'</span>)
    plt.xlabel(<span style="color: #8b2252;">'Numero di oggetti funzionanti estratti'</span>)
    plt.ylabel(<span style="color: #8b2252;">'Probabilit&#224;'</span>)
    plt.title(f<span style="color: #8b2252;">'Confronto: M=</span>{M}<span style="color: #8b2252;">, N=</span>{N}<span style="color: #8b2252;">, n=</span>{n}<span style="color: #8b2252;">, rapporto n/(M+N)=</span>{n/(M+N):.4f}<span style="color: #8b2252;">'</span>)
    plt.grid(alpha=0.3)
    plt.legend()
    plt.show()

    <span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo della divergenza di Kullback-Leibler (solo per i valori nel supporto dell'ipergeometrica)</span>
    <span style="color: #a0522d;">kl_div</span> = 0
    <span style="color: #a020f0;">for</span> i, k <span style="color: #a020f0;">in</span> <span style="color: #483d8b;">enumerate</span>(support):
        <span style="color: #a0522d;">p_hyper</span> = pmf_hypergeom[i]
        <span style="color: #a0522d;">p_binom</span> = binom_approx.pmf(k)
        <span style="color: #a020f0;">if</span> p_hyper &gt; 0 <span style="color: #a020f0;">and</span> p_binom &gt; 0:
            <span style="color: #a0522d;">kl_div</span> += p_hyper * np.log(p_hyper / p_binom)

    <span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Divergenza KL: </span>{kl_div:.6f}<span style="color: #8b2252;"> (pi&#249; vicino a 0 = migliore approssimazione)"</span>)
    <span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Rapporto n/(M+N): </span>{n/(M+N):.6f}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Esempio con alto rapporto n/(M+N)</span>
compare_hypergeom_binom(M=20, N=30, n=40)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Esempio con basso rapporto n/(M+N)</span>
compare_hypergeom_binom(M=200, N=300, n=40)
</pre>
</div></li>
</ol>
</div>
</div>
<div id="outline-container-orge9c02b0" class="outline-4">
<h4 id="orge9c02b0">Continuo uniforme</h4>
<div class="outline-text-4" id="text-orge9c02b0">
<p>
<b>Definizione (Distribuzione Uniforme Continua):</b> Una variabile aleatoria \(X\) segue una distribuzione uniforme continua sull'intervallo \([a,b]\) (scriviamo \(X \sim \text{Unif}(a,b)\)) se la sua densità di probabilità è costante sull'intervallo \([a,b]\).
</p>

<p>
La funzione di densità di probabilità è:
\[f_X(x) = \frac{1}{b-a} \mathbb{I}_{[a,b]}(x)\]
</p>

<p>
<b>Calcolo delle probabilità mediante integrazione:</b>
La probabilità che \(X\) appartenga a un intervallo \(I \subseteq [a,b]\) si calcola mediante l'integrale:
\[P(X \in I) = \int_I f_X(x) \, dx = \int_I \frac{1}{b-a} \, dx = \frac{1}{b-a} \int_I \, dx = \frac{|I|}{b-a}\]
</p>

<p>
dove \(|I|\) rappresenta la misura (lunghezza) dell'intervallo \(I\). Questo dimostra che la probabilità dipende solo dalla lunghezza dell'intervallo considerato, non dal valore specifico di \(x\) all'interno di esso - una caratteristica fondamentale della distribuzione uniforme.
</p>

<p>
<b>Verifica della normalizzazione:</b>
Integrando la densità su tutto lo spazio campionario:
\[\int_{-\infty}^{+\infty} f_X(x) \, dx = \int_{-\infty}^{a} f_X(x) \, dx + \int_{a}^{b} f_X(x) \, dx + \int_{b}^{+\infty} f_X(x) \, dx\]
</p>

<p>
Poiché \(f_X(x) = 0\) per \(x < a\) e \(x > b\):
\[\int_{-\infty}^{+\infty} f_X(x) \, dx = \int_{a}^{b} \frac{1}{b-a} \, dx = \frac{1}{b-a} \cdot (b-a) = 1\]
</p>

<p>
Questo risultato è coerente con l'interpretazione geometrica: la densità forma un rettangolo di altezza \(\frac{1}{b-a}\) e base \((b-a)\), con area totale pari a 1.
</p>

<p>
<b>Proprietà principali:</b>
</p>
<ul class="org-ul">
<li><b>Supporto:</b> \(X \in [a,b]\)</li>
<li><b>Valore atteso:</b> \(E[X] = \frac{a+b}{2}\)</li>
<li><b><a href="#org41a8e1b">Varianza</a>:</b> \(Var(X) = \frac{(b-a)^2}{12}\)</li>
</ul>

<p>
La funzione di ripartizione è:
\[F_X(x) = \frac{x-a}{b-a} \mathbb{I}_{[a,b]}(x) + \mathbb{I}_{(b,\infty)}(x)\]
</p>

<p>
che equivale a:
\[F_X(x) = \begin{cases}
0 & \text{se } x < a \\
\frac{x-a}{b-a} & \text{se } a \leq x \leq b \\
1 & \text{se } x > b
\end{cases}\]
</p>

<p>
<b>Proprietà aggiuntive:</b>
</p>
<ul class="org-ul">
<li>Se \(X \sim \text{Unif}(a,b)\), allora \(Y = c \cdot X + d \sim \text{Unif}(c \cdot a + d, c \cdot b + d)\) per \(c > 0\)</li>
<li>La distribuzione uniforme rappresenta la situazione di massima incertezza all'interno di un intervallo limitato</li>
<li>Il valore atteso coincide con il punto medio dell'intervallo</li>
</ul>

<p>
<b>Applicazioni:</b>
</p>
<ul class="org-ul">
<li>Modellazione di errori di arrotondamento</li>
<li>Generazione di numeri pseudo-casuali</li>
<li>Modellazione di situazioni in cui qualsiasi valore in un intervallo ha uguale probabilità di verificarsi</li>
</ul>


<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
<span style="color: #a020f0;">import</span> matplotlib.pyplot <span style="color: #a020f0;">as</span> plt
<span style="color: #a020f0;">import</span> scipy.stats <span style="color: #a020f0;">as</span> st
<span style="color: #a020f0;">from</span> ipywidgets <span style="color: #a020f0;">import</span> interact, FloatSlider

<span style="color: #b22222;"># </span><span style="color: #b22222;">Parametri della distribuzione uniforme continua</span>
<span style="color: #a0522d;">a</span> = 2
<span style="color: #a0522d;">b</span> = 5

<span style="color: #b22222;"># </span><span style="color: #b22222;">Creazione dell'oggetto distribuzione uniforme continua</span>
<span style="color: #a0522d;">unif_cont</span> = st.uniform(loc=a, scale=b-a)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Funzioni personalizzate per calcolare PDF e CDF</span>
<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">f_unif</span>(x, a, b):
    <span style="color: #8b2252;">"""Calcola la PDF di una uniforme continua"""</span>
    <span style="color: #a020f0;">if</span> a &lt;= x &lt;= b:
        <span style="color: #a020f0;">return</span> 1/(b-a)
    <span style="color: #a020f0;">else</span>:
        <span style="color: #a020f0;">return</span> 0

<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">F_unif</span>(x, a, b):
    <span style="color: #8b2252;">"""Calcola la CDF di una uniforme continua"""</span>
    <span style="color: #a020f0;">if</span> x &lt; a:
        <span style="color: #a020f0;">return</span> 0
    <span style="color: #a020f0;">elif</span> x &gt; b:
        <span style="color: #a020f0;">return</span> 1
    <span style="color: #a020f0;">else</span>:
        <span style="color: #a020f0;">return</span> (x-a)/(b-a)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo della PDF e CDF per diversi valori</span>
<span style="color: #a0522d;">x_values</span> = np.linspace(a-1, b+1, 1000)
<span style="color: #a0522d;">pdf_custom</span> = [f_unif(x, a, b) <span style="color: #a020f0;">for</span> x <span style="color: #a020f0;">in</span> x_values]
<span style="color: #a0522d;">pdf_scipy</span> = unif_cont.pdf(x_values)
<span style="color: #a0522d;">cdf_custom</span> = [F_unif(x, a, b) <span style="color: #a020f0;">for</span> x <span style="color: #a020f0;">in</span> x_values]
<span style="color: #a0522d;">cdf_scipy</span> = unif_cont.cdf(x_values)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo di momenti e statistiche</span>
<span style="color: #a0522d;">mean</span> = unif_cont.mean()
<span style="color: #a0522d;">var</span> = unif_cont.var()
<span style="color: #a0522d;">theo_mean</span> = (a+b)/2
<span style="color: #a0522d;">theo_var</span> = (b-a)**2/12

<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Valore atteso: </span>{mean}<span style="color: #8b2252;"> (teoricamente: </span>{theo_mean}<span style="color: #8b2252;">)"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Varianza: </span>{var}<span style="color: #8b2252;"> (teoricamente: </span>{theo_var}<span style="color: #8b2252;">)"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Generazione di variabili aleatorie con distribuzione uniforme continua</span>
<span style="color: #a0522d;">sample</span> = unif_cont.rvs(size=1000)
<span style="color: #a0522d;">sample_mean</span> = sample.mean()
<span style="color: #a0522d;">sample_var</span> = sample.var()
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Media campionaria: </span>{sample_mean}<span style="color: #8b2252;">"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Varianza campionaria: </span>{sample_var}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo di probabilit&#224; specifiche</span>
<span style="color: #a0522d;">point1</span> = (a + b)/4
<span style="color: #a0522d;">point2</span> = 3*(a + b)/4
<span style="color: #a0522d;">prob_between</span> = F_unif(point2, a, b) - F_unif(point1, a, b)
<span style="color: #a0522d;">prob_between_scipy</span> = unif_cont.cdf(point2) - unif_cont.cdf(point1)

<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(</span>{point1}<span style="color: #8b2252;"> &lt; X &lt; </span>{point2}<span style="color: #8b2252;">) = </span>{prob_between}<span style="color: #8b2252;"> (custom)"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(</span>{point1}<span style="color: #8b2252;"> &lt; X &lt; </span>{point2}<span style="color: #8b2252;">) = </span>{prob_between_scipy}<span style="color: #8b2252;"> (scipy)"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Visualizzazione della distribuzione uniforme continua</span>
plt.figure(figsize=(12, 5))

<span style="color: #b22222;"># </span><span style="color: #b22222;">Grafico della PDF</span>
plt.subplot(1, 2, 1)
plt.plot(x_values, pdf_scipy, <span style="color: #8b2252;">'b-'</span>, label=<span style="color: #8b2252;">'PDF'</span>)
plt.axvline(x=mean, color=<span style="color: #8b2252;">'r'</span>, linestyle=<span style="color: #8b2252;">'--'</span>, label=f<span style="color: #8b2252;">'Media = </span>{mean}<span style="color: #8b2252;">'</span>)
plt.axhline(y=1/(b-a), color=<span style="color: #8b2252;">'g'</span>, linestyle=<span style="color: #8b2252;">'-.'</span>)
plt.fill_between(x_values, pdf_scipy, alpha=0.2)
plt.xlim(a-0.5, b+0.5)
plt.ylim(-0.05, 1/(b-a)*1.2)
plt.xlabel(<span style="color: #8b2252;">'x'</span>)
plt.ylabel(<span style="color: #8b2252;">'f(x)'</span>)
plt.title(f<span style="color: #8b2252;">'PDF - Uniforme Continua [</span>{a}<span style="color: #8b2252;">, </span>{b}<span style="color: #8b2252;">]'</span>)
plt.grid(alpha=0.3)
plt.legend()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Grafico della CDF</span>
plt.subplot(1, 2, 2)
plt.plot(x_values, cdf_scipy, <span style="color: #8b2252;">'g-'</span>, label=<span style="color: #8b2252;">'CDF'</span>)
plt.axvline(x=mean, color=<span style="color: #8b2252;">'r'</span>, linestyle=<span style="color: #8b2252;">'--'</span>, label=f<span style="color: #8b2252;">'Media = </span>{mean}<span style="color: #8b2252;">'</span>)
plt.xlim(a-0.5, b+0.5)
plt.ylim(-0.05, 1.05)
plt.xlabel(<span style="color: #8b2252;">'x'</span>)
plt.ylabel(<span style="color: #8b2252;">'F(x)'</span>)
plt.title(f<span style="color: #8b2252;">'CDF - Uniforme Continua [</span>{a}<span style="color: #8b2252;">, </span>{b}<span style="color: #8b2252;">]'</span>)
plt.grid(alpha=0.3)
plt.legend()

plt.tight_layout()
plt.show()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Visualizzazione di un istogramma del campione generato</span>
plt.figure(figsize=(10, 5))
plt.hist(sample, bins=30, density=<span style="color: #008b8b;">True</span>, alpha=0.7, label=<span style="color: #8b2252;">'Campione'</span>)
plt.plot(x_values, pdf_scipy, <span style="color: #8b2252;">'r-'</span>, label=<span style="color: #8b2252;">'PDF teorica'</span>)
plt.axvline(x=mean, color=<span style="color: #8b2252;">'k'</span>, linestyle=<span style="color: #8b2252;">'--'</span>, label=f<span style="color: #8b2252;">'Media = </span>{mean}<span style="color: #8b2252;">'</span>)
plt.xlim(a-0.5, b+0.5)
plt.xlabel(<span style="color: #8b2252;">'Valore'</span>)
plt.ylabel(<span style="color: #8b2252;">'Densit&#224;'</span>)
plt.title(f<span style="color: #8b2252;">'Istogramma di 1000 campioni da Uniforme Continua [</span>{a}<span style="color: #8b2252;">, </span>{b}<span style="color: #8b2252;">]'</span>)
plt.grid(alpha=0.3)
plt.legend()
plt.show()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Funzione per visualizzare la distribuzione con parametri variabili</span>
<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">visualizza_unif_cont</span>(a=0, b=1):
    <span style="color: #8b2252;">"""Funzione per visualizzare la distribuzione uniforme continua con parametri variabili"""</span>
    <span style="color: #a020f0;">if</span> a &gt;= b:
        <span style="color: #483d8b;">print</span>(<span style="color: #8b2252;">"Errore: deve essere a &lt; b"</span>)
        <span style="color: #a020f0;">return</span>

    <span style="color: #a0522d;">unif_cont</span> = st.uniform(loc=a, scale=b-a)
    <span style="color: #a0522d;">x_values</span> = np.linspace(a-1, b+1, 1000)
    <span style="color: #a0522d;">pdf_values</span> = unif_cont.pdf(x_values)
    <span style="color: #a0522d;">cdf_values</span> = unif_cont.cdf(x_values)
    <span style="color: #a0522d;">mean</span> = unif_cont.mean()
    <span style="color: #a0522d;">std</span> = np.sqrt(unif_cont.var())

    plt.figure(figsize=(12, 5))

    <span style="color: #b22222;"># </span><span style="color: #b22222;">Grafico della PDF</span>
    plt.subplot(1, 2, 1)
    plt.plot(x_values, pdf_values, <span style="color: #8b2252;">'b-'</span>, label=<span style="color: #8b2252;">'PDF'</span>)
    plt.axvline(x=mean, color=<span style="color: #8b2252;">'r'</span>, linestyle=<span style="color: #8b2252;">'--'</span>, label=f<span style="color: #8b2252;">'Media = </span>{mean:.2f}<span style="color: #8b2252;">'</span>)
    plt.fill_between(x_values, pdf_values, alpha=0.2)
    plt.xlim(a-0.5, b+0.5)
    plt.ylim(-0.05, 1/(b-a)*1.2)
    plt.xlabel(<span style="color: #8b2252;">'x'</span>)
    plt.ylabel(<span style="color: #8b2252;">'f(x)'</span>)
    plt.title(f<span style="color: #8b2252;">'PDF - Uniforme Continua [</span>{a}<span style="color: #8b2252;">, </span>{b}<span style="color: #8b2252;">]'</span>)
    plt.grid(alpha=0.3)
    plt.legend()

    <span style="color: #b22222;"># </span><span style="color: #b22222;">Grafico della CDF</span>
    plt.subplot(1, 2, 2)
    plt.plot(x_values, cdf_values, <span style="color: #8b2252;">'g-'</span>, label=<span style="color: #8b2252;">'CDF'</span>)
    plt.axvline(x=mean, color=<span style="color: #8b2252;">'r'</span>, linestyle=<span style="color: #8b2252;">'--'</span>, label=f<span style="color: #8b2252;">'Media = </span>{mean:.2f}<span style="color: #8b2252;">'</span>)
    plt.xlim(a-0.5, b+0.5)
    plt.ylim(-0.05, 1.05)
    plt.xlabel(<span style="color: #8b2252;">'x'</span>)
    plt.ylabel(<span style="color: #8b2252;">'F(x)'</span>)
    plt.title(f<span style="color: #8b2252;">'CDF - Uniforme Continua [</span>{a}<span style="color: #8b2252;">, </span>{b}<span style="color: #8b2252;">]'</span>)
    plt.grid(alpha=0.3)
    plt.legend()

    plt.tight_layout()
    plt.show()

    <span style="color: #b22222;"># </span><span style="color: #b22222;">Statistiche</span>
    <span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Valore atteso: </span>{mean:.4f}<span style="color: #8b2252;">"</span>)
    <span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Deviazione standard: </span>{std:.4f}<span style="color: #8b2252;">"</span>)
    <span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Varianza: </span>{unif_cont.var():.4f}<span style="color: #8b2252;">"</span>)
    <span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"PDF costante: </span>{1/(b-a):.4f}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Esempio di utilizzo della funzione</span>
visualizza_unif_cont(a=2, b=5)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Implementazione dell'interfaccia interattiva (decommentare per utilizzarla)</span>
<span style="color: #b22222;"># </span><span style="color: #b22222;">interact(visualizza_unif_cont, a=FloatSlider(min=-5, max=5, step=0.5, value=0), </span>
<span style="color: #b22222;">#          </span><span style="color: #b22222;">b=FloatSlider(min=-4, max=10, step=0.5, value=1))</span>
</pre>
</div>
</div>
</div>
<div id="outline-container-org59e2742" class="outline-4">
<h4 id="org59e2742">Esponenziale</h4>
<div class="outline-text-4" id="text-org59e2742">
<p>
<b>Definizione (Distribuzione Esponenziale):</b> Una variabile aleatoria \(X\) segue una distribuzione esponenziale con parametro \(\lambda > 0\) (scriviamo \(X \sim \text{Exp}(\lambda)\)) se rappresenta il tempo di attesa fino al verificarsi del primo evento in un processo di Poisson con tasso \(\lambda\). Il supporto della distribuzione è l'insieme dei numeri reali positivi, \(\mathbb{R}^+\).
</p>

<p>
La funzione di densità di probabilità è:
\[f_X(x) = \lambda e^{-\lambda x} \mathbb{I}_{[0,\infty)}(x)\]
</p>

<p>
<b>Verifica della normalizzazione:</b>
Per verificare che la densità integri a 1, calcoliamo:
</p>
\begin{align}
\int_{0}^{\infty} f_X(x) \, dx &= \int_{0}^{\infty} \lambda e^{-\lambda x} \, dx
\end{align}

<p>
Utilizzando la sostituzione \(y = \lambda x\) (quindi \(dx = \frac{dy}{\lambda}\)), otteniamo:
</p>
\begin{align}
\int_{0}^{\infty} \lambda e^{-\lambda x} \, dx &= \int_{0}^{\infty} \lambda e^{-y} \frac{dy}{\lambda} \\
&= \int_{0}^{\infty} e^{-y} \, dy \\
&= [-e^{-y}]_{0}^{\infty} \\
&= 0 - (-1) = 1
\end{align}

<p>
<b>Proprietà principali:</b>
</p>
<ul class="org-ul">
<li><b>Supporto:</b> \(X \in [0,\infty)\)</li>
<li><b>Valore atteso:</b> \(E[X] = \frac{1}{\lambda}\)</li>
<li><b><a href="#org41a8e1b">Varianza</a>:</b> \(Var(X) = \frac{1}{\lambda^2}\)</li>
</ul>

<p>
La funzione di ripartizione è:
\[F_X(x) = \begin{cases}
0 & \text{se } x < 0 \\
1 - e^{-\lambda x} & \text{se } x \geq 0
\end{cases}\]
</p>

<p>
<b>Proprietà del massimo e minimo di variabili esponenziali:</b>
</p>

<p>
Siano \(X_1, X_2, \ldots, X_n\) variabili aleatorie indipendenti e identicamente distribuite (i.i.d.) con distribuzione \(\text{Exp}(\lambda)\).
</p>

<ol class="org-ol">
<li><p>
<b>Distribuzione del massimo:</b> 
</p>

<p>
Definiamo \(Y = \max(X_1, X_2, \ldots, X_n)\). La funzione di ripartizione di \(Y\) è:
</p>
\begin{align}
F_Y(x) &= P(Y \leq x) \\
&= P(\max(X_1, X_2, \ldots, X_n) \leq x) \\
&= P(X_1 \leq x, X_2 \leq x, \ldots, X_n \leq x)
\end{align}

<p>
Poiché le variabili sono indipendenti, possiamo scrivere:
</p>
\begin{align}
F_Y(x) &= P(X_1 \leq x) \cdot P(X_2 \leq x) \cdot \ldots \cdot P(X_n \leq x) \\
&= F_X(x)^n \\
&= (1 - e^{-\lambda x})^n \quad \text{per } x \geq 0
\end{align}</li>

<li><p>
<b>Distribuzione del minimo:</b>
</p>

<p>
Definiamo \(Z = \min(X_1, X_2, \ldots, X_n)\). La funzione di ripartizione di \(Z\) è:
</p>
\begin{align}
F_Z(x) &= P(Z \leq x) \\
&= P(\min(X_1, X_2, \ldots, X_n) \leq x) \\
&= 1 - P(\min(X_1, X_2, \ldots, X_n) > x) \\
&= 1 - P(X_1 > x, X_2 > x, \ldots, X_n > x)
\end{align}

<p>
Sfruttando l'indipendenza:
</p>
\begin{align}
F_Z(x) &= 1 - P(X_1 > x) \cdot P(X_2 > x) \cdot \ldots \cdot P(X_n > x) \\
&= 1 - (1 - F_X(x))^n \\
&= 1 - (1 - (1 - e^{-\lambda x}))^n \\
&= 1 - (e^{-\lambda x})^n \\
&= 1 - e^{-n\lambda x} \quad \text{per } x \geq 0
\end{align}

<p>
Questo dimostra che \(Z \sim \text{Exp}(n\lambda)\). Un risultato notevole: il minimo di \(n\) variabili esponenziali i.i.d. è ancora una variabile esponenziale con parametro moltiplicato per \(n\).
</p></li>
</ol>

<p>
(Un po' diverso dai fogli, da controllare)
</p>

<p>
<b>Proprietà di scala:</b>
</p>

<p>
Se \(X \sim \text{Exp}(\lambda)\) e \(Y = cX\) con \(c > 0\), allora \(Y \sim \text{Exp}(\lambda/c)\).
</p>

<p>
<b>Dimostrazione:</b>
</p>
\begin{align}
F_Y(x) &= P(Y \leq x) \\
&= P(cX \leq x) \\
&= P\left(X \leq \frac{x}{c}\right) \\
&= F_X\left(\frac{x}{c}\right) \\
&= 1 - e^{-\lambda \frac{x}{c}} \\
&= 1 - e^{-\frac{\lambda}{c}x}
\end{align}

<p>
Questo corrisponde alla funzione di ripartizione di una variabile esponenziale con parametro \(\frac{\lambda}{c}\).
</p>

<p>
<b>Proprietà di assenza di memoria:</b> Come per la distribuzione geometrica (suo analogo discreto), anche la distribuzione esponenziale gode della proprietà di assenza di memoria:
\[P(X > s+t | X > s) = P(X > t)\]
</p>

<p>
<b>Dimostrazione:</b>
</p>
\begin{align}
P(X > s+t | X > s) &= \frac{P(X > s+t)}{P(X > s)} \\
&= \frac{e^{-\lambda(s+t)}}{e^{-\lambda s}} \\
&= e^{-\lambda t} \\
&= P(X > t)
\end{align}

<p>
<b>Relazione con altre distribuzioni:</b>
</p>
<ul class="org-ul">
<li>Se \(X_1, X_2, \ldots, X_n\) sono variabili aleatorie esponenziali indipendenti con lo stesso parametro \(\lambda\), allora la loro somma segue una distribuzione Gamma con parametri \(n\) e \(\lambda\)</li>
<li>La distribuzione esponenziale è un caso particolare della distribuzione Gamma con parametro di forma uguale a 1</li>
</ul>

<p>
<b><b>Domande di approfondimento:</b></b>
</p>
<ol class="org-ol">
<li>Perché la distribuzione esponenziale è l'unica distribuzione continua con la proprietà di assenza di memoria?</li>
<li>Come cambierebbe il risultato sul minimo se le variabili esponenziali avessero parametri diversi?</li>
<li>Qual è l'interpretazione del parametro \(\lambda\) in termini di tasso di occorrenza degli eventi?</li>
</ol>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
<span style="color: #a020f0;">import</span> matplotlib.pyplot <span style="color: #a020f0;">as</span> plt
<span style="color: #a020f0;">import</span> scipy.stats <span style="color: #a020f0;">as</span> st

<span style="color: #b22222;"># </span><span style="color: #b22222;">Parametri della distribuzione esponenziale</span>
<span style="color: #a0522d;">lambda_val</span> = 0.5  <span style="color: #b22222;"># </span><span style="color: #b22222;">parametro della distribuzione</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Creazione dell'oggetto distribuzione esponenziale</span>
<span style="color: #a0522d;">exp_dist</span> = st.expon(scale=1/lambda_val)  <span style="color: #b22222;"># </span><span style="color: #b22222;">scipy.stats.expon usa scale=1/lambda</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo della PDF e CDF per diversi valori</span>
<span style="color: #a0522d;">x_values</span> = np.linspace(-1, 10, 1000)
<span style="color: #a0522d;">pdf_values</span> = exp_dist.pdf(x_values)
<span style="color: #a0522d;">cdf_values</span> = exp_dist.cdf(x_values)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo di momenti e statistiche</span>
<span style="color: #a0522d;">mean</span> = exp_dist.mean()
<span style="color: #a0522d;">var</span> = exp_dist.var()
<span style="color: #a0522d;">std</span> = np.sqrt(var)
<span style="color: #a0522d;">theo_mean</span> = 1/lambda_val
<span style="color: #a0522d;">theo_var</span> = 1/(lambda_val**2)

<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Valore atteso: </span>{mean}<span style="color: #8b2252;"> (teoricamente: </span>{theo_mean}<span style="color: #8b2252;">)"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Varianza: </span>{var}<span style="color: #8b2252;"> (teoricamente: </span>{theo_var}<span style="color: #8b2252;">)"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Generazione di variabili aleatorie</span>
<span style="color: #a0522d;">sample</span> = exp_dist.rvs(size=1000)
<span style="color: #a0522d;">sample_mean</span> = sample.mean()
<span style="color: #a0522d;">sample_var</span> = sample.var()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo di probabilit&#224; specifiche</span>
<span style="color: #a0522d;">point1</span> = 1/lambda_val  <span style="color: #b22222;"># </span><span style="color: #b22222;">media</span>
<span style="color: #a0522d;">prob_less_than_mean</span> = 1 - np.exp(-1)  <span style="color: #b22222;"># </span><span style="color: #b22222;">P(X &lt; 1/&#955;) = 1 - e^(-1)</span>
<span style="color: #a0522d;">prob_greater_than_mean</span> = np.exp(-1)  <span style="color: #b22222;"># </span><span style="color: #b22222;">P(X &gt; 1/&#955;) = e^(-1)</span>

<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">P(X &lt; media) = </span>{prob_less_than_mean:.4f}<span style="color: #8b2252;"> (esattamente 1-1/e &#8776; 0.6321)"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X &gt; media) = </span>{prob_greater_than_mean:.4f}<span style="color: #8b2252;"> (esattamente 1/e &#8776; 0.3679)"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Visualizzazione della distribuzione</span>
plt.figure(figsize=(12, 5))

<span style="color: #b22222;"># </span><span style="color: #b22222;">Grafico della PDF</span>
plt.subplot(1, 2, 1)
plt.plot(x_values, pdf_values, <span style="color: #8b2252;">'b-'</span>, label=<span style="color: #8b2252;">'PDF'</span>)
plt.axvline(x=mean, color=<span style="color: #8b2252;">'r'</span>, linestyle=<span style="color: #8b2252;">'--'</span>, label=f<span style="color: #8b2252;">'Media = </span>{mean}<span style="color: #8b2252;">'</span>)
plt.fill_between(x_values, pdf_values, alpha=0.2)
plt.xlabel(<span style="color: #8b2252;">'x'</span>)
plt.ylabel(<span style="color: #8b2252;">'f(x)'</span>)
plt.title(f<span style="color: #8b2252;">'PDF - Esponenziale con &#955;=</span>{lambda_val}<span style="color: #8b2252;">'</span>)
plt.grid(alpha=0.3)
plt.legend()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Grafico della CDF</span>
plt.subplot(1, 2, 2)
plt.plot(x_values, cdf_values, <span style="color: #8b2252;">'g-'</span>, label=<span style="color: #8b2252;">'CDF'</span>)
plt.axvline(x=mean, color=<span style="color: #8b2252;">'r'</span>, linestyle=<span style="color: #8b2252;">'--'</span>, label=f<span style="color: #8b2252;">'Media = </span>{mean}<span style="color: #8b2252;">'</span>)
plt.axhline(y=1-np.exp(-1), color=<span style="color: #8b2252;">'orange'</span>, linestyle=<span style="color: #8b2252;">'-.'</span>, 
            label=f<span style="color: #8b2252;">'1-1/e &#8776; </span>{1-np.exp(-1):.4f}<span style="color: #8b2252;">'</span>)
plt.xlabel(<span style="color: #8b2252;">'x'</span>)
plt.ylabel(<span style="color: #8b2252;">'F(x)'</span>)
plt.title(f<span style="color: #8b2252;">'CDF - Esponenziale con &#955;=</span>{lambda_val}<span style="color: #8b2252;">'</span>)
plt.grid(alpha=0.3)
plt.legend()

plt.tight_layout()
plt.show()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Dimostrazione della propriet&#224; di assenza di memoria</span>
<span style="color: #a0522d;">s</span>, <span style="color: #a0522d;">t</span> = 2, 3
<span style="color: #a0522d;">p_greater_t</span> = np.exp(-lambda_val * t)
<span style="color: #a0522d;">p_greater_s</span> = np.exp(-lambda_val * s)
<span style="color: #a0522d;">p_greater_s_plus_t</span> = np.exp(-lambda_val * (s + t))
<span style="color: #a0522d;">p_conditional</span> = p_greater_s_plus_t / p_greater_s

<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">Dimostrazione della propriet&#224; di assenza di memoria:"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X &gt; </span>{t}<span style="color: #8b2252;">) = e^(-&#955;&#183;</span>{t}<span style="color: #8b2252;">) = </span>{p_greater_t:.6f}<span style="color: #8b2252;">"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X &gt; </span>{s+t}<span style="color: #8b2252;"> | X &gt; </span>{s}<span style="color: #8b2252;">) = </span>{p_conditional:.6f}<span style="color: #8b2252;">"</span>)
</pre>
</div>
</div>
</div>
<div id="outline-container-org451db8e" class="outline-4">
<h4 id="org451db8e">Gaussiano</h4>
<div class="outline-text-4" id="text-org451db8e">
<p>
<b>Definizione (Distribuzione Gaussiana o Normale):</b> Una variabile aleatoria \(X\) segue una distribuzione gaussiana con parametri \(\mu\) (<a href="#org440f691">media</a>) e \(\sigma > 0\) (<a href="#orged12c36">deviazione standard</a>), scriviamo \(X \sim \mathcal{G}(\mu, \sigma)\), se la sua funzione di densità di probabilità è:
</p>

<p>
\[f_X(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}\]
</p>

<p>
Il codominio della funzione di densità è \(\mathbb{R}^+\). Si noti che \(\lim_{x \to \pm\infty} f_X(x) = 0\), quindi la distribuzione è asintotica rispetto all'asse delle ascisse.
</p>

<p>
<b>Analisi dei massimi e minimi:</b>
Calcoliamo la derivata prima della funzione di densità:
</p>
\begin{align}
f'_X(x) &= \frac{d}{dx}\left[\frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}\right] \\
&= \frac{1}{\sigma\sqrt{2\pi}} \cdot e^{-\frac{(x-\mu)^2}{2\sigma^2}} \cdot \left(-\frac{2(x-\mu)}{2\sigma^2}\right) \\
&= -\frac{x-\mu}{\sigma^2} \cdot \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
\end{align}

<p>
Ponendo \(f'_X(x) = 0\):
\[-\frac{x-\mu}{\sigma^2} \cdot \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}} = 0\]
</p>

<p>
Poiché il fattore esponenziale è sempre positivo, l'equazione è soddisfatta solo quando \(x-\mu = 0\), quindi \(x = \mu\).
</p>

<p>
Inoltre:
</p>
<ul class="org-ul">
<li>\(f'_X(x) > 0\) quando \(x < \mu\) (funzione crescente)</li>
<li>\(f'_X(x) < 0\) quando \(x > \mu\) (funzione decrescente)</li>
</ul>

<p>
Quindi \(x = \mu\) è punto di massimo della funzione di densità.
</p>

<p>
<b>Analisi della convessità:</b>
Calcoliamo la derivata seconda:
</p>
\begin{align}
f''_X(x) &= \frac{d}{dx}\left[-\frac{x-\mu}{\sigma^2} \cdot \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}\right] \\
&= -\frac{1}{\sigma^2} \cdot \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}} + \frac{x-\mu}{\sigma^2} \cdot \frac{x-\mu}{\sigma^2} \cdot \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}} \\
&= \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}} \cdot \left(\frac{(x-\mu)^2}{\sigma^4} - \frac{1}{\sigma^2}\right)
\end{align}

<p>
Ponendo \(f''_X(x) = 0\):
\[\frac{(x-\mu)^2}{\sigma^4} - \frac{1}{\sigma^2} = 0\]
</p>

<p>
Risolvendo:
\[(x-\mu)^2 = \sigma^2\]
\[x = \mu \pm \sigma\]
</p>

<p>
Quindi abbiamo due punti di flesso in \(x = \mu - \sigma\) e \(x = \mu + \sigma\).
</p>

<p>
Inoltre:
</p>
<ul class="org-ul">
<li>\(f''_X(x) < 0\) quando \(\mu - \sigma < x < \mu + \sigma\) (funzione concava)</li>
<li>\(f''_X(x) > 0\) quando \(x < \mu - \sigma\) o \(x > \mu + \sigma\) (funzione convessa)</li>
</ul>

<p>
<b>Effetti dei parametri sul grafico:</b>
</p>
<ul class="org-ul">
<li>Cambiare \(\mu\) trasla orizzontalmente il grafico: aumentare \(\mu\) sposta la curva verso destra, diminuire \(\mu\) la sposta verso sinistra.</li>
<li>Cambiare \(\sigma\) modifica la forma della curva: 
<ul class="org-ul">
<li>Aumentando \(\sigma\), i punti di flesso \(\mu \pm \sigma\) si allontanano da \(\mu\)</li>
<li>Aumentando \(\sigma\), la curva diventa più bassa al centro (in \(\mu\)) e più alta nelle code, per mantenere l'area totale uguale a 1</li>
<li>Diminuendo \(\sigma\), la curva diventa più concentrata attorno a \(\mu\) e più ripida</li>
</ul></li>
</ul>

<p>
<b>Normalizzazione della densità:</b>
La dimostrazione che l'area totale sotto la curva di densità è uguale a 1, ovvero \(\int_{-\infty}^{+\infty} f_X(x) \, dx = 1\), è un calcolo complesso. L'approccio classico utilizza un integrale doppio e il passaggio alle coordinate polari:
\[\int_{-\infty}^{+\infty} e^{-\frac{x^2}{2}} \, dx = \sqrt{2\pi}\]
</p>

<p>
Questo risultato garantisce che la funzione di densità sia correttamente normalizzata.
</p>

<p>
<b>Trasformazioni lineari:</b>
Se \(X \sim \mathcal{G}(\mu, \sigma)\) e \(Y = aX + b\) con \(a \neq 0\), allora \(Y \sim \mathcal{G}(a\mu + b, |a|\sigma)\).
</p>

<p>
<b>Somma di variabili gaussiane indipendenti:</b>
Se \(X_1 \sim \mathcal{G}(\mu_1, \sigma_1)\) e \(X_2 \sim \mathcal{G}(\mu_2, \sigma_2)\) sono indipendenti, allora:
\[X_1 + X_2 \sim \mathcal{G}(\mu_1 + \mu_2, \sqrt{\sigma_1^2 + \sigma_2^2})\]
</p>

<p>
Questo risultato si estende alla somma di un numero arbitrario di variabili gaussiane indipendenti:
\[\sum_{i=1}^n X_i \sim \mathcal{G}\left(\sum_{i=1}^n \mu_i, \sqrt{\sum_{i=1}^n \sigma_i^2}\right)\]
</p>

<p>
È importante sottolineare che la linearità del valore atteso e l'additività delle varianze per variabili indipendenti valgono per qualsiasi distribuzione, non solo per le gaussiane. Tuttavia, la peculiarità della distribuzione gaussiana è la <b>riproducibilità</b>: la somma di variabili gaussiane indipendenti è ancora una variabile gaussiana. Questa proprietà non è generalmente vera per altre distribuzioni.
</p>

<p>
<b>Standardizzazione:</b>
Se \(X \sim \mathcal{G}(\mu, \sigma)\), allora la trasformazione:
\[Z = \frac{X-\mu}{\sigma}\]
produce una variabile aleatoria \(Z \sim \mathcal{G}(0, 1)\), chiamata normale standard.
</p>

<p>
<b>Proprietà principali:</b>
</p>
<ul class="org-ul">
<li><b>Supporto:</b> \(X \in (-\infty, \infty)\)</li>
<li><b>Valore atteso:</b> \(E[X] = \mu\)</li>
<li><b><a href="#org41a8e1b">Varianza</a>:</b> \(Var(X) = \sigma^2\)</li>
</ul>

<p>
La funzione di ripartizione non ha una forma chiusa esprimibile in termini di funzioni elementari, ma si può scrivere come:
\[F_X(x) = \Phi\left(\frac{x-\mu}{\sigma}\right)\]
dove \(\Phi\) è la funzione di ripartizione della distribuzione normale standard \(\mathcal{G}(0,1)\).
</p>

<p>
<b>Proprietà di simmetria della funzione di ripartizione:</b>
Per la normale standard, vale la relazione:
\[\Phi(-x) = 1 - \Phi(x)\]
</p>

<p>
Questa proprietà deriva dalla simmetria della densità gaussiana rispetto all'origine.
</p>

<p>
<b>Calcolo degli intervalli di confidenza e relazione con la regola empirica:</b>
</p>

<p>
Possiamo trasformare un campione statistico in una variabile aleatoria e porci domande in funzione della sua probabilità. Ad esempio, possiamo chiederci qual è la probabilità che una variabile gaussiana si trovi entro \(n\) deviazioni standard dalla <a href="#org440f691">media</a>:
\[P(|X-\mu| \leq n\sigma)\]
</p>

<p>
Questo ci indica quale percentuale della popolazione si trova nell'intervallo \([\mu-n\sigma, \mu+n\sigma]\).
</p>

<p>
Standardizzando la variabile:
\[P(|X-\mu| \leq n\sigma) = P\left(\left|\frac{X-\mu}{\sigma}\right| \leq n\right) = P(-n \leq Z \leq n)\]
</p>

<p>
dove \(Z\) è una normale standard. Quindi:
\[P(-n \leq Z \leq n) = \Phi(n) - \Phi(-n) = \Phi(n) - (1 - \Phi(n)) = 2\Phi(n) - 1\]
</p>

<p>
Calcolando questo valore per \(n = 1, 2, 3\), otteniamo i risultati della regola empirica:
</p>
<ul class="org-ul">
<li>Per \(n = 1\): \(2\Phi(1) - 1 \approx 0.68\) (68% dei valori)</li>
<li>Per \(n = 2\): \(2\Phi(2) - 1 \approx 0.95\) (95% dei valori)</li>
<li>Per \(n = 3\): \(2\Phi(3) - 1 \approx 0.997\) (99.7% dei valori)</li>
</ul>

<p>
<b>Regola empirica (regola 68-95-99.7):</b> Per una variabile aleatoria normale:
</p>
<ul class="org-ul">
<li>Circa il 68% dei valori si trova entro 1 <a href="#orged12c36">deviazione standard</a> dalla <a href="#org440f691">media</a></li>
<li>Circa il 95% dei valori si trova entro 2 deviazioni standard dalla <a href="#org440f691">media</a></li>
<li>Circa il 99.7% dei valori si trova entro 3 deviazioni standard dalla <a href="#org440f691">media</a></li>
</ul>


<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
<span style="color: #a020f0;">import</span> matplotlib.pyplot <span style="color: #a020f0;">as</span> plt
<span style="color: #a020f0;">import</span> scipy.stats <span style="color: #a020f0;">as</span> st

<span style="color: #b22222;"># </span><span style="color: #b22222;">Parametri della distribuzione gaussiana</span>
<span style="color: #a0522d;">mu</span> = 0      <span style="color: #b22222;"># </span><span style="color: #b22222;">media</span>
<span style="color: #a0522d;">sigma</span> = 1   <span style="color: #b22222;"># </span><span style="color: #b22222;">deviazione standard</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Creazione dell'oggetto distribuzione gaussiana</span>
<span style="color: #a0522d;">norm_dist</span> = st.norm(loc=mu, scale=sigma)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo della PDF e CDF per diversi valori</span>
<span style="color: #a0522d;">x_values</span> = np.linspace(mu-4*sigma, mu+4*sigma, 1000)
<span style="color: #a0522d;">pdf_values</span> = norm_dist.pdf(x_values)
<span style="color: #a0522d;">cdf_values</span> = norm_dist.cdf(x_values)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo di probabilit&#224; per la regola empirica</span>
<span style="color: #a0522d;">p_1sigma</span> = 2 * norm_dist.cdf(mu + sigma) - 1
<span style="color: #a0522d;">p_2sigma</span> = 2 * norm_dist.cdf(mu + 2*sigma) - 1
<span style="color: #a0522d;">p_3sigma</span> = 2 * norm_dist.cdf(mu + 3*sigma) - 1

<span style="color: #483d8b;">print</span>(<span style="color: #8b2252;">"Regola empirica (68-95-99.7):"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(&#956;-&#963; &lt; X &lt; &#956;+&#963;) = </span>{p_1sigma:.6f}<span style="color: #8b2252;"> &#8776; 68.27%"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(&#956;-2&#963; &lt; X &lt; &#956;+2&#963;) = </span>{p_2sigma:.6f}<span style="color: #8b2252;"> &#8776; 95.45%"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(&#956;-3&#963; &lt; X &lt; &#956;+3&#963;) = </span>{p_3sigma:.6f}<span style="color: #8b2252;"> &#8776; 99.73%"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Visualizzazione della distribuzione gaussiana</span>
plt.figure(figsize=(12, 5))

<span style="color: #b22222;"># </span><span style="color: #b22222;">Grafico della PDF</span>
plt.subplot(1, 2, 1)
plt.plot(x_values, pdf_values, <span style="color: #8b2252;">'b-'</span>, label=<span style="color: #8b2252;">'PDF'</span>)
plt.axvline(x=mean, color=<span style="color: #8b2252;">'r'</span>, linestyle=<span style="color: #8b2252;">'--'</span>, label=f<span style="color: #8b2252;">'Media (&#956;) = </span>{mu}<span style="color: #8b2252;">'</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Aggiungiamo le aree per la regola empirica</span>
plt.fill_between(x_values[(x_values &gt;= mu-sigma) &amp; (x_values &lt;= mu+sigma)], 
                 pdf_values[(x_values &gt;= mu-sigma) &amp; (x_values &lt;= mu+sigma)], 
                 alpha=0.3, color=<span style="color: #8b2252;">'green'</span>, label=<span style="color: #8b2252;">'68.27% (&#177;1&#963;)'</span>)
plt.fill_between(x_values[(x_values &gt;= mu-2*sigma) &amp; (x_values &lt;= mu-sigma) | 
                          (x_values &gt;= mu+sigma) &amp; (x_values &lt;= mu+2*sigma)], 
                 pdf_values[(x_values &gt;= mu-2*sigma) &amp; (x_values &lt;= mu-sigma) | 
                            (x_values &gt;= mu+sigma) &amp; (x_values &lt;= mu+2*sigma)], 
                 alpha=0.2, color=<span style="color: #8b2252;">'blue'</span>, label=<span style="color: #8b2252;">'27.18% (&#177;2&#963;)'</span>)
plt.fill_between(x_values[(x_values &gt;= mu-3*sigma) &amp; (x_values &lt;= mu-2*sigma) | 
                          (x_values &gt;= mu+2*sigma) &amp; (x_values &lt;= mu+3*sigma)], 
                 pdf_values[(x_values &gt;= mu-3*sigma) &amp; (x_values &lt;= mu-2*sigma) | 
                            (x_values &gt;= mu+2*sigma) &amp; (x_values &lt;= mu+3*sigma)], 
                 alpha=0.1, color=<span style="color: #8b2252;">'purple'</span>, label=<span style="color: #8b2252;">'4.28% (&#177;3&#963;)'</span>)

plt.axvline(x=mu-sigma, color=<span style="color: #8b2252;">'green'</span>, linestyle=<span style="color: #8b2252;">':'</span>, label=<span style="color: #8b2252;">'&#956;&#177;&#963;'</span>)
plt.axvline(x=mu+sigma, color=<span style="color: #8b2252;">'green'</span>, linestyle=<span style="color: #8b2252;">':'</span>)
plt.xlabel(<span style="color: #8b2252;">'x'</span>)
plt.ylabel(<span style="color: #8b2252;">'f(x)'</span>)
plt.title(f<span style="color: #8b2252;">'PDF - Gaussiana con &#956;=</span>{mu}<span style="color: #8b2252;">, &#963;=</span>{sigma}<span style="color: #8b2252;">'</span>)
plt.grid(alpha=0.3)
plt.legend()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Grafico della CDF</span>
plt.subplot(1, 2, 2)
plt.plot(x_values, cdf_values, <span style="color: #8b2252;">'g-'</span>, label=<span style="color: #8b2252;">'CDF'</span>)
plt.axvline(x=mean, color=<span style="color: #8b2252;">'r'</span>, linestyle=<span style="color: #8b2252;">'--'</span>, label=f<span style="color: #8b2252;">'Media (&#956;) = </span>{mu}<span style="color: #8b2252;">'</span>)
plt.axhline(y=0.5, color=<span style="color: #8b2252;">'r'</span>, linestyle=<span style="color: #8b2252;">':'</span>)
plt.axvline(x=mu-sigma, color=<span style="color: #8b2252;">'green'</span>, linestyle=<span style="color: #8b2252;">':'</span>, label=<span style="color: #8b2252;">'&#956;&#177;&#963;'</span>)
plt.axvline(x=mu+sigma, color=<span style="color: #8b2252;">'green'</span>, linestyle=<span style="color: #8b2252;">':'</span>)
plt.xlabel(<span style="color: #8b2252;">'x'</span>)
plt.ylabel(<span style="color: #8b2252;">'F(x)'</span>)
plt.title(f<span style="color: #8b2252;">'CDF - Gaussiana con &#956;=</span>{mu}<span style="color: #8b2252;">, &#963;=</span>{sigma}<span style="color: #8b2252;">'</span>)
plt.grid(alpha=0.3)
plt.legend()

plt.tight_layout()
plt.show()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Esempio di somma di variabili gaussiane indipendenti</span>
<span style="color: #a0522d;">mu1</span>, <span style="color: #a0522d;">sigma1</span> = 0, 1
<span style="color: #a0522d;">mu2</span>, <span style="color: #a0522d;">sigma2</span> = 2, 1.5
<span style="color: #a0522d;">n_samples</span> = 1000

<span style="color: #b22222;"># </span><span style="color: #b22222;">Creazione delle distribuzioni</span>
<span style="color: #a0522d;">norm_X1</span> = st.norm(loc=mu1, scale=sigma1)
<span style="color: #a0522d;">norm_X2</span> = st.norm(loc=mu2, scale=sigma2)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Risultati teorici per la somma</span>
<span style="color: #a0522d;">mu_sum</span> = mu1 + mu2
<span style="color: #a0522d;">sigma_sum</span> = np.sqrt(sigma1**2 + sigma2**2)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Generazione di campioni</span>
<span style="color: #a0522d;">sample_X1</span> = norm_X1.rvs(size=n_samples)
<span style="color: #a0522d;">sample_X2</span> = norm_X2.rvs(size=n_samples)
<span style="color: #a0522d;">sample_sum</span> = sample_X1 + sample_X2  <span style="color: #b22222;"># </span><span style="color: #b22222;">somma delle variabili</span>

<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">Somma di variabili gaussiane indipendenti:"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"X&#8321; ~ N(</span>{mu1}<span style="color: #8b2252;">, </span>{sigma1}<span style="color: #8b2252;">&#178;), X&#8322; ~ N(</span>{mu2}<span style="color: #8b2252;">, </span>{sigma2}<span style="color: #8b2252;">&#178;)"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Media della somma: </span>{sample_sum.mean():.4f}<span style="color: #8b2252;"> (teoricamente: </span>{mu_sum}<span style="color: #8b2252;">)"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Deviazione standard della somma: </span>{sample_sum.std():.4f}<span style="color: #8b2252;"> (teoricamente: </span>{sigma_sum}<span style="color: #8b2252;">)"</span>)
</pre>
</div>
</div>
</div>
<div id="outline-container-orgf895136" class="outline-4">
<h4 id="orgf895136">Considerazioni su riproducibilità (da integrare)</h4>
<div class="outline-text-4" id="text-orgf895136">
<p>
La riproducibilità è una proprietà fondamentale di alcune distribuzioni di probabilità che si mantengono "della stessa famiglia" sotto certe operazioni, come la somma di variabili aleatorie indipendenti.
</p>

<p>
## Definizione formale di riproducibilità per la binomiale
</p>

<p>
<b>Teorema (Riproducibilità della distribuzione binomiale):</b> Se \(X_1 \sim \text{Bin}(n_1, p)\) e \(X_2 \sim \text{Bin}(n_2, p)\) sono variabili aleatorie binomiali indipendenti con lo stesso parametro di probabilità \(p\), allora la loro somma segue ancora una distribuzione binomiale:
</p>

<p>
\[X_1 + X_2 \sim \text{Bin}(n_1 + n_2, p)\]
</p>

<p>
## Dimostrazione intuitiva
</p>

<p>
La dimostrazione concettuale di questa proprietà è abbastanza intuitiva:
</p>

<ol class="org-ol">
<li>\(X_1\) rappresenta il numero di successi in \(n_1\) prove indipendenti, ciascuna con probabilità \(p\)</li>
<li>\(X_2\) rappresenta il numero di successi in \(n_2\) prove indipendenti, ciascuna con probabilità \(p\)</li>
<li>Quindi \(X_1 + X_2\) rappresenta il numero totale di successi in \(n_1 + n_2\) prove indipendenti, ciascuna con probabilità \(p\)</li>
</ol>

<p>
Questo corrisponde esattamente alla definizione di una variabile binomiale \(\text{Bin}(n_1 + n_2, p)\).
</p>

<p>
## Dimostrazione formale
</p>

<p>
Possiamo anche dimostrarlo usando le funzioni generatrici di probabilità (PGF):
</p>
<ul class="org-ul">
<li>La PGF di \(X_1 \sim \text{Bin}(n_1, p)\) è \(G_{X_1}(t) = (1-p+pt)^{n_1}\)</li>
<li>La PGF di \(X_2 \sim \text{Bin}(n_2, p)\) è \(G_{X_2}(t) = (1-p+pt)^{n_2}\)</li>
<li>La PGF della somma di variabili indipendenti è il prodotto delle PGF:
\[G_{X_1+X_2}(t) = G_{X_1}(t) \cdot G_{X_2}(t) = (1-p+pt)^{n_1} \cdot (1-p+pt)^{n_2} = (1-p+pt)^{n_1+n_2}\]</li>
</ul>

<p>
Questa è la PGF di una \(\text{Bin}(n_1 + n_2, p)\).
</p>

<p>
## Condizioni necessarie e limitazioni
</p>

<p>
È importante notare che la riproducibilità della binomiale richiede due condizioni fondamentali:
</p>

<ol class="org-ol">
<li><b><b>Stesso parametro \(p\)</b></b>: Le variabili devono avere la stessa probabilità di successo</li>
<li><b><b>Indipendenza</b></b>: Le variabili aleatorie devono essere indipendenti</li>
</ol>

<p>
Se \(X_1 \sim \text{Bin}(n_1, p_1)\) e \(X_2 \sim \text{Bin}(n_2, p_2)\) con \(p_1 \neq p_2\), la loro somma <b><b>non</b></b> segue una distribuzione binomiale.
</p>

<p>
## Confronto con altre distribuzioni
</p>

<p>
La riproducibilità è una caratteristica che la binomiale condivide con altre distribuzioni importanti:
</p>

<ul class="org-ul">
<li><b><b>Gaussiana</b></b>: \(X_1 \sim \mathcal{G}(\mu_1, \sigma_1)\) e \(X_2 \sim \mathcal{G}(\mu_2, \sigma_2)\) indipendenti ⟹ \(X_1 + X_2 \sim \mathcal{G}(\mu_1 + \mu_2, \sqrt{\sigma_1^2 + \sigma_2^2})\)</li>
<li><b><b>Poisson</b></b>: \(X_1 \sim \text{Pois}(\lambda_1)\) e \(X_2 \sim \text{Pois}(\lambda_2)\) indipendenti ⟹ \(X_1 + X_2 \sim \text{Pois}(\lambda_1 + \lambda_2)\)</li>
<li><b><b>Gamma</b></b>: Con stesso parametro di scala</li>
</ul>

<p>
## Applicazioni pratiche
</p>

<p>
Questa proprietà ha importanti applicazioni pratiche:
</p>

<ol class="org-ol">
<li>Semplifica il calcolo di distribuzioni di variabili somma</li>
<li>Permette di analizzare processi composti da più fasi binomiali indipendenti</li>
<li>Facilita la modellazione di conteggi totali in esperimenti di gruppo</li>
</ol>

<p>
## Domande di approfondimento
</p>

<ol class="org-ol">
<li>Come cambierebbe il risultato se le variabili binomiali avessero parametri \(p\) diversi?</li>
<li>Quali altre distribuzioni di probabilità godono della proprietà di riproducibilità?</li>
<li>Come si collegano la riproducibilità della binomiale e il teorema del limite centrale?</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-orge68f6db" class="outline-3">
<h3 id="orge68f6db">Tecniche python</h3>
<div class="outline-text-3" id="text-orge68f6db">
</div>
<div id="outline-container-orgcd62758" class="outline-4">
<h4 id="orgcd62758">Calcola funzione di ripartizione usando massa</h4>
<div class="outline-text-4" id="text-orgcd62758">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #483d8b;">sum</span>([X.pmf(i) <span style="color: #a020f0;">for</span> i <span style="color: #a020f0;">in</span> <span style="color: #483d8b;">range</span>(7)])
</pre>
</div>
</div>
</div>
<div id="outline-container-orga16c0a3" class="outline-4">
<h4 id="orga16c0a3">Inversa della funzione di ripartizione</h4>
<div class="outline-text-4" id="text-orga16c0a3">
<p>
<code>X.ppf(0.8)</code>
Per rispondere alla domanda: si determini il più piccolo dei valori per cui la funzione di ripartizione vale x.
Ovvero dato il numero r, trovare quel valore della variabile aleatoria tale per cui lo 0.x % dei valori ne sono più piccoli. Proprio il concetto di <b>quantile</b>
</p>
</div>
</div>
<div id="outline-container-org429311b" class="outline-4">
<h4 id="org429311b">(AGGIUNGERE CODICE) Stabilire se una popolazione può essere descritta da una variabile aleatoria</h4>
<div class="outline-text-4" id="text-org429311b">
<p>
Tracciare un grafico con l'asse delle ascisse recante i <a href="#orgbfbc9e0">quantili</a> del campione e quello delle ordinate quelli teorici della specifica distribuzione iptetica
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orgb225ee8" class="outline-2">
<h2 id="orgb225ee8"><span class="todo TODO">TODO</span> Creare una panoramica con tutti i grafici delle distribuzioni che abbiamo guardato fatte in python</h2>
</div>
<div id="outline-container-org7b7f9ea" class="outline-2">
<h2 id="org7b7f9ea"><span class="todo TODO">TODO</span> Creare boxplot di tutte le distribuzioni in python</h2>
</div>
<div id="outline-container-org4e12c02" class="outline-2">
<h2 id="org4e12c02"><span class="todo TODO">TODO</span> Creare ANKI usando copilot</h2>
</div>
<div id="outline-container-org98baca8" class="outline-2">
<h2 id="org98baca8"><span class="todo TODO">TODO</span> grafici interattivi (usare possibilmente all'esame)</h2>
</div>
<div id="outline-container-org9e2228d" class="outline-2">
<h2 id="org9e2228d"><span class="todo TODO">TODO</span> mathcal per E</h2>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Created: 2025-06-07 Sat 16:49</p>
</div>
</body>
</html>
