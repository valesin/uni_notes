<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="it" xml:lang="it">
<head>
<!-- 2025-06-08 Sun 22:00 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Statistica</title>
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" href="https://cdn.simplecss.org/simple.min.css" onerror="this.onerror=null;this.href='local.css';" />
<script>
    window.MathJax = {
      tex: {
        ams: { multlineWidth: '85%' },
        {packages: {'[+]': ['mathtools']}},
        tags: 'ams',
        tagSide: 'right',
        tagIndent: '.8em'
      },
      chtml: {
        scale: 1.0,
        displayAlign: 'center',
        displayIndent: '0em'
      },
      svg: {
        scale: 1.0,
        displayAlign: 'center',
        displayIndent: '0em'
      },
      output: {
        font: 'mathjax-modern',
        displayOverflow: 'scale'
      },
      loader: {
        load: ['[tex]/mathtools']
      },
    };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Statistica</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgff8a2ba">Python</a>
<ul>
<li><a href="#orgc08f01b">Tipi primitivi di collezioni</a></li>
<li><a href="#org7893313">NumPy</a></li>
<li><a href="#org3b17513">Pandas</a>
<ul>
<li><a href="#org174774b">Series</a>
<ul>
<li><a href="#org448df05">Creazione</a></li>
<li><a href="#orgc9e01a9">Accesso</a>
<ul>
<li><a href="#org987eed6">Accedere ad un valore specifico</a></li>
<li><a href="#orgd396d2f">Accedere ad una sottocollezione</a></li>
</ul>
</li>
<li><a href="#orga7a5000">Proprietà</a></li>
<li><a href="#orge324c00">Visualizzazione</a></li>
<li><a href="#orgaf56610">Operazioni su dati</a>
<ul>
<li><a href="#org240e4b9">Raccogliere valori in bins</a></li>
<li><a href="#org2e87c54">Scremare e contare</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orga0968b7">Dataframe</a>
<ul>
<li><a href="#org2b9ac2f">Lettura da CSV</a></li>
<li><a href="#org3313761">Creazione</a></li>
<li><a href="#orgc0691ad">Accesso</a>
<ul>
<li><a href="#orgba4c936">Colonne</a></li>
<li><a href="#org10d56e5">Righe</a></li>
<li><a href="#orgc48681e">Elementi specifici</a></li>
</ul>
</li>
<li><a href="#orgbb6609a">Filtri e selezione</a></li>
<li><a href="#org056ffe5">Riordinamento</a>
<ul>
<li><a href="#org38841f7">Riordina indici (righe)</a></li>
<li><a href="#org488a726">Riordina colonne</a></li>
</ul>
</li>
<li><a href="#org7f82827">Conta numero di casi e colonne</a></li>
<li><a href="#orgc050509">Ritorna valori unici di un attributo</a></li>
<li><a href="#orga5cbf69">Capisci esistenza valori nulli</a></li>
<li><a href="#org92865b5">Trova tipi dato colonne</a></li>
<li><a href="#org4138ab1">Filtra colonne per tipo</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#orge68f6db">Tecniche python</a>
<ul>
<li><a href="#orgcd62758">Calcola funzione di ripartizione usando massa</a></li>
<li><a href="#orga16c0a3">Inversa della funzione di ripartizione</a></li>
<li><a href="#org429311b">(AGGIUNGERE CODICE) Stabilire se una popolazione può essere descritta da una variabile aleatoria</a></li>
<li><a href="#org487ad7a">Mostrare media non robusta in distribuzione skewed</a></li>
</ul>
</li>
<li><a href="#org6927619">Grafici Statistici in Python: Guida Rapida (Approccio OOP)</a>
<ul>
<li><a href="#org4cf7804">Blueprint</a>
<ul>
<li><a href="#org959cd8d">Un solo grafico</a></li>
<li><a href="#org7609554">Due grafici</a></li>
</ul>
</li>
<li><a href="#orgcff9f4e">A bastoncini (Stem plots)</a></li>
<li><a href="#org51a8b7d">A barre (Bar charts)</a></li>
<li><a href="#org1a80a59">Poligonale (Line plots)</a></li>
<li><a href="#org2759f15">A torta (Pie charts)</a></li>
<li><a href="#org58062db">Istogramma (Histogram)</a></li>
<li><a href="#orgfe65122">Grafici multipli (Multiple plots)</a></li>
<li><a href="#orgfb76f92">Scatter plot (Diagramma di dispersione)</a></li>
<li><a href="#orgdec16b2">Tabella comparativa per scelta grafico</a></li>
<li><a href="#org7f0975c">Linee guida rapide</a></li>
<li><a href="#orgd313567">Personalizzazione rapida degli assi e grafici</a></li>
</ul>
</li>
<li><a href="#orgb4f68ba">Statistica descrittiva (continuare con anki e revisione da qui)</a>
<ul>
<li><a href="#org28044a0">Dati quantitativi e qualitativi</a></li>
<li><a href="#orgcd936b4">Frequenze</a>
<ul>
<li><a href="#org6c873ff">Arrotondare il numero di cifre decimali</a></li>
<li><a href="#org743852a">Mostrare frequenza relativa in percentuali</a></li>
<li><a href="#orgfa65629">Rinormalizzare su un sottoinsieme delle osservazioni</a></li>
</ul>
</li>
<li><a href="#org53b88bb">Frequenze cumulate</a>
<ul>
<li><a href="#org96ad5bf"><span class="todo TODO">TODO</span> Funzione cumulativa empirica</a></li>
<li><a href="#org3398d1a">Diagrammi di Pareto</a></li>
</ul>
</li>
<li><a href="#orgf5ed488">Frequenze congiunte e marginali</a></li>
<li><a href="#orgf028389">Indici di centralità</a>
<ul>
<li><a href="#org2a78fed">Media campionaria</a></li>
<li><a href="#orgfc9cf19">Mediana campionaria</a></li>
<li><a href="#org0baab7f">Moda campionaria</a></li>
</ul>
</li>
<li><a href="#org589bb4c">Indici di dispersione</a>
<ul>
<li><a href="#org9e46e20">Varianza campionaria</a>
<ul>
<li><a href="#org2dc05a8">Perchè si usa il quadrato?</a></li>
</ul>
</li>
<li><a href="#org39dcb4a">Deviazione standard</a></li>
</ul>
</li>
<li><a href="#orgc8c377b">Quantili</a>
<ul>
<li><a href="#org0403396">Box plot</a>
<ul>
<li><a href="#org8dc1c5f"><span class="todo TODO">TODO</span> Capire cosa ha detto il professore riguardo alla relazione fra box plot e distribuzione e capire cosa c'entra la media</a></li>
</ul>
</li>
<li><a href="#org5023b89">Range inter-quartile</a></li>
<li><a href="#orgf51e9d1">QQ plot</a>
<ul>
<li><a href="#org11d9f65"><span class="todo TODO">TODO</span> Come stampare?</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgf40875c">Distribuzione normale</a></li>
<li><a href="#org8a94430">Coefficiente di correlazione campionaria</a></li>
<li><a href="#org74d6d2e"><span class="todo TODO">TODO</span> Lorenz e Gini</a></li>
<li><a href="#org1e7948c">Grafici</a>
<ul>
<li><a href="#org0d01e63">A bastoncini</a></li>
<li><a href="#org2aa18b4">A barre</a></li>
<li><a href="#org36f7236">Poligonale</a></li>
<li><a href="#org5ad55a6">A torta (aerogramma)</a></li>
<li><a href="#orgf6fae5f">Istogramma</a></li>
<li><a href="#org538f387">Grafici multipli</a></li>
<li><a href="#org5750333">Scatter plot</a></li>
<li><a href="#org4031975">Quando usarli</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org88aaa01">Probabilità</a></li>
<li><a href="#org9eea659">Variabili aleatorie</a>
<ul>
<li><a href="#org0712b1e">Funzione di ripartizione e funzioni di probabilità</a>
<ul>
<li><a href="#orga1d27bb">Concetti fondamentali per variabili aleatorie</a></li>
<li><a href="#org8b3219b">Variabili aleatorie discrete</a></li>
<li><a href="#orgfe1ac5a">Variabili aleatorie continue</a></li>
<li><a href="#orgb058a68">Confronto e differenze principali</a></li>
</ul>
</li>
<li><a href="#org30aaf5e">Funzione indicatrice</a></li>
<li><a href="#org31227f4">Valore atteso e varianza</a></li>
<li><a href="#orgc7e5d71">Variabili aleatorie multivariate</a>
<ul>
<li><a href="#orgfc3764d">Massa di probabilità congiunta e marginale</a></li>
<li><a href="#orga8e1458">Indipendenza tra variabili aleatorie</a></li>
<li><a href="#org3f57426">Valore atteso di una somma di variabili aleatorie</a></li>
<li><a href="#org18e3960">Valore atteso del prodotto di variabili aleatorie</a></li>
<li><a href="#org573876c">Stima di una variabile aleatoria</a></li>
<li><a href="#org39d0753">Covarianza: definizione e proprietà</a></li>
<li><a href="#org6549c85">Varianza di somme di variabili aleatorie</a></li>
<li><a href="#org92f0dee">Covarianza di funzioni indicatrici</a></li>
<li><a href="#orgebfc0f0">Coefficiente di correlazione</a></li>
</ul>
</li>
<li><a href="#org98ebc0b">Disuguaglianza di Markov</a></li>
<li><a href="#org16eba75">Disuguaglianza di Chebyshev</a></li>
<li><a href="#org88f7a28">Legge dei grandi numeri</a></li>
<li><a href="#org93cdc7f">Legge dei grandi numeri</a></li>
<li><a href="#org74800df">Modelli di variabili aleatorie</a>
<ul>
<li><a href="#org57a32f3">Modello di Bernoulli</a></li>
<li><a href="#org0c4b5a7">Binomiale</a></li>
<li><a href="#org1e8f7e9">Geometrico (guardare anche dispensa)</a>
<ul>
<li><a href="#orgc76f5cc">Definizione e concetti base</a></li>
<li><a href="#orgf519898">Funzione di massa di probabilità</a></li>
<li><a href="#orga4968ae">Valore atteso</a></li>
<li><a href="#org732100c">Varianza</a></li>
<li><a href="#org316685e">Funzione di ripartizione</a></li>
<li><a href="#org698e5cf">Proprietà di assenza di memoria</a></li>
<li><a href="#org2cc6ff4">Relazione con altre distribuzioni</a></li>
<li><a href="#org0942342">Applicazioni</a></li>
<li><a href="#org3107891">Python</a></li>
</ul>
</li>
<li><a href="#org7036508">Uniforme discreto</a></li>
<li><a href="#orgb2fb813">Poisson</a></li>
<li><a href="#org6b15bd9">Ipergeometrica</a></li>
<li><a href="#orge9c02b0">Continuo uniforme</a></li>
<li><a href="#org59e2742">Esponenziale</a></li>
<li><a href="#org451db8e">Gaussiano</a></li>
</ul>
</li>
<li><a href="#org5f6d451">Riproducibilità (da integrare)</a></li>
<li><a href="#orgbd29f40">Teorema centrale del limite (da integrare)</a></li>
<li><a href="#orge1dd96e">Assenza di memoria</a></li>
</ul>
</li>
<li><a href="#org117121d">Statistica inferenziale</a>
<ul>
<li><a href="#org8673dd5">Stimatore non deviato</a>
<ul>
<li><a href="#org43be099">Media campionaria come stimatore</a></li>
<li><a href="#orgc7bb3f2">Varianza campionaria come stimatore</a></li>
</ul>
</li>
<li><a href="#orga719f4c">Errore quadratico medio</a></li>
<li><a href="#orge69270b">Consistenza in media quadratica</a></li>
<li><a href="#orgf4bb999">Consistenza debole</a></li>
</ul>
</li>
<li><a href="#orgb225ee8"><span class="todo TODO">TODO</span> Creare una panoramica con tutti i grafici delle distribuzioni che abbiamo guardato fatte in python</a></li>
<li><a href="#org7b7f9ea"><span class="todo TODO">TODO</span> Creare boxplot di tutte le distribuzioni in python</a></li>
<li><a href="#org4e12c02"><span class="todo TODO">TODO</span> Creare ANKI usando copilot</a></li>
<li><a href="#org98baca8"><span class="todo TODO">TODO</span> grafici interattivi (usare possibilmente all'esame)</a></li>
<li><a href="#org9e2228d"><span class="todo TODO">TODO</span> mathcal per E</a></li>
</ul>
</div>
</div>
<div id="outline-container-orgff8a2ba" class="outline-2">
<h2 id="orgff8a2ba">Python</h2>
<div class="outline-text-2" id="text-orgff8a2ba">
</div>
<div id="outline-container-orgc08f01b" class="outline-3">
<h3 id="orgc08f01b">Tipi primitivi di collezioni</h3>
<div class="outline-text-3" id="text-orgc08f01b">
<dl class="org-dl">
<dt>tupla</dt><dd><p>
sequenza <b>immutabile</b> e <b>fixed-length</b>, delimitata da <b>parentesi tonde</b>.
</p>
<ul class="org-ul">
<li>Non può essere modificata dopo la creazione</li>
<li>Efficiente in termini di memoria e velocità</li>
<li>Supporta l'unpacking: <code>a, b, c = (1, 2, 3)</code></li>
<li>Con asterisco: <code>a, *b = (1, 2, 3, 4)</code> → <code>a  =1, b = [2, 3, 4]</code></li>
<li>Può essere concatenata: <code>(1, 2) + (3, 4) → (1, 2, 3, 4)</code></li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">hero</span> = (<span style="color: #8b2252;">'Iron Man'</span>, <span style="color: #8b2252;">'Tony Stark'</span>, 1963, 191.85)
<span style="color: #a0522d;">name</span>, <span style="color: #a0522d;">identity</span>, <span style="color: #a0522d;">year</span>, <span style="color: #a0522d;">_</span> = hero  <span style="color: #b22222;"># </span><span style="color: #b22222;">unpacking</span>
</pre>
</div></dd>
<dt>lista</dt><dd><p>
semanticamente simile alla tupla, ma <b>mutabile</b>.
</p>
<ul class="org-ul">
<li>Modificabile con metodi come <code>append()</code>, <code>insert()</code>, <code>pop()</code>, <code>remove()</code></li>
<li>Supporta lo slicing: <code>lista[1:4], lista[-3:]</code></li>
<li>Può contenere elementi eterogenei</li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">heroes</span> = [<span style="color: #8b2252;">'Iron Man'</span>, <span style="color: #8b2252;">'Thor'</span>, <span style="color: #8b2252;">'Captain America'</span>]
heroes.append(<span style="color: #8b2252;">'Hulk'</span>)  <span style="color: #b22222;"># </span><span style="color: #b22222;">Aggiunge elemento</span>
heroes.pop(1)  <span style="color: #b22222;"># </span><span style="color: #b22222;">Rimuove Thor</span>
</pre>
</div></dd>
<dt>dizionario</dt><dd>memorizza associazioni <b>chiave-valore</b> ed è delimitato da <b>parentesi graffe</b>.
<ul class="org-ul">
<li>Accesso per chiave in tempo costante</li>
<li>Chiavi immutabili (stringhe, numeri, tuple)</li>
<li>Valori di qualsiasi tipo</li>
</ul></dd>
</dl>
</div>
</div>
<div id="outline-container-org7893313" class="outline-3">
<h3 id="org7893313">NumPy</h3>
<div class="outline-text-3" id="text-org7893313">
<p>
Array NumPy: Strutture multidimensionali e omogenee che offrono performance superiori per calcoli numerici.
</p>

<p>
Vantaggi:
</p>
<dl class="org-dl">
<dt>Operazioni vettorizzate (elemento per elemento)</dt><dd><code>array * 2</code>, <code>array1 + array2</code></dd>
<dt>Funzioni matematiche avanzate</dt><dd><code>np.sin()</code>, <code>np.mean()</code>, etc.</dd>
<dt>(no term)</dt><dd>Integrazione con matplotlib per visualizzazioni</dd>
</dl>

<p>
Uno svantaggio è che quando si creano grafici con NumPy, i valori di due array vengono accoppiati in base alla posizione/indice, non per valore logico.
</p>
</div>
</div>
<div id="outline-container-org3b17513" class="outline-3">
<h3 id="org3b17513">Pandas</h3>
<div class="outline-text-3" id="text-org3b17513">
<p>
La libreria <b>Pandas</b> fornisce due strutture dati fondamentali, <b>Series</b> e <b>DataFrame</b>.
</p>
</div>
<div id="outline-container-org174774b" class="outline-4">
<h4 id="org174774b">Series</h4>
<div class="outline-text-4" id="text-org174774b">
<p>
<b>Series</b> è un oggetto che associa un array mono-dimensionale di valori a un array di indici.
</p>

<p>
Caratteristiche:
</p>
<ul class="org-ul">
<li>Valori omogenei (stesso tipo)</li>
<li>Indici personalizzabili</li>
<li>Gestione automatica dei valori mancanti (NaN)</li>
<li>Comportamento ibrido tra array NumPy e dizionario</li>
</ul>


<p>
I due array sono accessibili mediante i metodi <code>.array</code>, che ritorna un <code>PandasArray</code>, tipicamente un wrapper di un array di NumPy, e <code>.index</code>.
</p>

<p>
Quando si disegna il grafico di due serie, i valori vengono associati in base all'indice.
</p>
</div>
<div id="outline-container-org448df05" class="outline-5">
<h5 id="org448df05">Creazione</h5>
<div class="outline-text-5" id="text-org448df05">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> pandas <span style="color: #a020f0;">as</span> pd

<span style="color: #b22222;"># </span><span style="color: #b22222;">Con indici espliciti</span>
<span style="color: #a0522d;">first_appearance</span> = pd.Series([1963, 1962, 1941], 
                             index=[<span style="color: #8b2252;">'Iron Man'</span>, <span style="color: #8b2252;">'Hulk'</span>, <span style="color: #8b2252;">'Captain America'</span>])

<span style="color: #b22222;"># </span><span style="color: #b22222;">Da dizionario (le chiavi diventano indici)</span>
<span style="color: #a0522d;">heights</span> = pd.Series({<span style="color: #8b2252;">'Iron Man'</span>: 198.51, <span style="color: #8b2252;">'Hulk'</span>: 244.0, <span style="color: #8b2252;">'Thor'</span>: 198.0})
</pre>
</div>
</div>
</div>
<div id="outline-container-orgc9e01a9" class="outline-5">
<h5 id="orgc9e01a9">Accesso</h5>
<div class="outline-text-5" id="text-orgc9e01a9">
</div>
<div id="outline-container-org987eed6" class="outline-6">
<h6 id="org987eed6">Accedere ad un valore specifico</h6>
<div class="outline-text-6" id="text-org987eed6">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">Per indice</span>
first_appearance[<span style="color: #8b2252;">'Iron Man'</span>]        <span style="color: #b22222;"># </span><span style="color: #b22222;">1963</span>
first_appearance.loc[<span style="color: #8b2252;">'Iron Man'</span>]    <span style="color: #b22222;"># </span><span style="color: #b22222;">1963 (pi&#249; esplicito)</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Per posizione</span>
first_appearance.iloc[0]            <span style="color: #b22222;"># </span><span style="color: #b22222;">1963 (primo elemento)</span>
</pre>
</div>
</div>
</div>
<div id="outline-container-orgd396d2f" class="outline-6">
<h6 id="orgd396d2f">Accedere ad una sottocollezione</h6>
<div class="outline-text-6" id="text-orgd396d2f">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">Slicing per indice (inclusivo dell'ultimo)</span>
first_appearance.loc[<span style="color: #8b2252;">'Hulk'</span>:<span style="color: #8b2252;">'Thor'</span>]

<span style="color: #b22222;"># </span><span style="color: #b22222;">Slicing per posizione (esclusivo dell'ultimo)</span>
first_appearance.iloc[0:2]  <span style="color: #b22222;"># </span><span style="color: #b22222;">primi due elementi</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Con lista di booleani (list comprehension)</span>
first_appearance[first_appearance &gt; 1960]

<span style="color: #b22222;"># </span><span style="color: #b22222;">Con espressione</span>
first_appearance[[1960 &lt;= y &lt; 1970 <span style="color: #a020f0;">for</span> y <span style="color: #a020f0;">in</span> first_appearance]]
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-orga7a5000" class="outline-5">
<h5 id="orga7a5000">Proprietà</h5>
<div class="outline-text-5" id="text-orga7a5000">
<div class="org-src-container">
<pre class="src src-python">first_appearance.array  <span style="color: #b22222;"># </span><span style="color: #b22222;">Accede all'array dei valori (PandasArray)</span>
first_appearance.index  <span style="color: #b22222;"># </span><span style="color: #b22222;">Accede all'array degli indici</span>
</pre>
</div>
</div>
</div>
<div id="outline-container-orge324c00" class="outline-5">
<h5 id="orge324c00">Visualizzazione</h5>
<div class="outline-text-5" id="text-orge324c00">
<div class="org-src-container">
<pre class="src src-python">first_appearance.plot.bar()  <span style="color: #b22222;"># </span><span style="color: #b22222;">Crea un grafico a barre</span>
<span style="color: #b22222;"># </span><span style="color: #b22222;">I valori vengono accoppiati per indice, non per posizione</span>
</pre>
</div>
</div>
</div>
<div id="outline-container-orgaf56610" class="outline-5">
<h5 id="orgaf56610">Operazioni su dati</h5>
<div class="outline-text-5" id="text-orgaf56610">
</div>
<div id="outline-container-org240e4b9" class="outline-6">
<h6 id="org240e4b9">Raccogliere valori in bins</h6>
<div class="outline-text-6" id="text-org240e4b9">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">Esempio di categorizzazione delle altezze in bins</span>
<span style="color: #a0522d;">height_categories</span> = pd.cut(heroes_df[<span style="color: #8b2252;">'height'</span>], 
                          bins=[0, 170, 190, 250],
                          labels=[<span style="color: #8b2252;">'Basso'</span>, <span style="color: #8b2252;">'Medio'</span>, <span style="color: #8b2252;">'Alto'</span>],
                          right=<span style="color: #008b8b;">True</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo delle frequenze per categoria</span>
<span style="color: #a0522d;">frequency_table</span> = pd.crosstab(index=height_categories, columns=<span style="color: #8b2252;">'count'</span>)
</pre>
</div>
</div>
</div>
<div id="outline-container-org2e87c54" class="outline-6">
<h6 id="org2e87c54">Scremare e contare</h6>
<div class="outline-text-6" id="text-org2e87c54">
<p>
Per scremare <code>serie[serie &lt; 50])</code>.
<code>len()</code> lunghezza compreso nan, <code>.count()</code> senza nan.
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orga0968b7" class="outline-4">
<h4 id="orga0968b7">Dataframe</h4>
<div class="outline-text-4" id="text-orga0968b7">
<p>
Un dataframe è una <b>tabella rettangolare</b> contenente una <b>collezione di Series</b>, con uno specifico nome e con lo stesso indice.
</p>

<p>
Può essere quindi pensata come un <b>dizionario di serie eterogenee</b>.
</p>

<p>
Gestisce automaticamente i dati mancanti (come?)
</p>

<p>
Per <b>accedere ad una colonna</b> si utilizza la sintassi dei dizionari, con il nome della colonna, es. <code>col = df['nome colonna']</code>. A più colonne con <code>rescol = df[['A', 'C']]</code>.
</p>

<p>
Per <b>accedere ad una riga</b> si utilizza <code>loc[]</code> e <code>iloc[]</code>.
</p>

<p>
Per <b>accedere ad un elemento</b> si usa <code>at[index, column]</code>.
</p>
</div>
<div id="outline-container-org2b9ac2f" class="outline-5">
<h5 id="org2b9ac2f">Lettura da CSV</h5>
<div class="outline-text-5" id="text-org2b9ac2f">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">Lettura di base</span>
<span style="color: #a0522d;">df</span> = pd.read_csv(<span style="color: #8b2252;">'file.csv'</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Con opzioni personalizzate</span>
<span style="color: #a0522d;">heroes</span> = pd.read_csv(<span style="color: #8b2252;">'heroes.csv'</span>,
                      delimiter=<span style="color: #8b2252;">';'</span>,  <span style="color: #b22222;"># </span><span style="color: #b22222;">Field separator</span>
                      quotechar=<span style="color: #8b2252;">'"'</span>,  <span style="color: #b22222;"># </span><span style="color: #b22222;">Quote character</span>
                      na_values=[<span style="color: #8b2252;">'NA'</span>, <span style="color: #8b2252;">''</span>],  <span style="color: #b22222;"># </span><span style="color: #b22222;">Values to treat as NaN</span>
                      index_col=<span style="color: #8b2252;">'name'</span>,  <span style="color: #b22222;"># </span><span style="color: #b22222;">Column to use as index</span>

                      <span style="color: #b22222;"># </span><span style="color: #b22222;">--- Numerical Separators ---</span>
                      decimal=<span style="color: #8b2252;">'.'</span>,  <span style="color: #b22222;"># </span><span style="color: #b22222;">Decimal separator (default is '.')</span>
                      thousands=<span style="color: #8b2252;">','</span>, <span style="color: #b22222;"># </span><span style="color: #b22222;">Thousands separator (e.g., "1,000.00")</span>

                      <span style="color: #b22222;"># </span><span style="color: #b22222;">--- Column Data Types ---</span>
                       dtype={
                           <span style="color: #8b2252;">'colonna_A'</span>: <span style="color: #483d8b;">float</span>,  <span style="color: #b22222;"># </span><span style="color: #b22222;">Force 'colonna_A' to float</span>
                           <span style="color: #8b2252;">'colonna_B'</span>: <span style="color: #483d8b;">str</span>,    <span style="color: #b22222;"># </span><span style="color: #b22222;">Force 'colonna_B' to string</span>
                           <span style="color: #8b2252;">'colonna_interi_con_NA'</span>: pd.Int64Dtype(), <span style="color: #b22222;"># </span><span style="color: #b22222;">Nullable integer column</span>
                           <span style="color: #8b2252;">'colonna_mista_numerica'</span>: np.float64  <span style="color: #b22222;"># </span><span style="color: #b22222;">Mixed int/float column</span>
                      }
                     )
</pre>
</div>
</div>
</div>
<div id="outline-container-org3313761" class="outline-5">
<h5 id="org3313761">Creazione</h5>
<div class="outline-text-5" id="text-org3313761">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">Da dizionario di liste/array</span>
<span style="color: #a0522d;">heroes_df</span> = pd.DataFrame({
    <span style="color: #8b2252;">'name'</span>: [<span style="color: #8b2252;">'Iron Man'</span>, <span style="color: #8b2252;">'Hulk'</span>, <span style="color: #8b2252;">'Thor'</span>],
    <span style="color: #8b2252;">'height'</span>: [198.51, 244.0, 198.0],
    <span style="color: #8b2252;">'first_appearance'</span>: [1963, 1962, 1963]
})

<span style="color: #b22222;"># </span><span style="color: #b22222;">Da lista di dizionari</span>
<span style="color: #a0522d;">heroes_df</span> = pd.DataFrame([
    {<span style="color: #8b2252;">'name'</span>: <span style="color: #8b2252;">'Iron Man'</span>, <span style="color: #8b2252;">'height'</span>: 198.51, <span style="color: #8b2252;">'first_appearance'</span>: 1963},
    {<span style="color: #8b2252;">'name'</span>: <span style="color: #8b2252;">'Hulk'</span>, <span style="color: #8b2252;">'height'</span>: 244.0, <span style="color: #8b2252;">'first_appearance'</span>: 1962},
    {<span style="color: #8b2252;">'name'</span>: <span style="color: #8b2252;">'Thor'</span>, <span style="color: #8b2252;">'height'</span>: 198.0, <span style="color: #8b2252;">'first_appearance'</span>: 1963}
])
</pre>
</div>
</div>
</div>
<div id="outline-container-orgc0691ad" class="outline-5">
<h5 id="orgc0691ad">Accesso</h5>
<div class="outline-text-5" id="text-orgc0691ad">
</div>
<div id="outline-container-orgba4c936" class="outline-6">
<h6 id="orgba4c936">Colonne</h6>
<div class="outline-text-6" id="text-orgba4c936">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">Singola colonna (restituisce Series)</span>
<span style="color: #a0522d;">heights</span> = heroes_df[<span style="color: #8b2252;">'height'</span>]

<span style="color: #b22222;"># </span><span style="color: #b22222;">Multiple colonne (restituisce DataFrame)</span>
<span style="color: #a0522d;">subset</span> = heroes_df[[<span style="color: #8b2252;">'name'</span>, <span style="color: #8b2252;">'height'</span>]]
</pre>
</div>
</div>
</div>
<div id="outline-container-org10d56e5" class="outline-6">
<h6 id="org10d56e5">Righe</h6>
<div class="outline-text-6" id="text-org10d56e5">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">Per indice</span>
heroes_df.loc[0]  <span style="color: #b22222;"># </span><span style="color: #b22222;">prima riga</span>
heroes_df.loc[<span style="color: #8b2252;">'Iron Man'</span>]  <span style="color: #b22222;"># </span><span style="color: #b22222;">se l'indice &#232; 'Iron Man'</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Per posizione</span>
heroes_df.iloc[1]  <span style="color: #b22222;"># </span><span style="color: #b22222;">seconda riga</span>
heroes_df.iloc[1:3]  <span style="color: #b22222;"># </span><span style="color: #b22222;">seconda e terza riga</span>
</pre>
</div>
</div>
</div>
<div id="outline-container-orgc48681e" class="outline-6">
<h6 id="orgc48681e">Elementi specifici</h6>
<div class="outline-text-6" id="text-orgc48681e">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">Per indice</span>
heroes_df.loc[0]  <span style="color: #b22222;"># </span><span style="color: #b22222;">prima riga</span>
heroes_df.loc[<span style="color: #8b2252;">'Iron Man'</span>]  <span style="color: #b22222;"># </span><span style="color: #b22222;">se l'indice &#232; 'Iron Man'</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Per posizione</span>
heroes_df.iloc[1]  <span style="color: #b22222;"># </span><span style="color: #b22222;">seconda riga</span>
heroes_df.iloc[1:3]  <span style="color: #b22222;"># </span><span style="color: #b22222;">seconda e terza riga</span>
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-orgbb6609a" class="outline-5">
<h5 id="orgbb6609a">Filtri e selezione</h5>
<div class="outline-text-5" id="text-orgbb6609a">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">Filtro con condizioni</span>
<span style="color: #a0522d;">tall_heroes</span> = heroes_df[heroes_df[<span style="color: #8b2252;">'height'</span>] &gt; 190]

<span style="color: #b22222;"># </span><span style="color: #b22222;">Filtri multipli</span>
<span style="color: #a0522d;">strong_tall</span> = heroes_df[(heroes_df[<span style="color: #8b2252;">'height'</span>] &gt; 190) &amp; 
                         (heroes_df[<span style="color: #8b2252;">'strength'</span>] &gt; 50)]

<span style="color: #b22222;"># </span><span style="color: #b22222;">Metodi di selezione</span>
heroes_df.query(<span style="color: #8b2252;">'height &gt; 190 and strength &gt; 50'</span>)
</pre>
</div>
</div>
</div>
<div id="outline-container-org056ffe5" class="outline-5">
<h5 id="org056ffe5">Riordinamento</h5>
<div class="outline-text-5" id="text-org056ffe5">
</div>
<div id="outline-container-org38841f7" class="outline-6">
<h6 id="org38841f7">Riordina indici (righe)</h6>
<div class="outline-text-6" id="text-org38841f7">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">Riordinare gli indici di un DataFrame</span>
<span style="color: #a0522d;">df_reordered</span> = dataframe.reindex([<span style="color: #8b2252;">'indice3'</span>, <span style="color: #8b2252;">'indice1'</span>, <span style="color: #8b2252;">'indice2'</span>])
<span style="color: #b22222;"># </span><span style="color: #b22222;">Specificare valori per righe mancanti</span>
<span style="color: #a0522d;">df_reordered</span> = dataframe.reindex([<span style="color: #8b2252;">'indice3'</span>, <span style="color: #8b2252;">'indice1'</span>, <span style="color: #8b2252;">'nuovo'</span>], fill_value=0)
</pre>
</div>
</div>
</div>
<div id="outline-container-org488a726" class="outline-6">
<h6 id="org488a726">Riordina colonne</h6>
<div class="outline-text-6" id="text-org488a726">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">Modo pi&#249; diretto per riordinare le colonne</span>
<span style="color: #a0522d;">df_reordered</span> = dataframe[[<span style="color: #8b2252;">'colonna3'</span>, <span style="color: #8b2252;">'colonna1'</span>, <span style="color: #8b2252;">'colonna2'</span>]]

<span style="color: #b22222;"># </span><span style="color: #b22222;">Esempio concreto</span>
<span style="color: #a0522d;">columns_order</span> = [<span style="color: #8b2252;">'name'</span>, <span style="color: #8b2252;">'strength'</span>, <span style="color: #8b2252;">'first_appearance'</span>]
<span style="color: #a0522d;">heroes_reordered</span> = heroes_df[columns_order]

<span style="color: #b22222;"># </span><span style="color: #b22222;">Metodo che usa loc inutilmente</span>
dataframe.loc[:,listaDiColonneRiordinate]
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-org7f82827" class="outline-5">
<h5 id="org7f82827">Conta numero di casi e colonne</h5>
<div class="outline-text-5" id="text-org7f82827">
<p>
<code>df.shape</code>, <code>[0]</code> righe, <code>[1]</code> colonne
</p>
</div>
</div>
<div id="outline-container-orgc050509" class="outline-5">
<h5 id="orgc050509">Ritorna valori unici di un attributo</h5>
<div class="outline-text-5" id="text-orgc050509">
<p>
<code>df['NomeColonna'].unique()</code>
</p>
</div>
</div>
<div id="outline-container-orga5cbf69" class="outline-5">
<h5 id="orga5cbf69">Capisci esistenza valori nulli</h5>
<div class="outline-text-5" id="text-orga5cbf69">
<p>
<code>pd.isnull(acqua['Oro']).any()</code> che resituisce True se esistono dei valori nulli
<code>pd.isnull(acqua['Oro']).sum()</code> che restituisce una serie con 1 al posto dei null, e poi ne conta le occorrenze
</p>
</div>
</div>
<div id="outline-container-org92865b5" class="outline-5">
<h5 id="org92865b5">Trova tipi dato colonne</h5>
<div class="outline-text-5" id="text-org92865b5">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">Funzione per mostrare i tipi di dato nel DataFrame</span>
<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">show_dtypes</span>(df):
    <span style="color: #a020f0;">for</span> col, dtype <span style="color: #a020f0;">in</span> df.dtypes.items():
        <span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"</span>{col}<span style="color: #8b2252;">: </span>{dtype}<span style="color: #8b2252;">"</span>)
    <span style="color: #483d8b;">print</span>(<span style="color: #8b2252;">"-"</span> * 30)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Mostriamo tutti i tipi di dato nel DataFrame</span>
<span style="color: #483d8b;">print</span>(<span style="color: #8b2252;">"Tipi di dato nel DataFrame:"</span>)
show_dtypes(df)
</pre>
</div>
</div>
</div>
<div id="outline-container-org4138ab1" class="outline-5">
<h5 id="org4138ab1">Filtra colonne per tipo</h5>
<div class="outline-text-5" id="text-org4138ab1">
<div class="org-src-container">
<pre class="src src-python">
<span style="color: #a0522d;">numeric_df</span> = df.select_dtypes(include=[<span style="color: #8b2252;">'number'</span>])
<span style="color: #483d8b;">print</span>(numeric_df.columns.tolist())
<span style="color: #b22222;">#</span><span style="color: #b22222;">interi -&gt; integer, float -&gt; float, stringhe -&gt; object, booleani -&gt; bool, date -&gt; datetime</span>
<span style="color: #b22222;"># </span><span style="color: #b22222;">posso usare anche dati di np come np.float64</span>
<span style="color: #b22222;"># </span><span style="color: #b22222;">altro argomento, exclude</span>
</pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="outline-container-orge68f6db" class="outline-2">
<h2 id="orge68f6db">Tecniche python</h2>
<div class="outline-text-2" id="text-orge68f6db">
</div>
<div id="outline-container-orgcd62758" class="outline-3">
<h3 id="orgcd62758">Calcola funzione di ripartizione usando massa</h3>
<div class="outline-text-3" id="text-orgcd62758">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #483d8b;">sum</span>([X.pmf(i) <span style="color: #a020f0;">for</span> i <span style="color: #a020f0;">in</span> <span style="color: #483d8b;">range</span>(7)])
</pre>
</div>
</div>
</div>
<div id="outline-container-orga16c0a3" class="outline-3">
<h3 id="orga16c0a3">Inversa della funzione di ripartizione</h3>
<div class="outline-text-3" id="text-orga16c0a3">
<p>
<code>X.ppf(0.8)</code>
Per rispondere alla domanda: si determini il più piccolo dei valori per cui la funzione di ripartizione vale x.
Ovvero dato il numero r, trovare quel valore della variabile aleatoria tale per cui lo 0.x % dei valori ne sono più piccoli. Proprio il concetto di <b>quantile</b>
</p>
</div>
</div>
<div id="outline-container-org429311b" class="outline-3">
<h3 id="org429311b">(AGGIUNGERE CODICE) Stabilire se una popolazione può essere descritta da una variabile aleatoria</h3>
<div class="outline-text-3" id="text-org429311b">
<p>
Tracciare un grafico con l'asse delle ascisse recante i quantili del campione e quello delle ordinate quelli teorici della specifica distribuzione ipotetica
</p>

<p>
Con la normale viene automaticamente normalizzato
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> statsmodels.api <span style="color: #a020f0;">as</span> sm
<span style="color: #a0522d;">fig</span> = plt.figure(figsize=(8, 8))
<span style="color: #a0522d;">ax</span> = fig.add_subplot(111)

sm.qqplot(campione, line=<span style="color: #8b2252;">'s'</span>, ax=ax)

ax.set_title(<span style="color: #8b2252;">'QQ-Plot contro la Distribuzione Normale'</span>)
ax.grid(<span style="color: #008b8b;">True</span>, linestyle=<span style="color: #8b2252;">'--'</span>, alpha=0.7)

plt.tight_layout()
plt.show()
</pre>
</div>

<p>
altrimenti no
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
<span style="color: #a020f0;">import</span> matplotlib.pyplot <span style="color: #a020f0;">as</span> plt
<span style="color: #a020f0;">import</span> statsmodels.api <span style="color: #a020f0;">as</span> sm
<span style="color: #a020f0;">import</span> scipy.stats <span style="color: #a020f0;">as</span> st

<span style="color: #b22222;"># </span><span style="color: #b22222;">Parametri della distribuzione ipergeometrica</span>
<span style="color: #a0522d;">M_tot</span> = 50  <span style="color: #b22222;"># </span><span style="color: #b22222;">Dimensione della popolazione</span>
<span style="color: #a0522d;">M_fun</span> = 20  <span style="color: #b22222;"># </span><span style="color: #b22222;">Numero di successi nella popolazione</span>
<span style="color: #a0522d;">n_estr</span> = 10  <span style="color: #b22222;"># </span><span style="color: #b22222;">Numero di estrazioni</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Creazione del QQ-plot utilizzando la distribuzione corretta</span>
<span style="color: #a0522d;">fig</span>, <span style="color: #a0522d;">ax</span> = plt.subplots(figsize=(8, 8))

<span style="color: #b22222;"># </span><span style="color: #b22222;">Creiamo la distribuzione ipergeometrica congelata con i parametri specifici</span>
<span style="color: #a0522d;">hypergeom_dist</span> = st.hypergeom(M=M_tot, n=M_fun, N=n_estr)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Passiamo l'oggetto distribuzione invece della funzione</span>
sm.qqplot(serie, dist=hypergeom_dist, fit=<span style="color: #008b8b;">False</span>, line=<span style="color: #8b2252;">'45'</span>, ax=ax)

ax.set_title(<span style="color: #8b2252;">'QQ-Plot: Campione vs Distribuzione Ipergeometrica'</span>)
ax.set_xlabel(<span style="color: #8b2252;">'Quantili Teorici (Ipergeometrica)'</span>)
ax.set_ylabel(<span style="color: #8b2252;">'Quantili del Campione (lat)'</span>)
ax.grid(<span style="color: #008b8b;">True</span>, linestyle=<span style="color: #8b2252;">'--'</span>, alpha=0.7)

plt.tight_layout()
plt.show()
</pre>
</div>
</div>
</div>
<div id="outline-container-org487ad7a" class="outline-3">
<h3 id="org487ad7a">Mostrare media non robusta in distribuzione skewed</h3>
<div class="outline-text-3" id="text-org487ad7a">
<p>
Calcolo l'ecdf e plotto il diagramma di pareto. Poi ci stampo la media con una linea verticale. 
</p>

<p>
Altro grafico: stampo un grafico poligonale con x=numero dell'osservazione (ordinate) e y intensità. Recupero l'indice del primo elemento maggiore della media calcolando ecdf(media) * len(serie). Mostro che l'elemento in quella posizione (con iloc, altrimenti non funziona) è maggiore della media e il precedente no. Stampo la linea verticale corrispondente alla media sul grafico
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo della media e identificazione del primo elemento maggiore della media</span>
<span style="color: #a0522d;">mean</span> = <span style="color: #483d8b;">sum</span>(serie.dropna()) / <span style="color: #483d8b;">len</span>(serie.dropna())  <span style="color: #b22222;"># </span><span style="color: #b22222;">Alternativa: serie.dropna().mean()</span>
<span style="color: #a0522d;">y</span> = serie.dropna().sort_values()  <span style="color: #b22222;"># </span><span style="color: #b22222;">Serie ordinata</span>
<span style="color: #a0522d;">ecdf_media</span> = ecdf(mean)  <span style="color: #b22222;"># </span><span style="color: #b22222;">Valore della ECDF in corrispondenza della media</span>
<span style="color: #a0522d;">pos_ultimo_minore</span> = <span style="color: #483d8b;">int</span>(ecdf_media * <span style="color: #483d8b;">len</span>(y)) - 1  <span style="color: #b22222;"># </span><span style="color: #b22222;">Posizione dell'ultimo elemento &#8804; media</span>
<span style="color: #a0522d;">pos_primo_maggiore</span> = pos_ultimo_minore + 1  <span style="color: #b22222;"># </span><span style="color: #b22222;">Posizione del primo elemento &gt; media</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Verifica</span>
<span style="color: #a0522d;">ultimo_minore</span> = y.iloc[pos_ultimo_minore]
<span style="color: #a0522d;">primo_maggiore</span> = y.iloc[pos_primo_maggiore]
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Media: </span>{mean:.2f}<span style="color: #8b2252;">"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Ultimo elemento &#8804; media: </span>{ultimo_minore:.2f}<span style="color: #8b2252;"> (posizione </span>{pos_ultimo_minore}<span style="color: #8b2252;">)"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Primo elemento &gt; media: </span>{primo_maggiore:.2f}<span style="color: #8b2252;"> (posizione </span>{pos_primo_maggiore}<span style="color: #8b2252;">)"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Creazione del grafico</span>
<span style="color: #a0522d;">fig</span>, <span style="color: #a0522d;">ax</span> = plt.subplots(figsize=(10, 6))

<span style="color: #b22222;"># </span><span style="color: #b22222;">Grafico poligonale dei valori ordinati</span>
ax.plot(np.arange(0, <span style="color: #483d8b;">len</span>(y)), y, <span style="color: #8b2252;">'b-'</span>, linewidth=2, markersize=6)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Linea orizzontale per la media</span>
ax.axhline(y=mean, color=<span style="color: #8b2252;">'r'</span>, linestyle=<span style="color: #8b2252;">'--'</span>, label=f<span style="color: #8b2252;">'Media (&#956;) = </span>{mean:.2f}<span style="color: #8b2252;">'</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Evidenziazione del punto di transizione</span>
ax.plot(pos_primo_maggiore, primo_maggiore, <span style="color: #8b2252;">'ro'</span>, markersize=8)  <span style="color: #b22222;"># </span><span style="color: #b22222;">Punto rosso</span>
ax.annotate(<span style="color: #8b2252;">'Primo elemento &gt; media'</span>, 
            xy=(pos_primo_maggiore, primo_maggiore),
            xytext=(pos_primo_maggiore + <span style="color: #483d8b;">len</span>(y)*0.05, primo_maggiore),
            arrowprops=<span style="color: #483d8b;">dict</span>(facecolor=<span style="color: #8b2252;">'black'</span>, shrink=0.05, width=1.5))

<span style="color: #b22222;"># </span><span style="color: #b22222;">Aggiunta di etichette e griglia</span>
ax.set_xlabel(<span style="color: #8b2252;">'Posizione nell</span><span style="color: #008b8b;">\'</span><span style="color: #8b2252;">ordinamento'</span>)
ax.set_ylabel(<span style="color: #8b2252;">'Valore'</span>)
ax.set_title(<span style="color: #8b2252;">'Distribuzione Ordinata con Identificazione del Primo Elemento &gt; Media'</span>)
ax.legend()
ax.grid(<span style="color: #008b8b;">True</span>, linestyle=<span style="color: #8b2252;">'--'</span>, alpha=0.7)

plt.tight_layout()
plt.show()
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-org6927619" class="outline-2">
<h2 id="org6927619">Grafici Statistici in Python: Guida Rapida (Approccio OOP)</h2>
<div class="outline-text-2" id="text-org6927619">
</div>
<div id="outline-container-org4cf7804" class="outline-3">
<h3 id="org4cf7804">Blueprint</h3>
<div class="outline-text-3" id="text-org4cf7804">
</div>
<div id="outline-container-org959cd8d" class="outline-4">
<h4 id="org959cd8d">Un solo grafico</h4>
<div class="outline-text-4" id="text-org959cd8d">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
<span style="color: #a020f0;">import</span> matplotlib.pyplot <span style="color: #a020f0;">as</span> plt

<span style="color: #b22222;"># </span><span style="color: #b22222;">Generazione dei dati</span>
<span style="color: #a0522d;">x</span> = np.arange(1, 11)
<span style="color: #a0522d;">y</span> = x ** 2

<span style="color: #b22222;"># </span><span style="color: #b22222;">Approccio OOP con fig e ax</span>
<span style="color: #a0522d;">fig</span>, <span style="color: #a0522d;">ax</span> = plt.subplots(figsize=(8, 6))
ax.vlines(x, ymin=0, ymax=y, color=<span style="color: #8b2252;">'blue'</span>)
ax.plot(x, y, <span style="color: #8b2252;">'o'</span>, color=<span style="color: #8b2252;">'red'</span>)  <span style="color: #b22222;"># </span><span style="color: #b22222;">Punti all'estremit&#224;</span>
ax.set_xlabel(<span style="color: #8b2252;">'x'</span>)
ax.set_ylabel(<span style="color: #8b2252;">'y = x^2'</span>)
ax.set_title(<span style="color: #8b2252;">'Grafico a bastoncini per y = x^2'</span>)
ax.grid(<span style="color: #008b8b;">True</span>)
plt.show()
</pre>
</div>
</div>
</div>
<div id="outline-container-org7609554" class="outline-4">
<h4 id="org7609554">Due grafici</h4>
<div class="outline-text-4" id="text-org7609554">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
<span style="color: #a020f0;">import</span> matplotlib.pyplot <span style="color: #a020f0;">as</span> plt

<span style="color: #b22222;"># </span><span style="color: #b22222;">Dati di base</span>
<span style="color: #a0522d;">x</span> = np.arange(1, 11)
<span style="color: #a0522d;">y1</span> = x ** 2
<span style="color: #a0522d;">y2</span> = x ** 3

<span style="color: #b22222;"># </span><span style="color: #b22222;">Creazione dei subplot affiancati</span>
<span style="color: #a0522d;">fig</span>, <span style="color: #a0522d;">axes</span> = plt.subplots(1, 2, figsize=(12, 5))

<span style="color: #b22222;"># </span><span style="color: #b22222;">Primo plot: quadrato</span>
axes[0].vlines(x, ymin=0, ymax=y1, color=<span style="color: #8b2252;">'blue'</span>)
axes[0].plot(x, y1, <span style="color: #8b2252;">'o'</span>, color=<span style="color: #8b2252;">'blue'</span>)
axes[0].set_title(<span style="color: #8b2252;">'y = x^2'</span>)
axes[0].set_xlabel(<span style="color: #8b2252;">'x'</span>)
axes[0].set_ylabel(<span style="color: #8b2252;">'y'</span>)
axes[0].grid(<span style="color: #008b8b;">True</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Secondo plot: cubo</span>
axes[1].vlines(x, ymin=0, ymax=y2, color=<span style="color: #8b2252;">'orange'</span>)
axes[1].plot(x, y2, <span style="color: #8b2252;">'s'</span>, color=<span style="color: #8b2252;">'orange'</span>)
axes[1].set_title(<span style="color: #8b2252;">'y = x^3'</span>)
axes[1].set_xlabel(<span style="color: #8b2252;">'x'</span>)
axes[1].set_ylabel(<span style="color: #8b2252;">'y'</span>)
axes[1].grid(<span style="color: #008b8b;">True</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Ottimizza la disposizione</span>
plt.tight_layout()
plt.show()
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-orgcff9f4e" class="outline-3">
<h3 id="orgcff9f4e">A bastoncini (Stem plots)</h3>
<div class="outline-text-3" id="text-orgcff9f4e">
<ul class="org-ul">
<li><b>Uso</b>: Variabili numeriche discrete, serie temporali con pochi punti, enfasi su valori puntuali</li>
<li><b>Dati</b>: Numerici discreti, conteggi di frequenza, serie temporali a campionamento regolare</li>
<li><b>Sintassi essenziale OOP</b>:</li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
<span style="color: #a020f0;">import</span> matplotlib.pyplot <span style="color: #a020f0;">as</span> plt

<span style="color: #b22222;"># </span><span style="color: #b22222;">Dati</span>
<span style="color: #a0522d;">x</span> = np.arange(1, 11)
<span style="color: #a0522d;">y</span> = np.random.randint(1, 30, size=10)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Approccio OOP</span>
<span style="color: #a0522d;">fig</span>, <span style="color: #a0522d;">ax</span> = plt.subplots(figsize=(8, 6))
ax.vlines(x, 0, y, colors=<span style="color: #8b2252;">'blue'</span>, linewidth=2)
ax.plot(x, y, <span style="color: #8b2252;">'o'</span>, color=<span style="color: #8b2252;">'red'</span>, markersize=8)
ax.set_xlabel(<span style="color: #8b2252;">'Valore x'</span>)
ax.set_ylabel(<span style="color: #8b2252;">'Frequenza'</span>)
ax.set_title(<span style="color: #8b2252;">'Grafico a bastoncini'</span>)
ax.grid(<span style="color: #008b8b;">True</span>, linestyle=<span style="color: #8b2252;">'--'</span>, alpha=0.7)
plt.show()
</pre>
</div>
</div>
</div>
<div id="outline-container-org51a8b7d" class="outline-3">
<h3 id="org51a8b7d">A barre (Bar charts)</h3>
<div class="outline-text-3" id="text-org51a8b7d">
<ul class="org-ul">
<li><b>Uso</b>: Confronto tra categorie, distribuzioni di frequenza di variabili categoriche, confronti tra gruppi</li>
<li><b>Dati</b>: Categorici, discreti con poche modalità, aggregazioni per categoria</li>
<li><b>Sintassi essenziale OOP</b>:</li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
<span style="color: #a020f0;">import</span> matplotlib.pyplot <span style="color: #a020f0;">as</span> plt

<span style="color: #b22222;"># </span><span style="color: #b22222;">Dati</span>
<span style="color: #a0522d;">categories</span> = [<span style="color: #8b2252;">'A'</span>, <span style="color: #8b2252;">'B'</span>, <span style="color: #8b2252;">'C'</span>, <span style="color: #8b2252;">'D'</span>, <span style="color: #8b2252;">'E'</span>]
<span style="color: #a0522d;">values</span> = [23, 45, 56, 78, 42]

<span style="color: #b22222;"># </span><span style="color: #b22222;">Approccio OOP</span>
<span style="color: #a0522d;">fig</span>, <span style="color: #a0522d;">ax</span> = plt.subplots(figsize=(8, 6))
<span style="color: #a0522d;">bars</span> = ax.bar(categories, values, color=<span style="color: #8b2252;">'skyblue'</span>, edgecolor=<span style="color: #8b2252;">'black'</span>)
ax.set_xlabel(<span style="color: #8b2252;">'Categoria'</span>)
ax.set_ylabel(<span style="color: #8b2252;">'Valore'</span>)
ax.set_title(<span style="color: #8b2252;">'Grafico a barre'</span>)
ax.grid(axis=<span style="color: #8b2252;">'y'</span>, linestyle=<span style="color: #8b2252;">'--'</span>, alpha=0.7)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Barre raggruppate (esempio)</span>
<span style="color: #a0522d;">width</span> = 0.35
<span style="color: #a0522d;">x</span> = np.arange(<span style="color: #483d8b;">len</span>(categories))
<span style="color: #a0522d;">fig</span>, <span style="color: #a0522d;">ax</span> = plt.subplots(figsize=(10, 6))
ax.bar(x - width/2, [23, 45, 56, 78, 42], width, label=<span style="color: #8b2252;">'Gruppo A'</span>)
ax.bar(x + width/2, [31, 39, 63, 52, 38], width, label=<span style="color: #8b2252;">'Gruppo B'</span>)
ax.set_xticks(x)
ax.set_xticklabels(categories)
ax.legend()
</pre>
</div>
</div>
</div>
<div id="outline-container-org1a80a59" class="outline-3">
<h3 id="org1a80a59">Poligonale (Line plots)</h3>
<div class="outline-text-3" id="text-org1a80a59">
<ul class="org-ul">
<li><b>Uso</b>: Serie temporali, tendenze, comparazioni di serie nel tempo, dati ordinati</li>
<li><b>Dati</b>: Serie temporali, dati ordinati, misurazioni sequenziali</li>
<li><b>Sintassi essenziale OOP</b>:</li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
<span style="color: #a020f0;">import</span> matplotlib.pyplot <span style="color: #a020f0;">as</span> plt

<span style="color: #b22222;"># </span><span style="color: #b22222;">Dati</span>
<span style="color: #a0522d;">x</span> = np.arange(0, 10, 0.5)
<span style="color: #a0522d;">y1</span> = np.sin(x)
<span style="color: #a0522d;">y2</span> = np.cos(x)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Approccio OOP</span>
<span style="color: #a0522d;">fig</span>, <span style="color: #a0522d;">ax</span> = plt.subplots(figsize=(10, 6))
ax.plot(x, y1, <span style="color: #8b2252;">'b-o'</span>, linewidth=2, markersize=6, label=<span style="color: #8b2252;">'sin(x)'</span>)
ax.plot(x, y2, <span style="color: #8b2252;">'r-s'</span>, linewidth=2, markersize=6, label=<span style="color: #8b2252;">'cos(x)'</span>)
ax.set_xlabel(<span style="color: #8b2252;">'x'</span>)
ax.set_ylabel(<span style="color: #8b2252;">'y'</span>)
ax.set_title(<span style="color: #8b2252;">'Grafico poligonale'</span>)
ax.legend()
ax.grid(<span style="color: #008b8b;">True</span>, linestyle=<span style="color: #8b2252;">'--'</span>, alpha=0.7)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Step plot per dati discreti</span>
<span style="color: #a0522d;">fig</span>, <span style="color: #a0522d;">ax</span> = plt.subplots(figsize=(10, 6))
ax.step(x, y1, <span style="color: #8b2252;">'g-'</span>, where=<span style="color: #8b2252;">'mid'</span>, linewidth=2, label=<span style="color: #8b2252;">'Step mid'</span>)
ax.set_title(<span style="color: #8b2252;">'Grafico a gradini'</span>)
ax.legend()
</pre>
</div>
</div>
</div>
<div id="outline-container-org2759f15" class="outline-3">
<h3 id="org2759f15">A torta (Pie charts)</h3>
<div class="outline-text-3" id="text-org2759f15">
<ul class="org-ul">
<li><b>Uso</b>: Proporzioni relative, poche categorie (≤7), parti di un intero</li>
<li><b>Dati</b>: Categorici, proporzioni/percentuali</li>
<li><b>Sintassi essenziale OOP</b>:</li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> matplotlib.pyplot <span style="color: #a020f0;">as</span> plt

<span style="color: #b22222;"># </span><span style="color: #b22222;">Dati</span>
<span style="color: #a0522d;">labels</span> = [<span style="color: #8b2252;">'A'</span>, <span style="color: #8b2252;">'B'</span>, <span style="color: #8b2252;">'C'</span>, <span style="color: #8b2252;">'D'</span>]
<span style="color: #a0522d;">values</span> = [35, 25, 20, 20]

<span style="color: #b22222;"># </span><span style="color: #b22222;">Approccio OOP</span>
<span style="color: #a0522d;">fig</span>, <span style="color: #a0522d;">ax</span> = plt.subplots(figsize=(8, 8))
ax.pie(values, labels=labels, autopct=<span style="color: #8b2252;">'%1.1f%%'</span>, 
       startangle=90, shadow=<span style="color: #008b8b;">True</span>,
       colors=[<span style="color: #8b2252;">'#ff9999'</span>,<span style="color: #8b2252;">'#66b3ff'</span>,<span style="color: #8b2252;">'#99ff99'</span>,<span style="color: #8b2252;">'#ffcc99'</span>])
ax.axis(<span style="color: #8b2252;">'equal'</span>)  <span style="color: #b22222;"># </span><span style="color: #b22222;">Per mantenere la forma circolare</span>
ax.set_title(<span style="color: #8b2252;">'Grafico a torta'</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Grafico a ciambella</span>
<span style="color: #a0522d;">fig</span>, <span style="color: #a0522d;">ax</span> = plt.subplots(figsize=(8, 8))
ax.pie(values, labels=labels, autopct=<span style="color: #8b2252;">'%1.1f%%'</span>, 
      wedgeprops=<span style="color: #483d8b;">dict</span>(width=0.5, edgecolor=<span style="color: #8b2252;">'w'</span>))
ax.set_title(<span style="color: #8b2252;">'Grafico a ciambella'</span>)
</pre>
</div>
</div>
</div>
<div id="outline-container-org58062db" class="outline-3">
<h3 id="org58062db">Istogramma (Histogram)</h3>
<div class="outline-text-3" id="text-org58062db">
<ul class="org-ul">
<li><b>Uso</b>: Distribuzione variabili continue, forma distribuzione, identificare outlier</li>
<li><b>Dati</b>: Numerici continui, dataset numerici grandi</li>
<li><b>Sintassi essenziale OOP</b>:</li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
<span style="color: #a020f0;">import</span> matplotlib.pyplot <span style="color: #a020f0;">as</span> plt

<span style="color: #b22222;"># </span><span style="color: #b22222;">Dati</span>
<span style="color: #a0522d;">data</span> = np.random.normal(0, 1, 1000)  <span style="color: #b22222;"># </span><span style="color: #b22222;">Distribuzione normale</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Approccio OOP</span>
<span style="color: #a0522d;">fig</span>, <span style="color: #a0522d;">ax</span> = plt.subplots(figsize=(10, 6))
ax.hist(data, bins=30, color=<span style="color: #8b2252;">'skyblue'</span>, edgecolor=<span style="color: #8b2252;">'black'</span>, alpha=0.7)
ax.set_title(<span style="color: #8b2252;">'Istogramma di una distribuzione normale'</span>)
ax.set_xlabel(<span style="color: #8b2252;">'Valore'</span>)
ax.set_ylabel(<span style="color: #8b2252;">'Frequenza'</span>)
ax.grid(axis=<span style="color: #8b2252;">'y'</span>, alpha=0.75, linestyle=<span style="color: #8b2252;">'--'</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Bin personalizzati</span>
<span style="color: #a0522d;">bin_edges</span> = np.hstack((np.arange(0, 5, 0.5), 
                      np.arange(5, 10, 1), 
                      np.arange(10, 20, 2)))
<span style="color: #a0522d;">fig</span>, <span style="color: #a0522d;">ax</span> = plt.subplots(figsize=(10, 6))
ax.hist(np.random.exponential(2, 1000), bins=bin_edges, 
       color=<span style="color: #8b2252;">'purple'</span>, alpha=0.7, edgecolor=<span style="color: #8b2252;">'black'</span>)
ax.set_title(<span style="color: #8b2252;">'Istogramma con bin personalizzati'</span>)
</pre>
</div>
</div>
</div>
<div id="outline-container-orgfe65122" class="outline-3">
<h3 id="orgfe65122">Grafici multipli (Multiple plots)</h3>
<div class="outline-text-3" id="text-orgfe65122">
<ul class="org-ul">
<li><b>Uso</b>: Confronti multipli, relazioni tra variabili, visualizzazione compatta</li>
<li><b>Dati</b>: Qualsiasi tipo di dati per confronto</li>
<li><b>Sintassi essenziale OOP</b>:</li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
<span style="color: #a020f0;">import</span> matplotlib.pyplot <span style="color: #a020f0;">as</span> plt
<span style="color: #a020f0;">import</span> matplotlib.gridspec <span style="color: #a020f0;">as</span> gridspec

<span style="color: #b22222;"># </span><span style="color: #b22222;">Griglia regolare</span>
<span style="color: #a0522d;">fig</span>, <span style="color: #a0522d;">axs</span> = plt.subplots(2, 2, figsize=(12, 10))

<span style="color: #b22222;"># </span><span style="color: #b22222;">Grafico 1: Linea</span>
<span style="color: #a0522d;">x</span> = np.linspace(0, 10, 100)
axs[0, 0].plot(x, np.sin(x), <span style="color: #8b2252;">'b-'</span>)
axs[0, 0].set_title(<span style="color: #8b2252;">'Grafico a linea: sin(x)'</span>)
axs[0, 0].grid(<span style="color: #008b8b;">True</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Grafico 2: Scatter</span>
axs[0, 1].scatter(np.random.rand(50), np.random.rand(50), 
                 c=np.random.rand(50), cmap=<span style="color: #8b2252;">'viridis'</span>)
axs[0, 1].set_title(<span style="color: #8b2252;">'Grafico a dispersione'</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Grafico 3: Barre</span>
<span style="color: #a0522d;">categories</span> = [<span style="color: #8b2252;">'A'</span>, <span style="color: #8b2252;">'B'</span>, <span style="color: #8b2252;">'C'</span>, <span style="color: #8b2252;">'D'</span>]
<span style="color: #a0522d;">values</span> = [3, 7, 2, 5]
axs[1, 0].bar(categories, values, color=<span style="color: #8b2252;">'green'</span>)
axs[1, 0].set_title(<span style="color: #8b2252;">'Grafico a barre'</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Grafico 4: Istogramma</span>
<span style="color: #a0522d;">data</span> = np.random.normal(0, 1, 1000)
axs[1, 1].hist(data, bins=30, color=<span style="color: #8b2252;">'purple'</span>, alpha=0.7)
axs[1, 1].set_title(<span style="color: #8b2252;">'Istogramma'</span>)

plt.tight_layout()

<span style="color: #b22222;"># </span><span style="color: #b22222;">GridSpec per layout personalizzati</span>
<span style="color: #a0522d;">fig</span> = plt.figure(figsize=(12, 10))
<span style="color: #a0522d;">gs</span> = gridspec.GridSpec(2, 2, width_ratios=[2, 1], height_ratios=[1, 2])

<span style="color: #b22222;"># </span><span style="color: #b22222;">Grafico principale (pi&#249; grande)</span>
<span style="color: #a0522d;">ax1</span> = fig.add_subplot(gs[0, 0])
ax1.plot(np.random.randn(100).cumsum(), <span style="color: #8b2252;">'b-'</span>)
ax1.set_title(<span style="color: #8b2252;">'Serie temporale'</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Grafico in alto a destra</span>
<span style="color: #a0522d;">ax2</span> = fig.add_subplot(gs[0, 1])
ax2.pie([35, 65], labels=[<span style="color: #8b2252;">'A'</span>, <span style="color: #8b2252;">'B'</span>], autopct=<span style="color: #8b2252;">'%1.1f%%'</span>)
ax2.set_title(<span style="color: #8b2252;">'Proporzioni'</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Grafico grande in basso</span>
<span style="color: #a0522d;">ax3</span> = fig.add_subplot(gs[1, :])
<span style="color: #a0522d;">x</span> = np.random.randn(1000)
<span style="color: #a0522d;">y</span> = x * 2 + np.random.randn(1000) * 0.5
ax3.scatter(x, y, alpha=0.5)
ax3.set_title(<span style="color: #8b2252;">'Dispersione con correlazione'</span>)

plt.tight_layout()
</pre>
</div>
</div>
</div>
<div id="outline-container-orgfb76f92" class="outline-3">
<h3 id="orgfb76f92">Scatter plot (Diagramma di dispersione)</h3>
<div class="outline-text-3" id="text-orgfb76f92">
<ul class="org-ul">
<li><b>Uso</b>: Relazione tra due variabili numeriche, correlazioni, pattern/cluster</li>
<li><b>Dati</b>: Coppie di variabili numeriche, dati correlati</li>
<li><b>Sintassi essenziale OOP</b>:</li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
<span style="color: #a020f0;">import</span> matplotlib.pyplot <span style="color: #a020f0;">as</span> plt

<span style="color: #b22222;"># </span><span style="color: #b22222;">Dati base</span>
<span style="color: #a0522d;">x</span> = np.random.rand(50)
<span style="color: #a0522d;">y</span> = 3 * x + np.random.randn(50) * 0.2

<span style="color: #b22222;"># </span><span style="color: #b22222;">Approccio OOP base</span>
<span style="color: #a0522d;">fig</span>, <span style="color: #a0522d;">ax</span> = plt.subplots(figsize=(10, 6))
ax.scatter(x, y, color=<span style="color: #8b2252;">'blue'</span>, alpha=0.7)
ax.set_title(<span style="color: #8b2252;">'Scatter plot con correlazione positiva'</span>)
ax.set_xlabel(<span style="color: #8b2252;">'Variabile X'</span>)
ax.set_ylabel(<span style="color: #8b2252;">'Variabile Y'</span>)
ax.grid(<span style="color: #008b8b;">True</span>, linestyle=<span style="color: #8b2252;">'--'</span>, alpha=0.7)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Con dimensione e colore variabili</span>
<span style="color: #a0522d;">n</span> = 50
<span style="color: #a0522d;">x</span> = np.random.rand(n)
<span style="color: #a0522d;">y</span> = np.random.rand(n)
<span style="color: #a0522d;">colors</span> = np.random.rand(n)
<span style="color: #a0522d;">sizes</span> = 1000 * np.random.rand(n)

<span style="color: #a0522d;">fig</span>, <span style="color: #a0522d;">ax</span> = plt.subplots(figsize=(10, 8))
<span style="color: #a0522d;">scatter</span> = ax.scatter(x, y, c=colors, s=sizes, alpha=0.5, cmap=<span style="color: #8b2252;">'viridis'</span>)
fig.colorbar(scatter, ax=ax, label=<span style="color: #8b2252;">'Valore colore'</span>)
ax.set_title(<span style="color: #8b2252;">'Scatter plot con dimensione e colore variabili'</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Per gruppi</span>
<span style="color: #a0522d;">groups</span> = {<span style="color: #8b2252;">'A'</span>: (np.random.normal(0, 1, 30), np.random.normal(0, 1, 30)),
          <span style="color: #8b2252;">'B'</span>: (np.random.normal(2, 1, 30), np.random.normal(3, 1, 30))}

<span style="color: #a0522d;">fig</span>, <span style="color: #a0522d;">ax</span> = plt.subplots(figsize=(10, 8))
<span style="color: #a020f0;">for</span> name, (x, y) <span style="color: #a020f0;">in</span> groups.items():
    ax.scatter(x, y, label=name, alpha=0.7, s=80)
ax.legend()
ax.set_title(<span style="color: #8b2252;">'Scatter plot raggruppato'</span>)
</pre>
</div>
</div>
</div>
<div id="outline-container-orgdec16b2" class="outline-3">
<h3 id="orgdec16b2">Tabella comparativa per scelta grafico</h3>
<div class="outline-text-3" id="text-orgdec16b2">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Tipo di dati</th>
<th scope="col" class="org-left">Obiettivo</th>
<th scope="col" class="org-left">Grafico consigliato</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Categorici (&lt;20 cat.)</td>
<td class="org-left">Confrontare valori</td>
<td class="org-left">Barre</td>
</tr>

<tr>
<td class="org-left">Categorici (≤7 cat.)</td>
<td class="org-left">Mostrare proporzioni</td>
<td class="org-left">Torta</td>
</tr>

<tr>
<td class="org-left">Numerici discreti</td>
<td class="org-left">Valori puntuali</td>
<td class="org-left">Bastoncini</td>
</tr>

<tr>
<td class="org-left">Numerici continui</td>
<td class="org-left">Distribuzione</td>
<td class="org-left">Istogramma</td>
</tr>

<tr>
<td class="org-left">Serie temporali</td>
<td class="org-left">Tendenze nel tempo</td>
<td class="org-left">Poligonale</td>
</tr>

<tr>
<td class="org-left">Due variabili numeriche</td>
<td class="org-left">Correlazione</td>
<td class="org-left">Scatter</td>
</tr>

<tr>
<td class="org-left">Più variabili</td>
<td class="org-left">Confronti multipli</td>
<td class="org-left">Grafici multipli</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="outline-container-org7f0975c" class="outline-3">
<h3 id="org7f0975c">Linee guida rapide</h3>
<div class="outline-text-3" id="text-org7f0975c">
<ul class="org-ul">
<li><b>Bastoncini</b>: Per valori discreti, evita fraintendimenti sugli intervalli</li>
<li><b>Barre</b>: Per confronti categorici chiari</li>
<li><b>Poligonale</b>: Per tendenze, connessioni e serie temporali</li>
<li><b>Torta</b>: Solo per poche categorie che sommano al 100%</li>
<li><b>Istogramma</b>: Per visualizzare distribuzione e forma di dati continui</li>
<li><b>Scatter</b>: Per esplorare relazioni statistiche tra due variabili</li>
<li><b>Multipli</b>: Per confrontare più aspetti contemporaneamente</li>
</ul>
</div>
</div>
<div id="outline-container-orgd313567" class="outline-3">
<h3 id="orgd313567">Personalizzazione rapida degli assi e grafici</h3>
<div class="outline-text-3" id="text-orgd313567">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">Personalizzazioni comuni dell'oggetto ax</span>
ax.set_title(<span style="color: #8b2252;">'Titolo del grafico'</span>, fontsize=14)
ax.set_xlabel(<span style="color: #8b2252;">'Etichetta asse x'</span>, fontsize=12)
ax.set_ylabel(<span style="color: #8b2252;">'Etichetta asse y'</span>, fontsize=12)
ax.set_xlim([xmin, xmax])
ax.set_ylim([ymin, ymax])
ax.grid(<span style="color: #008b8b;">True</span>, linestyle=<span style="color: #8b2252;">'--'</span>, alpha=0.7)
ax.legend(loc=<span style="color: #8b2252;">'best'</span>, fontsize=10)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Annotazioni</span>
ax.text(x, y, <span style="color: #8b2252;">"Testo"</span>, fontsize=12)
ax.annotate(<span style="color: #8b2252;">'Punto importante'</span>, xy=(x, y), xytext=(x+1, y+1),
           arrowprops=<span style="color: #483d8b;">dict</span>(facecolor=<span style="color: #8b2252;">'black'</span>, shrink=0.05))

<span style="color: #b22222;"># </span><span style="color: #b22222;">Stili di linea e marker</span>
<span style="color: #b22222;"># </span><span style="color: #b22222;">linestyle: '-', '--', '-.', ':', 'None'</span>
<span style="color: #b22222;"># </span><span style="color: #b22222;">marker: 'o', 's', '^', 'v', '&lt;', '&gt;', 'p', '*', '+', 'x', 'D'</span>
ax.plot(x, y, linestyle=<span style="color: #8b2252;">'--'</span>, marker=<span style="color: #8b2252;">'o'</span>, color=<span style="color: #8b2252;">'red'</span>, linewidth=2, markersize=8)
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-orgb4f68ba" class="outline-2">
<h2 id="orgb4f68ba">Statistica descrittiva (continuare con anki e revisione da qui)</h2>
<div class="outline-text-2" id="text-orgb4f68ba">
</div>
<div id="outline-container-org28044a0" class="outline-3">
<h3 id="org28044a0">Dati quantitativi e qualitativi</h3>
<div class="outline-text-3" id="text-org28044a0">
<dl class="org-dl">
<dt>Dati quantitativi</dt><dd>Misurati attraverso quantità numeriche
<ul class="org-ul">
<li><b><b>Discreti</b></b>: Ha senso considerare singoli valori specifici (es. anno di prima apparizione)</li>
<li><b><b>Continui</b></b>: Ha senso considerare intervalli di valori (es. altezza, peso)</li>
</ul></dd>

<dt>Dati qualitativi (categorici/nominali)</dt><dd>Misurati attraverso etichette selezionate da un insieme predefinito
<ul class="org-ul">
<li><b><b>Binari/booleani</b></b>: Due sole possibili etichette non confrontabili (es. Gender: M/F)</li>
<li><b><b>Nominali</b></b> (sconnessi): Multiple etichette non confrontabili tra loro (es. Name, Eye color)
<ul class="org-ul">
<li>È possibile solo stabilire relazioni di equivalenza (uguale/diverso)</li>
</ul></li>
<li><b><b>Ordinali</b></b>: Etichette tra cui è possibile stabilire una relazione d'ordine (es. Intelligence)</li>
</ul></dd>
</dl>

<p>
Certi dati temporali come gli anni sono formalmente numerici ma possono essere considerati qualitativi quando le operazioni aritmetiche perdono significato.
</p>

<p>
Nei dataset reali, va inoltre considerato che anche i dati continui vengono discretizzati quando memorizzati su computer, rendendo la distinzione talvolta sfumata.
</p>
</div>
</div>
<div id="outline-container-orgcd936b4" class="outline-3">
<h3 id="orgcd936b4">Frequenze</h3>
<div class="outline-text-3" id="text-orgcd936b4">
<p>
La <b>frequenza assoluta</b> è il conteggio del numero di volte che una data osservazione occorre in un campione.
</p>

<p>
La <b>frequenza relativa</b> è la frequenza del valore di un dato divisa per il numero totale di elementi in un insieme di dati, ovvero la frazione dei casi in cui quell'osservazione occorre.
</p>

<p>
In <code>pandas</code> si ottengono con il metodo <code>crosstab</code>, il cui parametro <code>normalize</code> è <code>True</code> per le frequenze relative, <code>False</code> altrimenti.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">tab_freq</span> = pd.crosstab(index = serie,
                       columns = <span style="color: #8b2252;">'Abs. frequence'</span>,
                       colnames=[<span style="color: #8b2252;">''</span>],
                       normalize = <span style="color: #008b8b;">True</span>)
</pre>
</div>
</div>
<div id="outline-container-org6c873ff" class="outline-4">
<h4 id="org6c873ff">Arrotondare il numero di cifre decimali</h4>
<div class="outline-text-4" id="text-org6c873ff">
<div class="org-src-container">
<pre class="src src-python">publisher_rel_freq.<span style="color: #483d8b;">apply</span>(<span style="color: #a020f0;">lambda</span> p: 100 * np.<span style="color: #483d8b;">round</span>(p, 3))
</pre>
</div>
</div>
</div>
<div id="outline-container-org743852a" class="outline-4">
<h4 id="org743852a">Mostrare frequenza relativa in percentuali</h4>
<div class="outline-text-4" id="text-org743852a">
<div class="org-src-container">
<pre class="src src-python">(publisher_rel_freq.<span style="color: #483d8b;">apply</span>(<span style="color: #a020f0;">lambda</span> p: np.<span style="color: #483d8b;">round</span>(100*p, 2))
                   .astype(<span style="color: #483d8b;">str</span>)
                   .<span style="color: #483d8b;">apply</span>(<span style="color: #a020f0;">lambda</span> s: s + <span style="color: #8b2252;">'%'</span>))
</pre>
</div>
</div>
</div>
<div id="outline-container-orgfa65629" class="outline-4">
<h4 id="orgfa65629">Rinormalizzare su un sottoinsieme delle osservazioni</h4>
<div class="outline-text-4" id="text-orgfa65629">
<p>
Quando si filtrano alcune osservazioni (ad esempio quelle con un valore inferiore ad un certo valore), la somma di tutte le frequenze relative non sarà più \(1\).
</p>

<p>
Per rinormalizzare le frequenze relative bisogna calcolare la somma delle osservazioni in considerazione e poi dividere ogni frequenza relativa per quel numero.
</p>

<p>
Il modo safe (utilizzando <code>.copy()</code> come best-practice per evitare ambiguità di modifica) su <b>dataframe</b> è il seguente:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">Filter the dataframe</span>
<span style="color: #a0522d;">filtered_df</span> = df[df[<span style="color: #8b2252;">'Rel. frequence'</span>] &gt; 0.04].copy()
<span style="color: #b22222;"># </span><span style="color: #b22222;">Renormalize just the 'Rel. frequence' column</span>
<span style="color: #a0522d;">filtered_df</span>[<span style="color: #8b2252;">'Rel. frequence'</span>] = filtered_df[<span style="color: #8b2252;">'Rel. frequence'</span>] / filtered_df[<span style="color: #8b2252;">'Rel. frequence'</span>].<span style="color: #483d8b;">sum</span>()
</pre>
</div>

<p>
Se invece si lavora su una <b>serie</b> (situazione solita), basta usare la notazione mediante un singolo operatore per applicare la trasformazione a tutti i valori della serie:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">Restringi serie</span>
<span style="color: #a0522d;">serie</span> = rel_freq[rel_freq &gt; 0.04]
<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcola somma serie ristretta</span>
<span style="color: #a0522d;">tsum</span> = serie.<span style="color: #483d8b;">sum</span>()
<span style="color: #b22222;"># </span><span style="color: #b22222;">Applica trasformazione con sintassi breve</span>
<span style="color: #a0522d;">serie_renormalizzata1</span> = serie/tsum
<span style="color: #b22222;"># </span><span style="color: #b22222;">Oppure applica trasformazione con apply, che &#232; inutile</span>
<span style="color: #b22222;">#</span><span style="color: #b22222;">serie_renormalizzata2 = serie.apply(lambda p: p/tsum)</span>
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-org53b88bb" class="outline-3">
<h3 id="org53b88bb">Frequenze cumulate</h3>
<div class="outline-text-3" id="text-org53b88bb">
<p>
La frequenza cumulata (assoluta o relativa), associata ad una modalità su cui è possibile definire una relazione d'ordine, è pari alla somma della sua frequenza (assoluta o relativa) e di quelle delle modalità che la precedono.
</p>

<p>
In <code>pandas</code> si ottiene con il metodo <code>.cumsum()</code> su un <code>dataframe</code>.
</p>
</div>
<div id="outline-container-org96ad5bf" class="outline-4">
<h4 id="org96ad5bf"><span class="todo TODO">TODO</span> Funzione cumulativa empirica</h4>
<div class="outline-text-4" id="text-org96ad5bf">
<p>
Una funzione che restituisce la proporzione di dati sotto un certo valore, aumentando di \(frac{1}{n}\) ad ogni osservazione di una certa modalità. È uno stimatore consistente e non deviato della funzione di ripartizione.
</p>

<div class="org-src-container">
<pre class="src src-python">
<span style="color: #b22222;">#</span><span style="color: #b22222;">manualmente, ordino</span>
<span style="color: #a0522d;">acres_sorted</span> = ps[<span style="color: #8b2252;">'Acres'</span>].sort_values()
<span style="color: #a0522d;">cumulative_prob_1</span> = np.arange(1, <span style="color: #483d8b;">len</span>(acres_sorted) + 1) / <span style="color: #483d8b;">len</span>(acres_sorted)

<span style="color: #b22222;">#</span><span style="color: #b22222;">altrimenti</span>
<span style="color: #a020f0;">import</span> statsmodels.api <span style="color: #a020f0;">as</span> sm
<span style="color: #a0522d;">ecdf</span> = sm.distributions.ECDF(ps[<span style="color: #8b2252;">'Acres'</span>]).dropna() <span style="color: #b22222;">#</span><span style="color: #b22222;">se non droppo i na, non avr&#242; mai 1 anche per valori grandi</span>
<span style="color: #a0522d;">x</span> = np.arange(0, 12)
<span style="color: #a0522d;">y</span> = ecdf(x)
</pre>
</div>


<p>
Plotting:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">Creiamo una figura con due subplot</span>
fig, (<span style="color: #a0522d;">ax1</span>, <span style="color: #a0522d;">ax2</span>) = plt.subplots(1, 2, figsize=(12, 5))

<span style="color: #b22222;"># </span><span style="color: #b22222;">Plot per dati continui - grafico a linea a gradini</span>
ax1.step(ecdf_continuous.x, ecdf_continuous.y, where=<span style="color: #8b2252;">'post'</span>, label=<span style="color: #8b2252;">'ECDF'</span>, color=<span style="color: #8b2252;">'blue'</span>)
ax1.set_title(<span style="color: #8b2252;">'ECDF per Dati Continui'</span>)
ax1.set_xlabel(<span style="color: #8b2252;">'Valore'</span>)
ax1.set_ylabel(<span style="color: #8b2252;">'Proporzione Cumulativa'</span>)
ax1.grid(<span style="color: #008b8b;">True</span>, linestyle=<span style="color: #8b2252;">'--'</span>, alpha=0.7)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Plot per dati discreti - grafico a gradini con punti marcati</span>
<span style="color: #a0522d;">x_unique</span> = np.unique(discrete_data)
<span style="color: #a0522d;">y_unique</span> = [np.mean(discrete_data &lt;= x) <span style="color: #a020f0;">for</span> x <span style="color: #a020f0;">in</span> x_unique]
ax2.step(x_unique, y_unique, where=<span style="color: #8b2252;">'post'</span>, label=<span style="color: #8b2252;">'ECDF'</span>, color=<span style="color: #8b2252;">'red'</span>)
ax2.scatter(x_unique, y_unique, color=<span style="color: #8b2252;">'red'</span>, zorder=3)  <span style="color: #b22222;"># </span><span style="color: #b22222;">Aggiungiamo punti per evidenziare i valori discreti</span>
ax2.set_title(<span style="color: #8b2252;">'ECDF per Dati Discreti'</span>)
ax2.set_xlabel(<span style="color: #8b2252;">'Valore'</span>)
ax2.set_ylabel(<span style="color: #8b2252;">'Proporzione Cumulativa'</span>)
ax2.grid(<span style="color: #008b8b;">True</span>, linestyle=<span style="color: #8b2252;">'--'</span>, alpha=0.7)

plt.tight_layout()
plt.show()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Alternativa per dati discreti - grafico a barre cumulative</span>
plt.figure(figsize=(8, 5))
<span style="color: #a0522d;">counts</span> = np.bincount(discrete_data)
<span style="color: #a0522d;">cumcounts</span> = np.cumsum(counts) / <span style="color: #483d8b;">len</span>(discrete_data)
plt.bar(<span style="color: #483d8b;">range</span>(<span style="color: #483d8b;">len</span>(cumcounts)), cumcounts, width=0.7, color=<span style="color: #8b2252;">'green'</span>, alpha=0.7)
plt.title(<span style="color: #8b2252;">'ECDF per Dati Discreti - Grafico a Barre Cumulative'</span>)
plt.xlabel(<span style="color: #8b2252;">'Valore'</span>)
plt.ylabel(<span style="color: #8b2252;">'Proporzione Cumulativa'</span>)
plt.grid(<span style="color: #008b8b;">True</span>, linestyle=<span style="color: #8b2252;">'--'</span>, alpha=0.7, axis=<span style="color: #8b2252;">'y'</span>)
plt.show()
</pre>
</div>
</div>
</div>
<div id="outline-container-org3398d1a" class="outline-4">
<h4 id="org3398d1a">Diagrammi di Pareto</h4>
<div class="outline-text-4" id="text-org3398d1a">
<p>
Il diagramma di Pareto di un campione unisce il grafico <b>a barre</b> delle <b>frequenze relative</b> ed il grafico <b>poligonale</b> delle <b>frequenze cumulate relative</b>.
</p>

<p>
Per generare il diagramma a partire da una serie, considerando solo i dati superiori ad un certo valore e scegliendo se renormalizzare o meno, si fa così:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">def</span> <span style="color: #0000ff;">pareto_plot</span>(data, threshold, renormalize=<span style="color: #008b8b;">True</span>, bins=30, figsize=(12, 6)):
    <span style="color: #8b2252;">"""Versione compatta del diagramma di Pareto con ECDF"""</span>
    <span style="color: #b22222;"># </span><span style="color: #b22222;">Filtraggio e istogramma</span>
    <span style="color: #a0522d;">filtered_data</span> = data[data &gt; threshold]
    <span style="color: #a0522d;">counts</span>, <span style="color: #a0522d;">bin_edges</span> = np.histogram(filtered_data, bins=bins)
    <span style="color: #a0522d;">bin_centers</span> = (bin_edges[:-1] + bin_edges[1:]) / 2

    <span style="color: #b22222;"># </span><span style="color: #b22222;">Ordinamento decrescente</span>
    <span style="color: #a0522d;">sorted_indices</span> = np.argsort(-counts)
    <span style="color: #a0522d;">sorted_counts</span> = counts[sorted_indices]
    <span style="color: #a0522d;">sorted_bin_centers</span> = bin_centers[sorted_indices]

    <span style="color: #b22222;"># </span><span style="color: #b22222;">Frequenze relative</span>
    <span style="color: #a0522d;">relative_freq</span> = sorted_counts / (filtered_data.<span style="color: #483d8b;">sum</span>() <span style="color: #a020f0;">if</span> renormalize <span style="color: #a020f0;">else</span> data.<span style="color: #483d8b;">sum</span>())

    <span style="color: #b22222;"># </span><span style="color: #b22222;">Grafico</span>
    <span style="color: #a0522d;">fig</span>, <span style="color: #a0522d;">ax1</span> = plt.subplots(figsize=figsize)
    ax1.bar(np.arange(<span style="color: #483d8b;">len</span>(sorted_counts)), relative_freq, color=<span style="color: #8b2252;">'skyblue'</span>, edgecolor=<span style="color: #8b2252;">'black'</span>)
    ax1.set_ylabel(<span style="color: #8b2252;">'Frequenza Relativa'</span>, color=<span style="color: #8b2252;">'skyblue'</span>)

    <span style="color: #b22222;"># </span><span style="color: #b22222;">ECDF su asse secondario</span>
    <span style="color: #a0522d;">ax2</span> = ax1.twinx()
    ax2.plot(np.arange(<span style="color: #483d8b;">len</span>(sorted_counts)), np.cumsum(relative_freq), <span style="color: #8b2252;">'o-'</span>, color=<span style="color: #8b2252;">'navy'</span>)
    ax2.set_ylabel(<span style="color: #8b2252;">'ECDF'</span>, color=<span style="color: #8b2252;">'navy'</span>)
    ax2.axhline(y=0.8, color=<span style="color: #8b2252;">'red'</span>, linestyle=<span style="color: #8b2252;">'--'</span>, alpha=0.7)

    plt.title(f<span style="color: #8b2252;">'Diagramma di Pareto (threshold &gt; </span>{threshold:.2f}<span style="color: #8b2252;">)'</span>)
    <span style="color: #a020f0;">return</span> fig, ax1, ax2

<span style="color: #a0522d;">fig</span>, <span style="color: #a0522d;">ax1</span>, <span style="color: #a0522d;">ax2</span> = pareto_diagram(y, threshold=0, renormalize=<span style="color: #008b8b;">True</span>)
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-orgf5ed488" class="outline-3">
<h3 id="orgf5ed488">Frequenze congiunte e marginali</h3>
<div class="outline-text-3" id="text-orgf5ed488">
<p>
Le <b>frequenze congiunte*</b> contano il numero di osservazioni in cui due caratteri (variabili) assumono simultaneamente determinati valori. Rappresentano la distribuzione bidimensionale di due variabili.
</p>

<p>
Vengono stampate nelle <b>tabelle di contingenza</b>, in cui le righe sono una variabile, le colonne l'altra e i valori sono le frequenze congiunte.
</p>

<p>
Le <b>frequenze marginali</b> sono i totali di riga e colonna in una tabella di contingenza e rappresentano le frequenze di ciascuna variabile considerata singolarmente.
</p>

<p>
Le tabelle di contingenza relative possono essere <b>normalizzate</b> nei seguenti modi:
</p>
<dl class="org-dl">
<dt>Normalizzazione totale ('all')</dt><dd>Ogni cella rappresenta la proporzione rispetto al totale generale delle osservazioni. Tutte le celle sommate danno 1.</dd>
<dt>Normalizzazione per riga ('index')</dt><dd>Ogni cella rappresenta la proporzione rispetto al totale della riga. Ogni riga somma a 1, mostrando la distribuzione della variabile colonna all'interno di ciascun valore della variabile riga.</dd>
<dt>Normalizzazione per colonna ('columns')</dt><dd>Ogni cella rappresenta la proporzione rispetto al totale della colonna. Ogni colonna somma a 1, mostrando la distribuzione della variabile riga all'interno di ciascun valore della variabile colonna.</dd>
</dl>
<p>
Se \(x\) è sulle righe e \(y\) sulle colonne, si normalizza in base ad \(x\) quando si vogliono ottenere le frequenze relative dei valori di \(y\) per gli specifici valori di \(x\). 
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #b22222;"># </span><span style="color: #b22222;">Creazione di una tabella di frequenze congiunte tra intelligenza e genere</span>
<span style="color: #a0522d;">int_gender_freq</span> = pd.crosstab(index=heroes[<span style="color: #8b2252;">'Intelligence'</span>], 
                              columns=heroes[<span style="color: #8b2252;">'Gender'</span>])

<span style="color: #b22222;"># </span><span style="color: #b22222;">Riordinare le righe secondo un ordine logico anzich&#233; alfabetico</span>
<span style="color: #a0522d;">int_gender_freq</span> = int_gender_freq.reindex([<span style="color: #8b2252;">'low'</span>, <span style="color: #8b2252;">'moderate'</span>,
                                           <span style="color: #8b2252;">'average'</span>, <span style="color: #8b2252;">'good'</span>, <span style="color: #8b2252;">'high'</span>])

<span style="color: #b22222;"># </span><span style="color: #b22222;">Riordinare le colonne specificando l'ordine desiderato</span>
int_gender_freq.loc[:,[<span style="color: #8b2252;">'M'</span>, <span style="color: #8b2252;">'F'</span>]]

<span style="color: #b22222;"># </span><span style="color: #b22222;">Selezionare solo alcune righe della tabella</span>
<span style="color: #a0522d;">subset_freq</span> = int_gender_freq.loc[<span style="color: #8b2252;">'moderate'</span>:<span style="color: #8b2252;">'good'</span>, :]

<span style="color: #b22222;"># </span><span style="color: #b22222;">Visualizzazione grafica con barre affiancate</span>
int_gender_freq.plot.bar(color=[<span style="color: #8b2252;">'pink'</span>, <span style="color: #8b2252;">'blue'</span>])
plt.show()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Visualizzazione grafica con barre impilate</span>
int_gender_freq.plot.bar(color=[<span style="color: #8b2252;">'pink'</span>, <span style="color: #8b2252;">'blue'</span>], stacked=<span style="color: #008b8b;">True</span>)
plt.show()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Gestione di variabili quantitative mediante discretizzazione</span>
<span style="color: #b22222;"># </span><span style="color: #b22222;">Converte i pesi numerici in intervalli categorici</span>
<span style="color: #a0522d;">weight_categories</span> = pd.cut(heroes[<span style="color: #8b2252;">'Weight'</span>], 
                          bins=[30, 50, 80, 100, 200, 500, 1000])

<span style="color: #b22222;"># </span><span style="color: #b22222;">Creazione di tabella di contingenza con la variabile discretizzata</span>
<span style="color: #a0522d;">weight_gender_freq</span> = pd.crosstab(index=weight_categories,
                                columns=[heroes[<span style="color: #8b2252;">'Gender'</span>]])

<span style="color: #b22222;"># </span><span style="color: #b22222;">Modifica della direzione degli intervalli (aperti a sinistra)</span>
<span style="color: #a0522d;">weight_gender_freq_left</span> = pd.crosstab(index=pd.cut(heroes[<span style="color: #8b2252;">'Weight'</span>],
                                                 bins=[30, 50, 80, 100, 200, 500, 1000],
                                                 right=<span style="color: #008b8b;">False</span>),
                                     columns=[heroes[<span style="color: #8b2252;">'Gender'</span>]])

<span style="color: #b22222;"># </span><span style="color: #b22222;">Aggiunta delle frequenze marginali (totali di riga e colonna)</span>
<span style="color: #a0522d;">freq_with_margins</span> = pd.crosstab(index=heroes[<span style="color: #8b2252;">'Intelligence'</span>], 
                               columns=heroes[<span style="color: #8b2252;">'Gender'</span>], 
                               margins=<span style="color: #008b8b;">True</span>)  <span style="color: #b22222;"># </span><span style="color: #b22222;">Aggiunge riga e colonna 'All'</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo delle frequenze relative normalizzate sul totale</span>
<span style="color: #a0522d;">rel_freq_all</span> = pd.crosstab(index=heroes[<span style="color: #8b2252;">'Intelligence'</span>], 
                          columns=heroes[<span style="color: #8b2252;">'Gender'</span>],
                          margins=<span style="color: #008b8b;">True</span>,
                          normalize=<span style="color: #8b2252;">'all'</span>)  <span style="color: #b22222;"># </span><span style="color: #b22222;">Ogni cella divisa per il numero totale</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo delle frequenze relative normalizzate per riga</span>
<span style="color: #a0522d;">rel_freq_row</span> = pd.crosstab(index=heroes[<span style="color: #8b2252;">'Intelligence'</span>], 
                          columns=heroes[<span style="color: #8b2252;">'Gender'</span>],
                          margins=<span style="color: #008b8b;">True</span>,
                          normalize=<span style="color: #8b2252;">'index'</span>)  <span style="color: #b22222;"># </span><span style="color: #b22222;">Ogni riga somma a 1</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo delle frequenze relative normalizzate per colonna</span>
<span style="color: #a0522d;">rel_freq_col</span> = pd.crosstab(index=heroes[<span style="color: #8b2252;">'Intelligence'</span>], 
                          columns=heroes[<span style="color: #8b2252;">'Gender'</span>],
                          margins=<span style="color: #008b8b;">True</span>,
                          normalize=<span style="color: #8b2252;">'columns'</span>)  <span style="color: #b22222;"># </span><span style="color: #b22222;">Ogni colonna somma a 1</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Visualizzazione delle frequenze normalizzate per colonna con grafico a barre</span>
pd.crosstab(index=heroes[<span style="color: #8b2252;">'Strength'</span>],
           columns=[heroes[<span style="color: #8b2252;">'Gender'</span>]],
           normalize=<span style="color: #8b2252;">'columns'</span>).plot.bar(color=[<span style="color: #8b2252;">'pink'</span>, <span style="color: #8b2252;">'blue'</span>],
                                        stacked=<span style="color: #008b8b;">False</span>)
plt.show()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Creazione di un diagramma di dispersione (scatter plot)</span>
<span style="color: #b22222;"># </span><span style="color: #b22222;">per visualizzare la relazione tra altezza e peso</span>
heroes[heroes[<span style="color: #8b2252;">'Gender'</span>]==<span style="color: #8b2252;">'M'</span>].plot.scatter(<span style="color: #8b2252;">'Height'</span>, <span style="color: #8b2252;">'Weight'</span>)
plt.show()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Aggiunta di una linea di tendenza manuale</span>
heroes[heroes[<span style="color: #8b2252;">'Gender'</span>]==<span style="color: #8b2252;">'M'</span>].plot.scatter(<span style="color: #8b2252;">'Height'</span>, <span style="color: #8b2252;">'Weight'</span>)
<span style="color: #a0522d;">trend</span> = <span style="color: #a020f0;">lambda</span> x: -1200 + x * 7
<span style="color: #a0522d;">x_range</span> = [170, 300]
<span style="color: #a0522d;">line</span>, = plt.plot(x_range, <span style="color: #483d8b;">list</span>(<span style="color: #483d8b;">map</span>(trend, x_range)), color=<span style="color: #8b2252;">'black'</span>)
line.set_dashes([3, 2])  <span style="color: #b22222;"># </span><span style="color: #b22222;">Linea tratteggiata</span>
line.set_linewidth(2)    <span style="color: #b22222;"># </span><span style="color: #b22222;">Spessore della linea</span>
plt.show()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo e visualizzazione di una linea di tendenza con regressione lineare</span>
<span style="color: #a020f0;">from</span> sklearn <span style="color: #a020f0;">import</span> linear_model

<span style="color: #b22222;"># </span><span style="color: #b22222;">Creazione di un modello di regressione lineare</span>
<span style="color: #a0522d;">regr</span> = linear_model.LinearRegression()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Preparazione dei dati (rimozione dei valori mancanti)</span>
<span style="color: #a0522d;">heroes_with_data</span> = heroes[heroes[<span style="color: #8b2252;">'Gender'</span>]==<span style="color: #8b2252;">'M'</span>].copy().dropna()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Definizione delle variabili X (predittore) e Y (risposta)</span>
<span style="color: #a0522d;">X</span> = heroes_with_data.loc[:, [<span style="color: #8b2252;">'Height'</span>]]
<span style="color: #a0522d;">Y</span> = heroes_with_data[<span style="color: #8b2252;">'Weight'</span>]

<span style="color: #b22222;"># </span><span style="color: #b22222;">Addestramento del modello</span>
regr.fit(X, Y)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Visualizzazione del grafico con la linea di tendenza</span>
heroes[heroes[<span style="color: #8b2252;">'Gender'</span>]==<span style="color: #8b2252;">'M'</span>].plot.scatter(<span style="color: #8b2252;">'Height'</span>, <span style="color: #8b2252;">'Weight'</span>)
<span style="color: #a0522d;">line</span>, = plt.plot([0, 1000], regr.predict([[0], [1000]]), color=<span style="color: #8b2252;">'black'</span>)
line.set_dashes([3, 2])
line.set_linewidth(2)
plt.show()
</pre>
</div>
</div>
</div>
<div id="outline-container-orgf028389" class="outline-3">
<h3 id="orgf028389">Indici di centralità</h3>
<div class="outline-text-3" id="text-orgf028389">
</div>
<div id="outline-container-org2a78fed" class="outline-4">
<h4 id="org2a78fed">Media campionaria</h4>
<div class="outline-text-4" id="text-org2a78fed">
<p>
La media campionaria è definita come:
</p>

<p>
\[\bar{x}=\frac{1}{n}\sum_{i=1}^{n}x_{i}\]
</p>

<p>
Ovvero è la somma di tutti i valori divisa per il numero di osservazioni.
</p>

<p>
È un po' come il <b>baricentro</b> del campione.
</p>

<p>
Dato che vanno eseguite operazioni matematiche sui valori, la media può essere calcolata soltanto su dati <b>quantitativi</b> o <b>qualitativi</b> a cui è associato un <b>valore numerico</b>.
</p>

<p>
Dato un campione \(y = ax + b\), per \(a,b\) fissati, è facile derivare attraverso la formula che \(\bar{y} = a\bar{x}+y\), ovvero la media di un campione trasformato è la media del campione originale trasformata nello stesso modo.
</p>

<p>
Se di un campione si conoscono i \(k\) distinti valori quantitativi \(v\) del campione e le relative frequenze \(f\), allora la media è definita come:
\[\bar{x}=\frac{1}{n}\sum_{j=1}^{k}v_{j}f_{j}\], che è anche la <b>media pesata</b> dei valori distinti, dove il peso di ogni valore è la sua frequenza. Questo ragionamento vale sia per le frequenze assolute che relative.
</p>

<p>
La media campionaria <b>non è robusta</b>, ovvero è molto sensibile a <b>outlier</b> (valori fuori scala).
</p>

<p>
La somma di <b>tutti</b> gli scarti dei valori di un campione dalla media è nulla, ovvero
\[\sum_{i=1}^{n} (x_i - \bar{x}) = 0\]
</p>

<p>
Si calcola con <code>serie.mean()</code>
</p>
</div>
</div>
<div id="outline-container-orgfc9cf19" class="outline-4">
<h4 id="orgfc9cf19">Mediana campionaria</h4>
<div class="outline-text-4" id="text-orgfc9cf19">
<p>
La mediana campionaria è un altro indice di centralità.
</p>

<p>
Per calcolarla, si ordinano i valori dell'insieme dal più piccolo al più grande.
</p>

<p>
Se il numero totale di elementi \(n\) è dispari, la mediana è il valore che occupa la posizione \((n + 1)/2\). Se n è pari, invece, la mediana è data dalla media dei due valori centrali, ovvero quelli nelle posizioni \(n/2\) e \(n/2 + 1\).
</p>

<p>
A differenza della media, la mediana può essere calcolata anche su dati <b>qualitativi</b>, purchè questi siano <b>ordinabili</b> e siano in numero <b>dispari</b>. Se fossero pari, sarebbe richiesto di fare la media dei due valori centrali, che è impossibile.
</p>

<p>
La mediana è <b>robusta</b> rispetto ad eventuali outlier.
</p>

<p>
<code>serie.quantile(0.5)</code>
</p>
</div>
</div>
<div id="outline-container-org0baab7f" class="outline-4">
<h4 id="org0baab7f">Moda campionaria</h4>
<div class="outline-text-4" id="text-org0baab7f">
<p>
La <b>moda</b> è l'attributo che appare più spesso. Se esistono più attributi con frequenza massima, vengono chiamati <b>valori modali</b>. Può essere calcolata su qualunque tipo di dato.
</p>
</div>
</div>
</div>
<div id="outline-container-org589bb4c" class="outline-3">
<h3 id="org589bb4c">Indici di dispersione</h3>
<div class="outline-text-3" id="text-org589bb4c">
<p>
Gli indici di dispersione sono utilizzati per capire la <b>variabilità</b> o lo <b>spread</b> di un campione.
</p>
</div>
<div id="outline-container-org9e46e20" class="outline-4">
<h4 id="org9e46e20">Varianza campionaria</h4>
<div class="outline-text-4" id="text-org9e46e20">
<p>
\[ s^2 = \frac{1}{n-1} \sum_{i=1}^{n} \left( x_i - \overline{x} \right)^2 \]
</p>

<p>
Il concetto su cui si basa la <b>varianza</b> campionaria è quello di accumulare la <b>distanza</b> fra ogni osservazione e la <b>mediana</b>.
</p>

<p>
Può essere calcolata <b>soltanto</b> su dati <b>quantitativi</b>.
</p>

<p>
Dalla definizione si ricava che la varianza di un campione trasformato linearmente è uguale alla varianza del campione originale <b>scalata</b> per <b>il quadrato del fattore</b>, mentre è <b>insensibile alla traslazione</b>.
</p>
</div>
<div id="outline-container-org2dc05a8" class="outline-5">
<h5 id="org2dc05a8">Perchè si usa il quadrato?</h5>
<div class="outline-text-5" id="text-org2dc05a8">
<p>
Per ottenere la somma delle differenze, occorre assicurarsi che le distanze calcolate siano sempre positive.
</p>

<p>
Infatti, le differenze per i valori superiori alla mediana, se lasciate invariate, produrrebbero valori negativi che ridurrebbero l'accumulazione totale.
</p>

<p>
Utilizzare la funzione del <b>valore assoluto</b> rappresenterebbe una scelta <b>subottimale</b>, poiché non si presta agevolmente a trasformazioni lineari.
</p>

<p>
Di conseguenza, l'approccio preferito consiste nel considerare il <b>quadrato</b> di ciascuna differenza.
</p>
</div>
</div>
</div>
<div id="outline-container-org39dcb4a" class="outline-4">
<h4 id="org39dcb4a"><a id="org6db3d4f">Deviazione standard</a></h4>
<div class="outline-text-4" id="text-org39dcb4a">
<p>
La <b><a href="#org6db3d4f">deviazione standard</a></b> è il quadrato della varianza campionaria. È utile perchè permette di esprimere la varianza nell'unità di misura originale.
</p>

<p>
Come la varianza è insensibile alla traslazione, mentre scala come il <b>valore assoluto del fattore</b> con cui è scalato il campione. Infatti, non avrebbe senso considerare una varianza negativa.
</p>
</div>
</div>
</div>
<div id="outline-container-orgc8c377b" class="outline-3">
<h3 id="orgc8c377b">Quantili</h3>
<div class="outline-text-3" id="text-orgc8c377b">
<p>
Un <b>quantile</b> di <b>livello</b> \(p \in [0,1]\) di un campione da \(n\) <b>osservazioni</b> è uno dei valori del campione tale per cui \(np\) valori del campione sono minori o uguali ad esso e \(n(1-p)\) valori ne sono invece maggiori o uguali. \(100p\) è la percentuale di valori minori o uguali a quello scelto e \(100(1-p)\) resto sono maggiori.
</p>

<p>
Se \(np\) è <b>intero</b>, allora si cerca quel valore che sia
</p>
<ul class="org-ul">
<li>maggiore o uguale a \(np\) elementi.
Lo trovo ordinando i valori del campione e scandendo dal più piccolo. L'elemento che incontro dopo \(n-p\) passi, ovvero in posizione \(np\), rispetta questa condizione.</li>
<li>minore o uguale a \(n-np\) elementi. Ordinando i valori e scandendo dal più grande, l'elemento \(n-np\) rispetta la condizione. Dato che al \(1°\) elemento scandito mi trovo in posizione \(n\), al \(2°\) in posizione \(n-1\), la posizione in cui mi trovo al passo \(k\) è \(n+1-k\). Quindi, l'elemento che ho trovato è quello in posizione \(n+1-(n-np)\), ovvero \(np+1\).</li>
</ul>

<p>
Di conseguenza, quando \(np\) è intero ci sono \(2\) elementi che rispettano entrambe le condizioni. Allora se ne prende la <b>media</b>.
</p>

<p>
Dato che il numero di valori in un campione è intero, se \(np\) è <b>razionale</b>, allora si cerca il valore che sia minore uguale di \(\lceil np \rceil\) elementi e maggiore di \(\lceil n-np \rceil\) elementi. Quindi l'$np$-esimo elemento a partire dal più piccolo non basta e quindi devo andare in posizione \(np+1\), mentre, come nel caso intero, prendo l'$n-np$-esimo elemento partendo dal più grande, ovvero quello in posizione \(np+1\).
</p>

<p>
Quindi, quando \(np\) è razionale esiste <b>un solo</b> valore che rispetta entrambe le condizioni, ovvero quello in posizione \(np+1\) del campione ordinato ed è proprio quello il quantile di livello \(p\).
</p>

<p>
I quantili dove \(p\) ha al massimo due cifre decimali sono chiamati <b>percentili</b>, mentre quelli con \(p\) corrispondente ai multipli interi di \(0.25\) sono chiamati <b>quartili</b>.
</p>
</div>
<div id="outline-container-org0403396" class="outline-4">
<h4 id="org0403396">Box plot</h4>
<div class="outline-text-4" id="text-org0403396">
<p>
Il box plot è un grafico che riassume le seguenti informazioni: <b>minimo e massimo</b> del campione come <b>estremi dei baffi</b>, <b>primo e terzo</b> quartile come <b>estremi della scatola</b> e <b>mediana</b> (o secondo quartile) come <b>linea nella scatola</b>.
</p>
</div>
<div id="outline-container-org8dc1c5f" class="outline-5">
<h5 id="org8dc1c5f"><span class="todo TODO">TODO</span> Capire cosa ha detto il professore riguardo alla relazione fra box plot e distribuzione e capire cosa c'entra la media</h5>
</div>
</div>
<div id="outline-container-org5023b89" class="outline-4">
<h4 id="org5023b89">Range inter-quartile</h4>
<div class="outline-text-4" id="text-org5023b89">
<p>
Il <b>range inter-quartile</b> è un <b>indice di dispersione</b> ed è calcolato come \(Q_3 - Q_1\), ovvero l'ampiezza della fascia di valori che circonda la mediana senza entrare nei quartili adiacenti. Corrisponde alla lunghezza del box nel box plot.
</p>

<p>
Quanto più questo range è grande, quanto più i valori sono lontani dalla mediana.
</p>
</div>
</div>
<div id="outline-container-orgf51e9d1" class="outline-4">
<h4 id="orgf51e9d1">QQ plot</h4>
<div class="outline-text-4" id="text-orgf51e9d1">
<p>
I grafici <b>quantile-quantile</b> sono usati per confrontare le distribuzioni di due campioni.
</p>

<p>
L'idea è quella di introdurre un sistema di riferimento cartesiano sulle cui assi si trovano i corrispondenti quantili dei due campioni e i punti si trovano in corrispondenza del valore dei due quantili.
</p>

<p>
Se i valori si allineano sulla funzione identità \(y=x\), ovvero sulla bisettrice del primo e terzo quadrante, allora le distribuzioni sono simili. Se si allineano su una retta diversa da quella precedente, c'è una relazione lineare fra i due campioni.
</p>

<p>
Il QQ plot è uno metodo grafico e <b>qualitativo</b>, usato soltanto per refutare o validare ipotesi su campioni.
</p>
</div>
<div id="outline-container-org11d9f65" class="outline-5">
<h5 id="org11d9f65"><span class="todo TODO">TODO</span> Come stampare?</h5>
</div>
</div>
</div>
<div id="outline-container-orgf40875c" class="outline-3">
<h3 id="orgf40875c">Distribuzione normale</h3>
<div class="outline-text-3" id="text-orgf40875c">
<p>
Un campione le frequenza dei valori presi in ordine cresce fino a raggiungere il picco in corrispondenza della mediana e poi decresce con la stessa pendenza con cui era cresciuta viene chiamata <b>normale</b>.
</p>

<p>
Questi campioni seguono una <b>regola empirica</b> per la quale (dove \(\bar{x}\) è la media e \(s\) è la <a href="#org6db3d4f">deviazione standard</a>:
</p>
<ul class="org-ul">
<li>il \(68\%\) delle osservazioni hanno valore compreso entro \(\bar{x} \pm s\)</li>
<li>il \(95\%\) delle osservazioni hanno valore compreso entro \(\bar{x} \pm 2s\)</li>
<li>il \(99.7\%\) delle osservazioni hanno valore compreso entro \(\bar{x} \pm 3s\)</li>
</ul>
</div>
</div>
<div id="outline-container-org8a94430" class="outline-3">
<h3 id="org8a94430">Coefficiente di correlazione campionaria</h3>
<div class="outline-text-3" id="text-org8a94430">
<p>
Lo scatter plot di due osservazioni in coppia può presentare una <b>tendenza lineare</b>. Ovvero, i valori sulle ascisse e sulle ordinate sono fra loro proporzionali.
</p>

<p>
Date le <b>medie campionarie</b> \(\bar x, \bar y\), consideriamo gli <b>scarti</b> fra ogni valore di un campione e la propria media. Quando lo scarto di un valore è <b>non negativo</b>, esso è più grande della propria media, ovvero \(x-\bar x \geq 0\). Quando è <b>non positivo</b>, esso è minore e quindi \(x-\bar x \leq 0\).
</p>

<p>
Allora, data una coppia di valori \((x_i, y_i)\), se essi sono entrambi maggiori o uguali della propria media, oppure sono entrambi minori o uguali, il <b>prodotto degli scarti</b> sarà <b>non negativo</b>, ovvero \((x-\bar x)(y-\bar y)\geq 0\). Quando invece un elemento della coppia è piccolo, mentre l'altro è grande, il prodotto dei loro scarti sarà <b>non positivo</b>.
</p>

<p>
Sommiamo quindi <b>tutti gli scarti</b>, ottenendo \(\sum_{i=1}^{n} (x-\bar x)(y-\bar y)\). Quanti più casi di coppie in cui gli entrambi gli elementi sono grandi o piccoli allo stesso momento, quanto maggiore sarà il risultato della sommatoria, mentre ogni caso di elementi l'uno grande e l'altro piccolo diminuisce il suo valore. <b>Standardizziamo</b> la somma dividendo per \((n-1)\). Questo indice si chiama <b>covarianza campionaria</b>:
</p>

<p>
\[ s_{xy} = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{n-1}\]
</p>

<p>
La covarianza campionaria è illimitata e la sua unità di misura è il prodotto delle unità di misura dei campioni.
</p>

<p>
Dato che i valori della covarianza sono nell'intervallo \([s_xs_y]\) (non dimostrato), normalizziamo dividendo per il <b>prodotto delle due deviazioni standard</b>. Il risultato è quindi quello di <b>scalare</b> il risultato della sommatoria limitandolo all'intervallo \([-1,1]\).
</p>

<p>
Si ottiene il <b>coefficiente di correlazione campionaria</b>
\[r_{xy} = \frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{(n-1)s_x s_y}\]
</p>

<p>
Quando \(r_{xy} > 0\), i dati sono <b>correlati positivamente</b>, o <b>direttamente</b>. Quando \(r_{xy} < 0\), i dati sono <b>correlati negativamente</b>, o <b>inversamente</b>. Quando \(r_{xy}\) è nullo, non c'è correlazione.
</p>

<p>
Inoltre, quanto più il valore di \(r_{xy}\) è alto, quanto più la correlazione è forte.
</p>

<p>
Se un dato è una trasformazione lineare dell'altro, allora:
</p>
<ul class="org-ul">
<li>\(r = 1\) se il coefficiente moltiplicativo è positivo;</li>
<li>\(r = -1\) se è negativo.</li>
</ul>

<p>
Se entrambi i dati sono trasformazioni lineari dello stesso dato, \(r = 1\) se i coefficienti sono concordi e l'opposto se discordi.
</p>

<p>
<code>pd.DataFrame({'x': serie1, 'y': serie2}).corr().loc['x', 'y']</code>, a partire da un dataframe fornisce una tabella (simmetrica diagonalmente) dei coefficienti di correlazione per tale riga e colonna. Se non ho un dataframe, me lo creo
</p>
</div>
</div>
<div id="outline-container-org74d6d2e" class="outline-3">
<h3 id="org74d6d2e"><span class="todo TODO">TODO</span> Lorenz e Gini</h3>
<div class="outline-text-3" id="text-org74d6d2e">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">def</span> <span style="color: #0000ff;">gini</span>(series):
    <span style="color: #a020f0;">return</span> 1 - <span style="color: #483d8b;">sum</span>(series.value_counts(normalize=<span style="color: #008b8b;">True</span>)
                         .<span style="color: #483d8b;">map</span>(<span style="color: #a020f0;">lambda</span> f: f**2))

<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">normalized_gini</span>(series):
    <span style="color: #a0522d;">s</span> = <span style="color: #483d8b;">len</span>(series.unique())
    <span style="color: #a020f0;">return</span> s * gini(series) / (s-1)
</pre>
</div>
</div>
</div>
<div id="outline-container-org1e7948c" class="outline-3">
<h3 id="org1e7948c">Grafici</h3>
<div class="outline-text-3" id="text-org1e7948c">
</div>
<div id="outline-container-org0d01e63" class="outline-4">
<h4 id="org0d01e63">A bastoncini</h4>
<div class="outline-text-4" id="text-org0d01e63">
<div class="org-src-container">
<pre class="src src-python">plt.vlines(selected_freq.index, 0, selected_freq.values)
<span style="color: #b22222;">#</span><span style="color: #b22222;">aggiungi puntino sull'estremit&#224; superiore</span>
plt.plot(selected_freq.index, selected_freq.values, <span style="color: #8b2252;">'o'</span>)
plt.show()
</pre>
</div>
<p>
oppure
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">fig</span>, <span style="color: #a0522d;">ax</span> = plt.subplots()
ax.vlines(dim_freq.index, 0, dim_freq.values)
<span style="color: #b22222;">#</span><span style="color: #b22222;">aggiungi puntino sull'estremit&#224; superiore</span>
ax.plot(dim_freq.index, dim_freq.values, <span style="color: #8b2252;">'o'</span>)
plt.show()
</pre>
</div>
</div>
</div>
<div id="outline-container-org2aa18b4" class="outline-4">
<h4 id="org2aa18b4">A barre</h4>
<div class="outline-text-4" id="text-org2aa18b4">
<p>
Metodo definito su dataframe
</p>
<div class="org-src-container">
<pre class="src src-python">dataframe.plot.bar(legend=<span style="color: #008b8b;">False</span>)
plt.show
</pre>
</div>
<p>
Metodo OOP
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">fig</span>, <span style="color: #a0522d;">ax</span> = plt.subplots()
ax.bar(dim_freq.index, dim_freq[<span style="color: #8b2252;">'Abs. frequence'</span>])
plt.show()
</pre>
</div>
<p>
Parametro <code>alpha</code> minore di 1 per sovrapporre colori diversi
</p>
</div>
</div>
<div id="outline-container-org36f7236" class="outline-4">
<h4 id="org36f7236">Poligonale</h4>
<div class="outline-text-4" id="text-org36f7236">
<p>
Metodo definito su dataframe, <code>marker</code> per puntino
</p>
<div class="org-src-container">
<pre class="src src-python">dataframe.plot(marker=<span style="color: #8b2252;">'o'</span>, color=<span style="color: #8b2252;">'blue'</span>, legend=<span style="color: #008b8b;">False</span>)
plt.show
</pre>
</div>
<p>
Metodo pyplot
</p>
<div class="org-src-container">
<pre class="src src-python">plt.plot(dim_freq, label = <span style="color: #8b2252;">'norm'</span>)
<span style="color: #a0522d;">new</span> = dim_freq * 2
plt.plot(new)
plt.legend()
</pre>
</div>
<p>
Usare invece step per step discreti
</p>
</div>
</div>
<div id="outline-container-org5ad55a6" class="outline-4">
<h4 id="org5ad55a6">A torta (aerogramma)</h4>
<div class="outline-text-4" id="text-org5ad55a6">
<p>
Sul dataframe
</p>
<div class="org-src-container">
<pre class="src src-python">dataframe.plot.pie(y=<span style="color: #8b2252;">'Abs. frequence'</span>, colors=[<span style="color: #8b2252;">'pink'</span>, <span style="color: #8b2252;">'blue'</span>])
plt.show()
</pre>
</div>
</div>
</div>
<div id="outline-container-orgf6fae5f" class="outline-4">
<h4 id="orgf6fae5f">Istogramma</h4>
<div class="outline-text-4" id="text-orgf6fae5f">
<div class="org-src-container">
<pre class="src src-python">series.hist(bins=50)
plt.show()
</pre>
</div>
<p>
oppure con diverse dimensioni dei bin
</p>
<div class="org-src-container">
<pre class="src src-python">heroes[<span style="color: #8b2252;">'Weight'</span>].hist(bins=np.hstack((np.arange(0, 200, 20),
                                      np.arange(200, 500, 50),
                                      np.arange(500, 1000, 100))))
</pre>
</div>
</div>
</div>
<div id="outline-container-org538f387" class="outline-4">
<h4 id="org538f387">Grafici multipli</h4>
<div class="outline-text-4" id="text-org538f387">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a0522d;">fig</span>, <span style="color: #a0522d;">axs</span> = plt.subplots(2)
axs[0].vlines(dim_freq.index, 0, dim_freq[<span style="color: #8b2252;">'Abs. frequence'</span>])
axs[0].set_title(<span style="color: #8b2252;">'bastoncini'</span>)
axs[1].plot(dim_freq)
axs[1].set_title(<span style="color: #8b2252;">'poligono'</span>)
fig.tight_layout()
</pre>
</div>
</div>
</div>
<div id="outline-container-org5750333" class="outline-4">
<h4 id="org5750333">Scatter plot</h4>
<div class="outline-text-4" id="text-org5750333">
<p>
Si stampa con
<code>heroes[heroes['Gender'] =</code> 'M'].plot.scatter('Height', 'Weight')=
</p>
</div>
</div>
<div id="outline-container-org4031975" class="outline-4">
<h4 id="org4031975">Quando usarli</h4>
<div class="outline-text-4" id="text-org4031975">
<dl class="org-dl">
<dt>frequenze</dt><dd>A barre non va bene, perchè essendo l'attributo di tipo numerico, si corre il rischio che ogni barra venga percepita come associata più a un intervallo di valori piuttosto che a un unico numero. Per evitare questo fraintendimento è più opportuno generare un grafico a bastoncini.</dd>
</dl>
</div>
</div>
</div>
</div>
<div id="outline-container-org88aaa01" class="outline-2">
<h2 id="org88aaa01">Probabilità</h2>
<div class="outline-text-2" id="text-org88aaa01">
<p>
Lo <b>spazio campionario (sample space)</b> è l'insieme dei possibili esiti e si indica con la lettera \(\Omega\).
</p>

<p>
Gli elementi \(\omega\) dello spazio campionario si chiamano <b>esiti o eventi elementari (outcomes)</b> e sono <b>concetti primitivi</b> (non definiti).
</p>

<p>
L'<b>evento (event)</b> è un sottoinsieme \(A\subseteq \Omega\). Gli eventi possibili sono anche \(\Omega\) stesso, quindi <b>evento certo</b>, e \(\emptyset\), ovvero <b>evento impossibile</b>.
</p>

<p>
Gli eventi sono insiemi e vi si possono applicare le tipiche operazioni insiemistiche, con le solite proprietà quali commutatività, associatività, distribuzione dell'unione sull'intersezione e viceversa e leggi di DeMorgan.
</p>

<p>
Una famiglia di eventi \(\mathcal{A}\) (ovvero un insieme di sottoinsiemi di \(\Omega\)) è un <b>algebra</b> secondo l'<b>algebra degli insiemi</b> se:
</p>
<ul class="org-ul">
<li>\(\Omega \in \mathcal{A}\), ovvero l'evento certo fa parte dell'algebra,</li>
<li>\(A \in \mathcal{A} \Rightarrow A^c  \in \mathcal{A}\), ovvero se un evento fa parte dell'algebra, deve farne parte anche il suo complemento,</li>
<li>\(A_i \in \mathcal{A} \; \forall i \in \mathcal{N} \Rightarrow \bigcup_{i = 1}^{\infty} {A_i } \in \mathcal{A}\), ovvero l'unione di ogni evento è presente nell'algebra.</li>
</ul>

<p>
Per estendere questo concetto agli insiemi infiniti, si introduce il concetto di \(\sigma\) -algebra
</p>

<p>
Se prendo in considerazione i singoletti di ogni evento allora l'insieme \(\mathcal{A}\) è l'<b>insieme delle parti</b> di \(\Omega\).
</p>

<p>
La teoria della probabilità di <b>Kolmogorov</b> si basa su questi tre assiomi:
</p>
<ol class="org-ol">
<li>\(P(E)\in\mathbb{R}, P(E) \geq 0 \qquad \forall E \in F\), ovvero la probabilità di un evento è un numero reale non negativo.</li>
<li>\(P(\Omega) = 1\), ovvero la probabilità dello spazio campione è \(1\).</li>
<li>La probabilità dell'unione di due eventi mutualmente esclusivi è la somma delle loro probabilità.</li>
</ol>

<p>
Da questi assiomi seguono le seguenti proposizioni (facilmente dimostrabili):
</p>
<ul class="org-ul">
<li>\(P(A \cup B) = P(A) + P(B) - P(A \cap B)\), ovvero la probabilità dell'unione di eventi non disgiunti è la somma delle probabilità singole meno la probabilità dell'intersezione</li>
<li>\[P\left(A^{c}\right) = P(\Omega\setminus A) = 1 - P(A)\]
ovvero la probabilità dell'evento complementare è il complemento a 1 della probabilità di un evento. Si dimostra così:
\[1 = P(S) = P(E \cup E^C) = P(E)+P(E^C)\]</li>
</ul>

<p>
Altri corollari sono che ogni probabilità è minore di \(1\) e che un evento sottoinsieme di un altro ha probabilità minore dell'altro. (Vedere eventualmente dimostrazione)
</p>

<p>
Per ottenere la probabilità dell'unione di più eventi, si può procedere in questo modo:
\[P(A_1 \cup A_2 \cup ... \cup A_n) = 1 - P((A_1 \cup A_2 \cup ... \cup A_n)^c) = 1-P(A_1^cA_2^c ...A_n^c)\]
</p>

<p>
Uno spazio è detto <b>equiprobabile</b> se ogni evento ha la stessa probabiltità di verificarsi.
</p>

<p>
La <b>probabilità condizionata</b> di un evento \(E\) dato che l'evento \(F\) è avvenuto è indicata con \(P(E|F)\). Si parla dei casi in cui sia \(E\) che \(F\) accadono, ma restringendo lo spazio campionario ai soli casi in cui \(F\) è accaduto, quindi:
\[P(E|F)=\frac{P(EF)}{P(F)}\]
</p>

<p>
Registra il concetto di "come cambia la certezza conoscendo un informazione parziale sull'esito dell'esperimento casuale".
</p>

<p>
Dato che un evento \(E\) può essere scritto come \(EF \cup EF^C\) e dato che dalla definizione di probabilità condizionata si ha che \(P(EF) = P(E|F)P(F)\), si ottiene la formula
\[P(E) = P(E|F)P(F) + P(E|F^C)[1-P(F)]\]
</p>

<p>
Questa formula mi permette di calcolare la probabilità di un evento \(E\) in base ad un altro evento \(F\), conoscendo la probabilità \(F\) e quella di \(E\) condizionato ad \(F\).
(Keep going from example 3.7.d included)
</p>

<p>
La formula può essere estesa ad \(n\) eventi \(F_i\) tali per cui essi sono tutti mutualmente esclusivi e \(\bigcup_{i=1}^n F_i = \Omega\). Dato che gli eventi sono mutualmente esclusivi fra loro, ottengo la formula
\[P(E) = \sum_{i=1}^n P(EF_i) = \sum_{i=1}^n P(E|F_i)P(F_i)\].
</p>

<p>
Questa formula mi dice che \(P(E)\) è la media degli \(P(E|F_i)\) pesata in base ad \(F_i\).
</p>

<p>
Ho un evento \(E\). Non ne conosco la probabilità. Conosco però la probabilità di un gruppo di \(n\) eventi disgiunti e "totali" \(F_i\) e so come l'esito di un certo esperimento influenza \(E\). Dato che un evento \(E\) posso scriverlo come \(\bigcup_{i=1}^n EF_i\), allora riesco a ricavare \(P(E)\) soltanto in funzione di \(P(F_i)\) e \(P(E|F_i)\).
</p>

<p>
La <b>formula di Bayes</b> è la seguente:
\[P(F_j|E) = \frac{P(F_jE)}{P(E)} = \frac{P(E|F_j)P(F_j)}{\sum_{i=1}^n P(E|F_i)P(F_i)}\]
si usa per incorporare la probabilità di un evento \(E\) (evidence) nell'evento \(F_j\) (hypotesis).
</p>

<p>
Ho un'insieme di ipotesi disgiunte \(F_i\) che unite coprono l'intero spazio delle probabilità. Voglio capire come l'indizio \(E\) influenzi una delle ipotesi. Allora considero la probabilità che l'indizio sia "corretto" per ogni ipotesi e lo peso in base alla probabilità che quell'ipotesi fosse vera in partenza (vedi aggiunta sotto). Prese tutte le ipotesi così aggiornate, l'ipotesi per cui mi sto interrogando va divisa per tutte le ipotesi aggiornate.
</p>

<p>
Dato \(P(H|E)\), ovvero quanto è probabile l'ipotesi considerando il dato nuovo, allora la quantità \(P(E|H)\) è come chiedersi quanto il nuovo dato "predica" l'ipotesi. Se \(P(E|H)\) è molto alto, allora c'è una forte "correlazione" fra gli eventi.
</p>

<p>
Due eventi sono detti <b>indipendenti</b> quando \(P(E|F) = P(E)\), in base alla definizione di probabilità condizionata. Questa definizione non è molto apprezzata perchè "tratta \(E\) ed \(F\) in modo diverso" (probabilmente si intende il fatto che la probabilità condizionata a sinistra non è commutativa).
</p>

<p>
Allora sostituisco alla probabilità condizionata la sua definizione e manipolando ottengo \(P(EF) = P(E)P(F)\), ovvero la probabilità dell'intersezione fattorizza come il prodotto dei singoli eventi. Questa seconda formula è chiaramente simmetrica e anche l'indipendenza è una relazione simmetrica.
</p>

<p>
L'indipendenza cattura il concetto che i due eventi non sono influenzati, ovvero il fatto che un evento sia accaduto non cambia la probabilità che l'altro accada. La probabilità di un evento \(E\) l'altro \(F\) è la stessa probabilità dell'evento \(E\) da solo, che è come dire che la proporzione dei casi in cui gli eventi \(E\) ed \(F\) avvengono entrambi rispetto ai casi in cui avviene \(F\) è la stessa proporzione con cui \(E\) avviene nella popolazione generale.
</p>

<p>
Il fatto che \(F\) sia accaduto o meno non cambia l'informazione riguardo \(E\). Infatti, se provo ad incorporare un evidenza indipendente nella mia ipotesi (caso d'uso della formula di Bayes), ottengo \(P(H|E) = \frac{P(EF)}{P(E)} = \frac{P(E)P(F)}{P(E)} = P(H)\)
</p>

<pre class="example" id="org9f9d3dd">
Immagina una popolazione in cui:
la metà sono uomini, P(U)=0.5
un decimo sono mancini, P(M)=0.1

Se gli eventi sono indipendenti, allora la probabilità di trovare un mancino fra gli uomini è P(M|U) = P(M), perchè un uomo può essere mancino tanto quanto la popolazione generale.

Allo stesso modo, ci si aspetta che la metà dei mancini sia uomo e quindi P(U|M)=P(U).

Quindi ci sono P(U) uomini fra i P(M) mancini, quindi il 50% dei mancini, che sono il 10%, per un totale di 5%.

Allo stesso modo c'è il 10% di mancini fra gli uomini, che sono il 50%, per un totale di 5%.

Si può considerare che data una persona fra 100, la probabilità che sia un uomo mancino è come pescare una palla da un urna di 100, dove 50 sono uomo e 50 donna, e poi pescare una palla da un'altra urna, sempre da 100, dove 10 sono mancino e 90 no. Esistono 50×10 casi in cui la persona sia un uomo mancino e 10000 casi in totale.
</pre>

<p>
Se \(E\) e \(F\) sono indipendenti, lo sono anche \(E\) e \(F^c\). La dimostrazione è una semplice manipolazione algebrica di \(P(E) = P(EF)+P(EF^c)\) (3 assioma) usando che \(P(EF) = P(E)P(F)\) per ipotesi.
</p>

<p>
Se \(E\) e \(F\) sono disgiunti e le probabilità di entrambi sono maggiori di \(0\), allora non sono indipendenti. L'intuizione è che se gli eventi sono disgiunti e gli eventi sono entrambi maggiori di \(0\), allora \(F^c\) è un sovrainsieme di \(E\) e quindi \(P(E|F^c)\) è necessariamente minore di $P(E)
</p>

<p>
Più di due eventi \(E\) &isin; \(I\) sono indipendenti gli eventi di ogni sottoinsieme di \(I\) sono indipendenti fra loro.
</p>

<p>
<b><b>Sistemi in serie</b></b>
</p>

<ul class="org-ul">
<li>\[ P(\text{funziona}) = P(A_1) P(A_2) \dots P(A_n) \]</li>

<li>\[ P(\text{non funziona}) = 1 - P(A_1) P(A_2) \dots P(A_n) \]</li>
</ul>

<p>
&#x2014;
</p>

<p>
<b><b>Sistemi in parallelo</b></b>
</p>

<ul class="org-ul">
<li>\[ P(\text{funziona}) = 1 - P(A_1^c) P(A_2^c) \dots P(A_n^c) \]</li>

<li>\[ P(\text{non funziona}) = P(A_1^c) P(A_2^c) \dots P(A_n^c) \]</li>
</ul>
</div>
</div>
<div id="outline-container-org9eea659" class="outline-2">
<h2 id="org9eea659">Variabili aleatorie</h2>
<div class="outline-text-2" id="text-org9eea659">
<p>
Le <b>variabili aleatorie</b> si usano per poter codificare degli esiti di un esperimento aleatorio in termini numerici, permettendo più operazioni matematiche sui risultati.
</p>

<p>
Una variabile aleatoria si "costruisce" associando ad ogni esito \(\omega \in \Omega\) un valore mediante la funzione \(X:\Omega\rightarrow\mathbb{R}\) e poi ragionando sui valori assunti dalla variabile e non più sugli esiti dell'esperimento.
</p>

<p>
I valori assunti dalla variabile (specificazioni) sono definiti in base agli esiti dell'esperimento su cui è definita:
\[\{ X=\alpha \} = \{\omega\in\Omega:X(\omega)=\alpha\}\]
ovvero, la variabile assume il valore \(\alpha\) quando l'immagine dell'esito \(\omega\) è \(\alpha\).
</p>
</div>
<div id="outline-container-org0712b1e" class="outline-3">
<h3 id="org0712b1e">Funzione di ripartizione e funzioni di probabilità</h3>
<div class="outline-text-3" id="text-org0712b1e">
</div>
<div id="outline-container-orga1d27bb" class="outline-4">
<h4 id="orga1d27bb">Concetti fondamentali per variabili aleatorie</h4>
<div class="outline-text-4" id="text-orga1d27bb">
<p>
<b>Definizione (Funzione di ripartizione):</b> La funzione di ripartizione di una variabile aleatoria \(X\), indicata con \(F_X(x)\), è definita come:
\[F_X(x) = P(X \leq x)\]
</p>

<p>
La funzione di ripartizione \(F_X(x)\) possiede le seguenti proprietà universali:
</p>
<ol class="org-ol">
<li>È sempre \(F_X(x) \geq 0\) (la probabilità non è mai negativa)</li>
<li>\(\lim_{x \to -\infty} F_X(x) = 0\) e \(\lim_{x \to \infty} F_X(x) = 1\)</li>
<li>È continua da destra: \(\lim_{h \to 0^+} F_X(x+h) = F_X(x)\)</li>
<li>È non decrescente: se \(a < b\), allora \(F_X(a) \leq F_X(b)\)</li>
</ol>

<p>
Per qualsiasi tipo di variabile aleatoria, vale il teorema fondamentale:
\[P(a < X \leq b) = F_X(b) - F_X(a)\]
</p>

<p>
<b>Dimostrazione:</b>
</p>
\begin{align}
F_X(b) - F_X(a) &= P(X \leq b) - P(X \leq a)\\
&= P((X \leq a) \cup (a < X \leq b)) - P(X \leq a)\\
&= P(X \leq a) + P(a < X \leq b) - P(X \leq a)\\
&= P(a < X \leq b)
\end{align}
</div>
</div>
<div id="outline-container-org8b3219b" class="outline-4">
<h4 id="org8b3219b">Variabili aleatorie discrete</h4>
<div class="outline-text-4" id="text-org8b3219b">
<p>
<b>Definizione (Funzione di massa di probabilità):</b> Per una variabile aleatoria discreta \(X\), la funzione di massa di probabilità, indicata con \(p_X(x)\), è definita come:
\[p_X(x) = P(X = x)\]
</p>

<p>
<b>Interpretazione intuitiva:</b> La PMF assegna a ciascun valore possibile della variabile aleatoria la sua probabilità di occorrenza.
</p>

<p>
<b>Proprietà della PMF:</b>
</p>
<ol class="org-ol">
<li>\(p_X(x) \geq 0\) per ogni \(x \in \mathbb{R}\)</li>
<li>\(\sum_{x \in \text{Supp}(X)} p_X(x) = 1\), dove \(\text{Supp}(X)\) è il supporto di \(X\)</li>
<li>\(p_X(x) > 0\) se e solo se \(x\) appartiene al supporto di \(X\)</li>
</ol>

<p>
<b>Relazione con la funzione di ripartizione:</b> Per variabili discrete, la CDF è una funzione a gradini con salti nei punti del supporto:
\[F_X(x) = \sum_{t \leq x} p_X(t)\]
</p>

<p>
La funzione di massa può essere ricavata dalla funzione di ripartizione come:
\[p_X(x) = F_X(x) - \lim_{y \to x^-} F_X(y)\]
</p>

<p>
che rappresenta l'ampiezza del "salto" della funzione di ripartizione nel punto \(x\).
</p>
</div>
</div>
<div id="outline-container-orgfe1ac5a" class="outline-4">
<h4 id="orgfe1ac5a">Variabili aleatorie continue</h4>
<div class="outline-text-4" id="text-orgfe1ac5a">
<p>
<b>Definizione (Funzione di densità di probabilità):</b> Per una variabile aleatoria continua \(X\), la funzione di densità di probabilità, indicata con \(f_X(x)\), è definita tale che per ogni insieme misurabile \(B \subset \mathbb{R}\):
\[P(X \in B) = \int_B f_X(x) \, dx\]
</p>

<p>
<b>Interpretazione intuitiva:</b> Mentre la PMF fornisce direttamente probabilità, la PDF rappresenta la "concentrazione di probabilità" intorno a ciascun punto. La probabilità in un intervallo infinitesimale \([x, x+dx]\) è approssimativamente \(f_X(x) \, dx\).
</p>

<p>
<b>Proprietà della PDF:</b>
</p>
<ol class="org-ol">
<li>\(f_X(x) \geq 0\) per ogni \(x \in \mathbb{R}\)</li>
<li>\(\int_{-\infty}^{+\infty} f_X(x) \, dx = 1\)</li>
<li>\(P(X = c) = 0\) per qualsiasi singolo valore \(c\)</li>
</ol>

<p>
<b>Relazione con la funzione di ripartizione:</b> Per variabili continue, la CDF e la PDF sono legate dalle seguenti relazioni:
\[F_X(x) = \int_{-\infty}^{x} f_X(t) \, dt\]
\[f_X(x) = \frac{d}{dx}F_X(x)\]
</p>

<p>
dove la seconda equazione è valida nei punti in cui \(F_X\) è differenziabile.
</p>

<p>
<b>Calcolo di probabilità:</b> La probabilità che \(X\) appartenga a un intervallo \([a,b]\) si calcola mediante:
\[P(a \leq X \leq b) = \int_a^b f_X(x) \, dx = F_X(b) - F_X(a)\]
</p>

<p>
Poiché \(P(X = a) = P(X = b) = 0\) per variabili continue, abbiamo anche:
\[P(a < X < b) = P(a \leq X \leq b) = P(a < X \leq b) = P(a \leq X < b) = F_X(b) - F_X(a)\]
</p>
</div>
</div>
<div id="outline-container-orgb058a68" class="outline-4">
<h4 id="orgb058a68">Confronto e differenze principali</h4>
<div class="outline-text-4" id="text-orgb058a68">
<p>
<b>Differenze chiave tra PMF e PDF:</b>
</p>

<ol class="org-ol">
<li><b>Interpretazione dei valori:</b>
<ul class="org-ul">
<li>PMF: \(p_X(x)\) rappresenta direttamente \(P(X = x)\)</li>
<li>PDF: \(f_X(x)\) non rappresenta una probabilità ma una densità; \(f_X(x)\) può assumere valori maggiori di 1</li>
</ul></li>

<li><b>Calcolo delle probabilità:</b>
<ul class="org-ul">
<li>PMF: \(P(X \in A) = \sum_{x \in A} p_X(x)\)</li>
<li>PDF: \(P(X \in A) = \int_A f_X(x) \, dx\)</li>
</ul></li>

<li><b>Probabilità di punti singoli:</b>
<ul class="org-ul">
<li>PMF: \(P(X = x)\) può essere positiva</li>
<li>PDF: \(P(X = x) = 0\) per ogni \(x\)</li>
</ul></li>

<li><b>Natura della funzione di ripartizione:</b>
<ul class="org-ul">
<li>Per variabili discrete: \(F_X\) è una funzione a gradini, discontinua nei punti del supporto</li>
<li>Per variabili continue: \(F_X\) è una funzione continua (e differenziabile quasi ovunque)</li>
</ul></li>
</ol>
</div>
</div>
</div>
<div id="outline-container-org30aaf5e" class="outline-3">
<h3 id="org30aaf5e">Funzione indicatrice</h3>
<div class="outline-text-3" id="text-org30aaf5e">
<p>
Una <b>funzione indicatrice</b> di un evento A, indicata con \(I_A(x)\), è una funzione che restituisce \(1\) se è avvenuto l'evento e \(0\) altrimenti:
\[I_A(x) = \begin{cases}
1 & \text{se } x \in A \\
0 & \text{se } x \notin A
\end{cases}\]
</p>

<p>
Le funzioni indicatrici sono utili per rappresentare eventi in forma matematica e per calcolare probabilità attraverso il valore atteso.
</p>
</div>
</div>
<div id="outline-container-org31227f4" class="outline-3">
<h3 id="org31227f4">Valore atteso e varianza</h3>
<div class="outline-text-3" id="text-org31227f4">
<p>
<b>Definizione (Valore atteso):</b> Per variabili aleatorie discrete, il valore atteso (o media), indicato con \(E[X]\), è definito come:
\[E[X] = \sum_x x \cdot P(X = x) = \sum_x x \cdot p_X(x)\]
</p>

<p>
ovvero la media ponderata in cui i pesi sono le probabilità.
</p>

<p>
Quando si tratta di insiemi enumerabili infiniti, è necessario verificare la convergenza della serie. Nei casi che tratteremo, assumeremo sempre che la serie converga.
</p>

<p>
Il valore atteso di una funzione indicatrice è:
\[E[I_A] = 1 \cdot P(I_A = 1) + 0 \cdot P(I_A = 0) = P(A)\]
</p>

<p>
<b>Proprietà (Area e valore atteso):</b> Per variabili aleatorie non negative, il valore atteso può essere interpretato geometricamente come l'area sotto la curva della funzione di sopravvivenza \(1-F_X(x)\):
\[E[X] = \int_{0}^{+\infty} (1-F_X(x)) \, dx\]
</p>

<p>
Per variabili aleatorie discrete positive, questa proprietà si può verificare dividendo l'area in rettangoli di altezza \(P(X > k)\) e larghezza 1, ottenendo:
\[E[X] = \sum_{k=0}^{\infty} P(X > k) = \sum_{k=1}^{\infty} k \cdot P(X = k)\]
</p>

<p>
<b>Definizione (Varianza):</b> La varianza, indicata con \(\text{Var}(X)\), si calcola con:
\[\text{Var}(X) = E[(X - E[X])^2]\]
</p>

<p>
che può essere espanso per ottenere una formula più pratica:
\[\text{Var}(X) = E[X^2] - (E[X])^2\]
</p>

<p>
Da questo consegue che la varianza di una variabile aleatoria che descrive \(n\) valori equiprobabili da 1 a \(n\) ha la forma:
\[\text{Var}(X) = \frac{n^2-1}{12}\]
</p>

<p>
La varianza della funzione indicatrice, considerando che questa è idempotente (ovvero \(I_A(x)^2 = I_A(x)\)), è:
\[\text{Var}(I_A) = E[I_A] - (E[I_A])^2 = P(A) - P(A)^2 = P(A) \cdot (1 - P(A)) = P(A) \cdot P(A^c)\]
</p>

<p>
<b>Definizione (Varianza):</b> La varianza, indicata con \(\text{Var}(X)\), si calcola con:
\[\text{Var}(X) = E[(X - E[X])^2]\]
</p>

<p>
che può essere espanso per ottenere una formula più pratica:
\[\text{Var}(X) = E[X^2] - (E[X])^2\]
</p>

<p>
Da questo consegue che la varianza di una variabile aleatoria che descrive \(n\) valori equiprobabili da 1 a \(n\) ha la forma:
\[\text{Var}(X) = \frac{n^2-1}{12}\]
</p>

<p>
La varianza della funzione indicatrice, considerando che questa è idempotente (ovvero \(I_A(x)^2 = I_A(x)\)), è:
\[\text{Var}(I_A) = E[I_A] - (E[I_A])^2 = P(A) - P(A)^2 = P(A) \cdot (1 - P(A)) = P(A) \cdot P(A^c)\]
</p>

<p>
## Varianza di somme di variabili aleatorie non necessariamente indipendenti
</p>

<p>
<b>Definizione (Varianza di una somma):</b> Per variabili aleatorie \(X\) e \(Y\) non necessariamente indipendenti, la varianza della loro somma è data da:
\[\text{Var}(X+Y) = \text{Var}(X) + \text{Var}(Y) + 2\text{Cov}(X,Y)\]
</p>

<p>
dove \(\text{Cov}(X,Y)\) rappresenta la covarianza tra \(X\) e \(Y\).
</p>

<p>
<b>Dimostrazione:</b>
Partendo dalla definizione di varianza e usando la linearità del valore atteso:
</p>
\begin{align}
\text{Var}(X+Y) &= E[(X+Y - E[X+Y])^2] \\
&= E[(X+Y - E[X] - E[Y])^2] \\
&= E[(X-E[X])^2 + 2(X-E[X])(Y-E[Y]) + (Y-E[Y])^2] \\
&= E[(X-E[X])^2] + 2E[(X-E[X])(Y-E[Y])] + E[(Y-E[Y])^2] \\
&= \text{Var}(X) + 2\text{Cov}(X,Y) + \text{Var}(Y)
\end{align}

<p>
<b>Generalizzazione:</b> Per una somma di \(n\) variabili aleatorie, la formula si estende a:
\[\text{Var}\left(\sum_{i=1}^n X_i\right) = \sum_{i=1}^n \text{Var}(X_i) + 2\sum_{i<j} \text{Cov}(X_i, X_j)\]
</p>

<p>
<b>Osservazione:</b> Quando le variabili sono indipendenti, \(\text{Cov}(X_i, X_j) = 0\) per \(i \neq j\), e la formula si semplifica nella ben nota additività delle varianze:
\[\text{Var}\left(\sum_{i=1}^n X_i\right) = \sum_{i=1}^n \text{Var}(X_i)\]
</p>

<p>
Questo evidenzia come la presenza di covarianza non nulla possa aumentare o diminuire la varianza totale, a seconda del segno della covarianza stessa.
</p>
</div>
</div>
<div id="outline-container-orgc7e5d71" class="outline-3">
<h3 id="orgc7e5d71">Variabili aleatorie multivariate</h3>
<div class="outline-text-3" id="text-orgc7e5d71">
<p>
<b>Definizione (Funzione di ripartizione congiunta):</b> La funzione di ripartizione congiunta di due variabili aleatorie X e Y, indicata con \(F_{X,Y}(x,y)\), è definita come:
\[F_{X,Y}(x,y) = P(X \leq x, Y \leq y)\]
</p>

<p>
Quando consideriamo \(\lim_{y \to \infty} F_{X,Y}(x,y)\) otteniamo \(F_X(x)\), che viene chiamata <b>funzione di ripartizione marginale</b>.
</p>
</div>
<div id="outline-container-orgfc3764d" class="outline-4">
<h4 id="orgfc3764d">Massa di probabilità congiunta e marginale</h4>
<div class="outline-text-4" id="text-orgfc3764d">
<p>
<b>Definizione (Massa di probabilità congiunta):</b> La funzione di massa di probabilità congiunta di due variabili aleatorie discrete \(X\) e \(Y\), indicata con \(p_{X,Y}(x,y)\), è definita come:
\[p_{X,Y}(x,y) = P(X = x, Y = y)\]
</p>

<p>
<b>Definizione (Massa di probabilità marginale):</b> La funzione di massa di probabilità marginale di \(X\), indicata con \(p_X(x)\), si ottiene sommando la massa di probabilità congiunta su tutti i possibili valori di \(Y\):
\[p_X(x) = \sum_y p_{X,Y}(x,y)\]
</p>

<p>
Analogamente, per la variabile aleatoria \(Y\):
\[p_Y(y) = \sum_x p_{X,Y}(x,y)\]
</p>

<p>
La massa di probabilità marginale si ottiene considerando tutte le associazioni della prima variabile con la seconda, fissando la prima e poi sommandole, poiché gli eventi \(\{X=x, Y=y\}\) per valori diversi di \(y\) sono disgiunti.
</p>
</div>
</div>
<div id="outline-container-orga8e1458" class="outline-4">
<h4 id="orga8e1458">Indipendenza tra variabili aleatorie</h4>
<div class="outline-text-4" id="text-orga8e1458">
<p>
<b>Definizione (Indipendenza di variabili aleatorie):</b> Due variabili aleatorie \(X\) e \(Y\) si dicono indipendenti se ogni evento della specificazione dell'una è indipendente da ogni evento dell'altra. 
</p>

<p>
Formalmente, \(X\) e \(Y\) sono indipendenti se e solo se:
</p>

<ol class="org-ol">
<li>Per la funzione di ripartizione congiunta:
\[F_{X,Y}(x,y) = F_X(x) \cdot F_Y(y) \quad \text{per ogni } x, y \in \mathbb{R}\]</li>

<li>Per la funzione di massa di probabilità congiunta (nel caso discreto):
\[p_{X,Y}(x,y) = p_X(x) \cdot p_Y(y) \quad \text{per ogni } x, y\]</li>
</ol>
</div>
</div>
<div id="outline-container-org3f57426" class="outline-4">
<h4 id="org3f57426">Valore atteso di una somma di variabili aleatorie</h4>
<div class="outline-text-4" id="text-org3f57426">
<p>
Considerando un vettore multivariato, ovvero un insieme di variabili aleatorie \(X_1, X_2, \ldots, X_n\), possiamo definire una nuova variabile aleatoria \(S = X_1 + X_2 + \ldots + X_n\).
</p>

<p>
Il valore atteso di \(S\) può essere calcolato come:
\[E[S] = E[X_1 + X_2 + \ldots + X_n]\]
</p>

<p>
Dalla definizione di valore atteso:
\[E[S] = \sum_{x_1} \sum_{x_2} \ldots \sum_{x_n} (x_1 + x_2 + \ldots + x_n) \cdot p_{X_1,X_2,\ldots,X_n}(x_1,x_2,\ldots,x_n)\]
</p>

<p>
Riarrangiando i termini:
\[E[S] = \sum_{x_1} x_1 \sum_{x_2} \ldots \sum_{x_n} p_{X_1,X_2,\ldots,X_n}(x_1,x_2,\ldots,x_n) + \ldots\]
</p>

<p>
Dove le sommatorie interne corrispondono alle probabilità marginali di ciascuna variabile. Quindi:
\[E[S] = \sum_{x_1} x_1 \cdot p_{X_1}(x_1) + \sum_{x_2} x_2 \cdot p_{X_2}(x_2) + \ldots + \sum_{x_n} x_n \cdot p_{X_n}(x_n)\]
</p>

<p>
\[E[S] = E[X_1] + E[X_2] + \ldots + E[X_n]\]
</p>

<p>
Questo dimostra la linearità del valore atteso: \(E[X+Y] = E[X] + E[Y]\).
</p>
</div>
</div>
<div id="outline-container-org18e3960" class="outline-4">
<h4 id="org18e3960">Valore atteso del prodotto di variabili aleatorie</h4>
<div class="outline-text-4" id="text-org18e3960">
<p>
Per definizione di valore atteso:
\[E(XY)=\sum_i \sum_j x_i \cdot y_j \cdot P(X=x_i,Y=y_j)\]
</p>

<p>
Per definizione di indipendenza, vale che:
\[P(X=x_i,Y=y_j) = P(X=x_i) \cdot P(Y=y_j)\]
</p>

<p>
Quindi otteniamo:
\[E(XY)=\sum_i \sum_j x_i \cdot y_j \cdot P(X=x_i) \cdot P(Y=y_j)\]
</p>

<p>
Che è separabile in:
\[E(XY) = \left(\sum_i x_i \cdot P(X=x_i)\right) \cdot \left(\sum_j y_j \cdot P(Y=y_j)\right) = E(X) \cdot E(Y)\]
</p>

<p>
Da questo deriva che se \(X\) e \(Y\) sono indipendenti, allora \(E(XY) = E(X) \cdot E(Y)\).
</p>
</div>
</div>
<div id="outline-container-org573876c" class="outline-4">
<h4 id="org573876c">Stima di una variabile aleatoria</h4>
<div class="outline-text-4" id="text-org573876c">
<p>
Se vogliamo stimare il valore di una variabile aleatoria \(X\) usando un valore costante \(c\), possiamo calcolare il valore atteso dell'errore quadratico:
\[E[(X-c)^2]\]
</p>

<p>
Riscrivendo questo, aggiungendo e sottraendo il valore atteso di \(X\), indicato con \(\mu\):
\[E[(X-c)^2] = E[(X-\mu+\mu-c)^2]\]
</p>

<p>
Espandendo il binomio:
\[E[(X-c)^2] = E[(X-\mu)^2 + 2(X-\mu)(\mu-c) + (\mu-c)^2]\]
</p>

<p>
\[E[(X-c)^2] = E[(X-\mu)^2] + 2(\mu-c)E[X-\mu] + (\mu-c)^2\]
</p>

<p>
Poiché \(E[X-\mu] = E[X] - \mu = 0\):
\[E[(X-c)^2] = E[(X-\mu)^2] + (\mu-c)^2\]
</p>

<p>
\[E[(X-c)^2] = \text{Var}(X) + (\mu-c)^2\]
</p>

<p>
Questo è sempre maggiore o uguale alla varianza di \(X\), poiché \((\mu-c)^2 \geq 0\). L'errore minimo si ottiene quando \(c = \mu = E[X]\), e in quel caso l'errore quadratico medio è esattamente uguale alla varianza.
</p>
</div>
</div>
<div id="outline-container-org39d0753" class="outline-4">
<h4 id="org39d0753">Covarianza: definizione e proprietà</h4>
<div class="outline-text-4" id="text-org39d0753">
<p>
<b>Definizione (Covarianza):</b> La covarianza tra due variabili aleatorie \(X\) e \(Y\), indicata con \(\text{Cov}(X,Y)\), è definita come:
\[\text{Cov}(X,Y) = E[(X-\mu_X)(Y-\mu_Y)]\]
dove \(\mu_X = E[X]\) e \(\mu_Y = E[Y]\).
</p>

<p>
Usando la linearità del valore atteso, si può dimostrare che:
\[\text{Cov}(X,Y) = E[XY] - E[X] \cdot E[Y]\]
</p>

<p>
<b>Proprietà della covarianza:</b>
</p>

<ol class="org-ol">
<li>\(\text{Cov}(aX,Y) = a \cdot \text{Cov}(X,Y)\) per ogni costante \(a\)</li>
<li>\(\text{Cov}(X+Y,Z) = \text{Cov}(X,Z) + \text{Cov}(Y,Z)\)</li>
<li>Più in generale: \(\text{Cov}\left(\sum_{i} X_i, \sum_{j} Y_j\right) = \sum_{i} \sum_{j} \text{Cov}(X_i, Y_j)\)</li>
<li>\(\text{Cov}(X,X) = \text{Var}(X)\)</li>
</ol>
</div>
</div>
<div id="outline-container-org6549c85" class="outline-4">
<h4 id="org6549c85">Varianza di somme di variabili aleatorie</h4>
<div class="outline-text-4" id="text-org6549c85">
<p>
Per la varianza di una somma di variabili aleatorie \(X\) e \(Y\), vale:
\[\text{Var}(X+Y) = E[(X+Y)^2] - (E[X+Y])^2\]
</p>

<p>
Usando la linearità del valore atteso e sviluppando i quadrati:
\[\text{Var}(X+Y) = E[X^2 + 2XY + Y^2] - (E[X] + E[Y])^2\]
\[\text{Var}(X+Y) = E[X^2] + 2E[XY] + E[Y^2] - E[X]^2 - 2E[X]E[Y] - E[Y]^2\]
\[\text{Var}(X+Y) = \text{Var}(X) + \text{Var}(Y) + 2(E[XY] - E[X]E[Y])\]
\[\text{Var}(X+Y) = \text{Var}(X) + \text{Var}(Y) + 2\text{Cov}(X,Y)\]
</p>

<p>
In generale, per una somma di \(n\) variabili aleatorie:
\[\text{Var}\left(\sum_{i=1}^n X_i\right) = \sum_{i=1}^n \text{Var}(X_i) + 2\sum_{i<j} \text{Cov}(X_i, X_j)\]
</p>

<p>
Quando \(X\) e \(Y\) sono indipendenti, la loro covarianza è 0 e quindi:
\[\text{Var}(X+Y) = \text{Var}(X) + \text{Var}(Y)\]
</p>

<p>
Quindi la varianza è additiva solo quando le variabili sono indipendenti.
</p>
</div>
</div>
<div id="outline-container-org92f0dee" class="outline-4">
<h4 id="org92f0dee">Covarianza di funzioni indicatrici</h4>
<div class="outline-text-4" id="text-org92f0dee">
<p>
Se consideriamo due variabili aleatorie uguali alle funzioni indicatrici di due insiemi, quindi \(X=I_A\) e \(Y=I_B\), allora:
</p>

<p>
\[E(X) = P(X=1) = P(A)\]
\[E(Y) = P(Y=1) = P(B)\]
\[E(XY) = P(XY=1) = P(X=1, Y=1) = P(A \cap B)\]
</p>

<p>
La covarianza diventa:
\[\text{Cov}(X,Y) = E(XY) - E(X)E(Y) = P(A \cap B) - P(A)P(B)\]
</p>

<p>
Osservazione: \(\text{Cov}(X,Y) > 0\) quando \(P(A \cap B) > P(A)P(B)\), ovvero quando:
\[\frac{P(A \cap B)}{P(B)} > P(A)\]
</p>

<p>
Notando che \(\frac{P(A \cap B)}{P(B)} = P(A|B)\), la covarianza è positiva quando \(P(A|B) > P(A)\), cioè quando sapere che \(B\) è vero aumenta la probabilità che \(A\) sia vero. Questo indica che gli eventi sono positivamente correlati.
</p>
</div>
</div>
<div id="outline-container-orgebfc0f0" class="outline-4">
<h4 id="orgebfc0f0">Coefficiente di correlazione</h4>
<div class="outline-text-4" id="text-orgebfc0f0">
<p>
Poiché il valore della covarianza dipende fortemente dalle unità di misura delle variabili, viene introdotto il coefficiente di correlazione che "normalizza" la covarianza:
</p>

<p>
\[\rho_{X,Y} = \frac{\text{Cov}(X,Y)}{\sqrt{\text{Var}(X) \cdot \text{Var}(Y)}} = \frac{\text{Cov}(X,Y)}{\sigma_X \cdot \sigma_Y}\]
</p>

<p>
Il coefficiente di correlazione \(\rho_{X,Y}\) è sempre compreso tra -1 e 1:
</p>
<ul class="org-ul">
<li>\(\rho_{X,Y} = 1\): correlazione positiva perfetta</li>
<li>\(\rho_{X,Y} = -1\): correlazione negativa perfetta</li>
<li>\(\rho_{X,Y} = 0\): nessuna correlazione lineare</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org98ebc0b" class="outline-3">
<h3 id="org98ebc0b">Disuguaglianza di Markov</h3>
<div class="outline-text-3" id="text-org98ebc0b">
<p>
<b>Teorema (Disuguaglianza di Markov):</b> Se \(X\) è una variabile aleatoria non negativa (cioè \(X \geq 0\)), allora per ogni costante \(a > 0\):
\[P(X \geq a) \leq \frac{E[X]}{a}\]
</p>

<p>
<b>Dimostrazione:</b> 
Partiamo dalla definizione del valore atteso:
\[E[X] = \int_{0}^{\infty} x \cdot f_X(x) \, dx\]
</p>

<p>
Possiamo dividere l'integrale in due parti:
\[E[X] = \int_{0}^{a} x \cdot f_X(x) \, dx + \int_{a}^{\infty} x \cdot f_X(x) \, dx\]
</p>

<p>
Dato che \(X\) è non negativa, entrambi gli addendi sono non negativi, quindi:
\[E[X] \geq \int_{a}^{\infty} x \cdot f_X(x) \, dx\]
</p>

<p>
Nell'intervallo \([a, \infty)\), ogni valore di \(x\) è maggiore o uguale ad \(a\), quindi:
\[\int_{a}^{\infty} x \cdot f_X(x) \, dx \geq \int_{a}^{\infty} a \cdot f_X(x) \, dx = a \cdot \int_{a}^{\infty} f_X(x) \, dx = a \cdot P(X \geq a)\]
</p>

<p>
Combinando queste disuguaglianze:
\[E[X] \geq a \cdot P(X \geq a)\]
</p>

<p>
Da cui:
\[P(X \geq a) \leq \frac{E[X]}{a}\]
</p>

<p>
<b>Interpretazione:</b> La disuguaglianza di Markov fornisce un limite superiore alla probabilità che una variabile aleatoria non negativa superi un certo valore. Ad esempio, per una variabile con valore atteso 10, la probabilità che superi 100 è al massimo 10/100 = 0.1 o 10%.
</p>
</div>
</div>
<div id="outline-container-org16eba75" class="outline-3">
<h3 id="org16eba75">Disuguaglianza di Chebyshev</h3>
<div class="outline-text-3" id="text-org16eba75">
<p>
<b>Teorema (Disuguaglianza di Chebyshev):</b> Data una variabile aleatoria \(X\) con valore atteso \(E[X] = \mu\) e varianza \(Var(X) = \sigma^2\), allora per ogni \(r > 0\):
\[P(|X-\mu| \geq r) \leq \frac{\sigma^2}{r^2}\]
</p>

<p>
<b>Dimostrazione:</b>
Osserviamo che:
\[|X-\mu| \geq r \Rightarrow (X-\mu)^2 \geq r^2\]
</p>

<p>
Poiché \(r > 0\), vale anche l'implicazione contraria, quindi:
\[P(|X-\mu| \geq r) = P((X-\mu)^2 \geq r^2)\]
</p>

<p>
Definiamo \(Y := (X-\mu)^2\). Notiamo che \(Y\) è una variabile aleatoria non negativa con \(E[Y] = Var(X) = \sigma^2\). 
</p>

<p>
Applicando la disuguaglianza di Markov a \(Y\):
\[P(Y \geq r^2) \leq \frac{E[Y]}{r^2} = \frac{Var(X)}{r^2} = \frac{\sigma^2}{r^2}\]
</p>

<p>
Sostituendo:
\[P(|X-\mu| \geq r) \leq \frac{\sigma^2}{r^2}\]
</p>

<p>
<b>Caso particolare:</b> Se consideriamo \(r = k\sigma\) (cioè misuriamo la distanza in termini di deviazioni standard), otteniamo:
\[P(|X-\mu| \geq k\sigma) \leq \frac{1}{k^2}\]
</p>

<p>
<b>Interpretazione:</b> La disuguaglianza di Chebyshev fornisce un limite superiore alla probabilità che una variabile aleatoria si discosti dal suo valore atteso di una certa quantità. Ad esempio:
</p>
<ul class="org-ul">
<li>La probabilità di deviare di almeno 2 deviazioni standard è al massimo 1/4 = 25%</li>
<li>La probabilità di deviare di almeno 3 deviazioni standard è al massimo 1/9 ≈ 11.1%</li>
<li>La probabilità di deviare di almeno 4 deviazioni standard è al massimo 1/16 = 6.25%</li>
</ul>

<p>
Questo ci fornisce un nuovo modo di interpretare la varianza: non solo come misura di dispersione, ma anche come unità di misura per la distanza dal valore atteso. La probabilità che un valore si discosti di \(k\) deviazioni standard dal valore atteso decresce almeno proporzionalmente a \(1/k^2\).
</p>
</div>
</div>
<div id="outline-container-org88f7a28" class="outline-3">
<h3 id="org88f7a28">Legge dei grandi numeri</h3>
<div class="outline-text-3" id="text-org88f7a28">
<p>
<b>Teorema (Legge debole dei grandi numeri):</b> Sia \(X_1, X_2, ..., X_n\) una sequenza di variabili aleatorie indipendenti e identicamente distribuite, ciascuna con valore atteso \(\mu\) e varianza \(\sigma^2\). Definiamo la media campionaria \(\overline{X}_n = \frac{1}{n}\sum_{i=1}^n X_i\). Allora, per ogni \(\varepsilon > 0\):
</p>

<p>
\[\lim_{n \to \infty} P(|\overline{X}_n - \mu| < \varepsilon) = 1\]
</p>

<p>
Ovvero, la media campionaria converge in probabilità al valore atteso \(\mu\).
</p>

<p>
<b>Dimostrazione (usando Chebyshev):</b>
Poiché le variabili sono indipendenti e identicamente distribuite:
</p>
<ul class="org-ul">
<li>\(E[\overline{X}_n] = \mu\)</li>
<li>\(Var(\overline{X}_n) = \frac{\sigma^2}{n}\)</li>
</ul>

<p>
Applicando la disuguaglianza di Chebyshev:
\[P(|\overline{X}_n - \mu| \geq \varepsilon) \leq \frac{Var(\overline{X}_n)}{\varepsilon^2} = \frac{\sigma^2}{n\varepsilon^2}\]
</p>

<p>
Quando \(n \to \infty\), questo limite tende a 0, quindi:
\[\lim_{n \to \infty} P(|\overline{X}_n - \mu| \geq \varepsilon) = 0\]
</p>

<p>
Quindi:
\[\lim_{n \to \infty} P(|\overline{X}_n - \mu| < \varepsilon) = 1\]
</p>

<p>
<b>Interpretazione:</b> La legge dei grandi numeri afferma che, con un numero sufficientemente grande di osservazioni, la media campionaria di variabili aleatorie indipendenti e identicamente distribuite si avvicina al loro valore atteso. Ad esempio, lanciando una moneta equilibrata molte volte, la proporzione di teste si avvicinerà a 0.5.
</p>
</div>
</div>
<div id="outline-container-org93cdc7f" class="outline-3">
<h3 id="org93cdc7f">Legge dei grandi numeri</h3>
<div class="outline-text-3" id="text-org93cdc7f">
<p>
<b>versione forte</b>: $P(lim<sub>n&rarr; +&infin;</sub> \overline{X}<sub>n</sub> = &mu;) = 1, ovvero all'aumentare dei campioni considerati, la probabilità che la media campionaria come stimatore raggiunga <b>esattamente</b> il valore atteso della variabile da stimare si avvicina a \(1\)
</p>

<p>
<b>versione debole</b>: \(\lim_{n\to +\infty} P(|\overline{X}_n -\mu|>\epsilon) = 0\), ovvero fissato un \(\epsilon\) piccolo, l'errore da parte della media campionaria rispetto al valore atteso da stimare diventerà minore di \(\epsilon\) prima o poi
</p>
</div>
</div>
<div id="outline-container-org74800df" class="outline-3">
<h3 id="org74800df">Modelli di variabili aleatorie</h3>
<div class="outline-text-3" id="text-org74800df">
</div>
<div id="outline-container-org57a32f3" class="outline-4">
<h4 id="org57a32f3">Modello di Bernoulli</h4>
<div class="outline-text-4" id="text-org57a32f3">
<p>
<b>Definizione (Distribuzione di Bernoulli):</b> Una variabile aleatoria \(X\) segue una distribuzione di Bernoulli con parametro \(p\) (scriviamo \(X \sim \text{Bern}(p)\)) se rappresenta un esperimento con esito binario, dove:
</p>
<ul class="org-ul">
<li>\(X = 1\) rappresenta il "successo" con probabilità \(p\)</li>
<li>\(X = 0\) rappresenta il "fallimento" con probabilità \(1-p\)</li>
</ul>

<p>
La funzione di massa di probabilità è:
\[p_X(x) = p^x (1-p)^{1-x} \cdot I_{\{0,1\}}(x)\]
</p>

<p>
dove \(I_{\{0,1\}}(x)\) è la funzione indicatrice dell'insieme \(\{0,1\}\).
</p>

<p>
<b>Proprietà principali:</b>
</p>
<ul class="org-ul">
<li><b>Valore atteso:</b> \(E[X] = p\)</li>
<li><b>Varianza:</b> \(Var(X) = p(1-p)\)</li>
</ul>

<p>
<b>Dimostrazione della varianza:</b>
</p>
\begin{align}
Var(X) &= E[X^2] - E[X]^2 \\
&= E[X] - E[X]^2 \quad \text{(usando l'idempotenza: } X^2 = X \text{ per } X \in \{0,1\}\text{)} \\
&= p - p^2 \\
&= p(1-p)
\end{align}

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
<span style="color: #a020f0;">import</span> matplotlib.pyplot <span style="color: #a020f0;">as</span> plt
<span style="color: #a020f0;">import</span> scipy.stats <span style="color: #a020f0;">as</span> st

<span style="color: #b22222;"># </span><span style="color: #b22222;">Creazione di una distribuzione di Bernoulli</span>
<span style="color: #a0522d;">p</span> = 0.3  <span style="color: #b22222;"># </span><span style="color: #b22222;">parametro della distribuzione</span>
<span style="color: #a0522d;">bernoulli</span> = st.bernoulli(p)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo della funzione di massa di probabilit&#224;</span>
<span style="color: #a0522d;">x</span> = np.array([0, 1])
<span style="color: #a0522d;">pmf_values</span> = bernoulli.pmf(x)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X=0) = </span>{pmf_values[0]}<span style="color: #8b2252;">"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X=1) = </span>{pmf_values[1]}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo della funzione di ripartizione</span>
<span style="color: #a0522d;">cdf_values</span> = bernoulli.cdf(x)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X&#8804;0) = </span>{cdf_values[0]}<span style="color: #8b2252;">"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X&#8804;1) = </span>{cdf_values[1]}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Generazione di variabili aleatorie</span>
<span style="color: #a0522d;">random_sample</span> = bernoulli.rvs(size=1000)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Media campionaria: </span>{random_sample.mean()}<span style="color: #8b2252;">"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Varianza campionaria: </span>{random_sample.var()}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo dei momenti</span>
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Valore atteso teorico: </span>{bernoulli.mean()}<span style="color: #8b2252;">"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Varianza teorica: </span>{bernoulli.var()}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Visualizzazione della distribuzione</span>
plt.figure(figsize=(8, 4))
plt.bar(x, pmf_values, width=0.2)
plt.xlabel(<span style="color: #8b2252;">'x'</span>)
plt.ylabel(<span style="color: #8b2252;">'P(X=x)'</span>)
plt.title(f<span style="color: #8b2252;">'Distribuzione di Bernoulli con p=</span>{p}<span style="color: #8b2252;">'</span>)
plt.xticks([0, 1])
plt.grid(alpha=0.3)
plt.show()
</pre>
</div>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
<span style="color: #a020f0;">import</span> matplotlib.pyplot <span style="color: #a020f0;">as</span> plt
<span style="color: #a020f0;">import</span> scipy.stats <span style="color: #a020f0;">as</span> st

<span style="color: #b22222;"># </span><span style="color: #b22222;">Creazione di una distribuzione di Bernoulli</span>
<span style="color: #a0522d;">p</span> = 0.3  <span style="color: #b22222;"># </span><span style="color: #b22222;">parametro della distribuzione</span>
<span style="color: #a0522d;">bernoulli</span> = st.bernoulli(p)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo della funzione di massa di probabilit&#224;</span>
<span style="color: #a0522d;">x</span> = np.array([0, 1])
<span style="color: #a0522d;">pmf_values</span> = bernoulli.pmf(x)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X=0) = </span>{pmf_values[0]}<span style="color: #8b2252;">"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X=1) = </span>{pmf_values[1]}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo della funzione di ripartizione</span>
<span style="color: #a0522d;">cdf_values</span> = bernoulli.cdf(x)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X&#8804;0) = </span>{cdf_values[0]}<span style="color: #8b2252;">"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X&#8804;1) = </span>{cdf_values[1]}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Generazione di variabili aleatorie</span>
<span style="color: #a0522d;">random_sample</span> = bernoulli.rvs(size=1000)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Media campionaria: </span>{random_sample.mean()}<span style="color: #8b2252;">"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Varianza campionaria: </span>{random_sample.var()}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo dei momenti</span>
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Valore atteso teorico: </span>{bernoulli.mean()}<span style="color: #8b2252;">"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Varianza teorica: </span>{bernoulli.var()}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Visualizzazione della distribuzione e della funzione di ripartizione</span>
plt.figure(figsize=(12, 5))

<span style="color: #b22222;"># </span><span style="color: #b22222;">Subplot per la PMF</span>
plt.subplot(1, 2, 1)
plt.bar(x, pmf_values, width=0.2)
plt.xlabel(<span style="color: #8b2252;">'x'</span>)
plt.ylabel(<span style="color: #8b2252;">'P(X=x)'</span>)
plt.title(f<span style="color: #8b2252;">'PMF - Bernoulli con p=</span>{p}<span style="color: #8b2252;">'</span>)
plt.xticks([0, 1])
plt.grid(alpha=0.3)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Subplot per la CDF</span>
plt.subplot(1, 2, 2)
<span style="color: #b22222;"># </span><span style="color: #b22222;">Definiamo i punti per la CDF</span>
<span style="color: #a0522d;">x_step</span> = [-0.5, 0, 1, 1.5]  <span style="color: #b22222;"># </span><span style="color: #b22222;">Punti x: prima di 0, a 0, a 1, dopo 1</span>
<span style="color: #a0522d;">y_step</span> = [0, 1-p, 1, 1]     <span style="color: #b22222;"># </span><span style="color: #b22222;">Valori CDF corrispondenti</span>
plt.step(x_step, y_step, <span style="color: #8b2252;">'r-'</span>, where=<span style="color: #8b2252;">'post'</span>, lw=2, label=<span style="color: #8b2252;">'CDF'</span>)
<span style="color: #b22222;"># </span><span style="color: #b22222;">Aggiungiamo i punti per evidenziare le discontinuit&#224;</span>
plt.plot([0, 0], [0, 1-p], <span style="color: #8b2252;">'k--'</span>, alpha=0.3)  <span style="color: #b22222;"># </span><span style="color: #b22222;">Linea verticale a x=0</span>
plt.plot([1, 1], [1-p, 1], <span style="color: #8b2252;">'k--'</span>, alpha=0.3)  <span style="color: #b22222;"># </span><span style="color: #b22222;">Linea verticale a x=1</span>
plt.scatter([0, 1], [1-p, 1], color=<span style="color: #8b2252;">'red'</span>, s=50, zorder=3)  <span style="color: #b22222;"># </span><span style="color: #b22222;">Punti inclusi</span>
plt.scatter([0, 1], [0, 1-p], facecolors=<span style="color: #8b2252;">'white'</span>, edgecolors=<span style="color: #8b2252;">'red'</span>, s=50, zorder=3)  <span style="color: #b22222;"># </span><span style="color: #b22222;">Punti esclusi</span>
plt.xlabel(<span style="color: #8b2252;">'x'</span>)
plt.ylabel(<span style="color: #8b2252;">'P(X&#8804;x)'</span>)
plt.title(f<span style="color: #8b2252;">'CDF - Bernoulli con p=</span>{p}<span style="color: #8b2252;">'</span>)
plt.xticks([-0.5, 0, 1, 1.5])
plt.yticks([0, 1-p, 1])
plt.grid(alpha=0.3)
plt.legend()

plt.tight_layout()
plt.show()
</pre>
</div>
</div>
</div>
<div id="outline-container-org0c4b5a7" class="outline-4">
<h4 id="org0c4b5a7">Binomiale</h4>
<div class="outline-text-4" id="text-org0c4b5a7">
<p>
Supporto discreto.
Modella un esperimento che consiste in \(n\) sottoesperimenti bernoulliani di parametro \(p\) indipendenti fra loro.
Indicato con \(X \sim B(n,p)\)
</p>

<p>
Riguardo la funzione di massa di probabilità, si consideri che la probabilità che una singola serie di \(n\) esperimenti abbia \(i\) successi e \(n-i\) insuccessi è \(p^i(1-p)^{(n-i)}\). In totale, ci sono \(\binom{n}{i}\) combinazioni con \(i\) successi e quindi ottengo:
\[p_X(i) = \binom{n}{i} p^i (1-p)^{n-i} \mathbb{I}_{\{0,1,...,n\}}(i)\]
</p>

<p>
Dimostrazione che la somma è 1:
\[\sum_{i=0}^n \binom{n}{i} p^i (1-p)^{n-i} = (p+(1-p))^n = 1^n = 1\]
nel secondo step uso il binomio di Newton:
\[\sum_{i=0}^n \binom{n}{i} a^i b^{n-i} = (a+b)^n\]
</p>

<p>
Per il valore atteso, se applico la definizione esce una sommatoria difficile da calcolare.
Invece, considero le variabili bernoulliane \(X_i\) che compongono la binomiale e uso la linearità del valore atteso, quindi:
\[E(X) = E\left(\sum X_i\right) = \sum E(X_i) = \sum p = np\]
</p>

<p>
Stesso per la varianza, sfruttando l'indipendenza per usare la linearità della varianza, quindi il risultato è:
\[\text{Var}(X) = np(1-p)\]
</p>

<p>
La funzione di ripartizione è:
\[P(X \leq x) = \sum_{i=0}^{\lfloor x \rfloor} \binom{n}{i} p^i (1-p)^{n-i} \mathbb{I}_{[0,n]}(x) + \mathbb{I}_{(n,\infty)}(x)\]
</p>

<p>
Se \(X_1 \sim B(n,p)\) e \(X_2 \sim B(m,p)\) sono binomiali e indipendenti, allora:
\[(X_1 + X_2) \sim B(n+m,p)\]
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
<span style="color: #a020f0;">import</span> matplotlib.pyplot <span style="color: #a020f0;">as</span> plt
<span style="color: #a020f0;">import</span> scipy.stats <span style="color: #a020f0;">as</span> st

<span style="color: #b22222;"># </span><span style="color: #b22222;">Parametri della distribuzione binomiale</span>
<span style="color: #a0522d;">n</span> = 10  <span style="color: #b22222;"># </span><span style="color: #b22222;">numero di prove</span>
<span style="color: #a0522d;">p</span> = 0.3  <span style="color: #b22222;"># </span><span style="color: #b22222;">probabilit&#224; di successo</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Creazione dell'oggetto distribuzione binomiale</span>
<span style="color: #a0522d;">binom</span> = st.binom(n, p)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo della funzione di massa di probabilit&#224;</span>
<span style="color: #a0522d;">x</span> = np.arange(0, n+1)
<span style="color: #a0522d;">pmf_values</span> = binom.pmf(x)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Funzione personalizzata per calcolare la PMF</span>
<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">p_binom</span>(k, n, p):
    <span style="color: #8b2252;">"""Calcola la PMF di una binomiale"""</span>
    <span style="color: #a020f0;">from</span> scipy.special <span style="color: #a020f0;">import</span> comb
    <span style="color: #a020f0;">if</span> k &lt; 0 <span style="color: #a020f0;">or</span> k &gt; n:
        <span style="color: #a020f0;">return</span> 0
    <span style="color: #a020f0;">return</span> comb(n, k) * p**k * (1-p)**(n-k)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Verifica</span>
<span style="color: #483d8b;">print</span>(<span style="color: #8b2252;">"PMF calcolata manualmente:"</span>)
<span style="color: #a020f0;">for</span> k <span style="color: #a020f0;">in</span> <span style="color: #483d8b;">range</span>(n+1):
    <span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X=</span>{k}<span style="color: #8b2252;">) = </span>{p_binom(k, n, p)}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo della funzione di ripartizione</span>
<span style="color: #a0522d;">cdf_values</span> = binom.cdf(x)
<span style="color: #483d8b;">print</span>(<span style="color: #8b2252;">"</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">Valori della funzione di ripartizione:"</span>)
<span style="color: #a020f0;">for</span> k <span style="color: #a020f0;">in</span> <span style="color: #483d8b;">range</span>(n+1):
    <span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X&#8804;</span>{k}<span style="color: #8b2252;">) = </span>{cdf_values[k]}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo dei momenti</span>
<span style="color: #a0522d;">mean</span> = binom.mean()
<span style="color: #a0522d;">var</span> = binom.var()
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">Valore atteso: </span>{mean}<span style="color: #8b2252;"> (teoricamente: </span>{n*p}<span style="color: #8b2252;">)"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Varianza: </span>{var}<span style="color: #8b2252;"> (teoricamente: </span>{n*p*(1-p)}<span style="color: #8b2252;">)"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Generazione di un campione di variabili aleatorie binomiali</span>
<span style="color: #a0522d;">random_sample</span> = binom.rvs(size=1000)
<span style="color: #a0522d;">sample_mean</span> = random_sample.mean()
<span style="color: #a0522d;">sample_var</span> = random_sample.var()
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">Media campionaria: </span>{sample_mean}<span style="color: #8b2252;">"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Varianza campionaria: </span>{sample_var}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Visualizzazione della distribuzione</span>
plt.figure(figsize=(10, 5))
plt.bar(x, pmf_values, width=0.4)
plt.plot(x, cdf_values, <span style="color: #8b2252;">'r-o'</span>, alpha=0.5, label=<span style="color: #8b2252;">'Funzione di ripartizione'</span>)
plt.xlabel(<span style="color: #8b2252;">'Numero di successi'</span>)
plt.ylabel(<span style="color: #8b2252;">'Probabilit&#224;'</span>)
plt.title(f<span style="color: #8b2252;">'Distribuzione Binomiale B(</span>{n}<span style="color: #8b2252;">, </span>{p}<span style="color: #8b2252;">)'</span>)
plt.grid(alpha=0.3)
plt.legend()
plt.show()
</pre>
</div>
</div>
</div>
<div id="outline-container-org1e8f7e9" class="outline-4">
<h4 id="org1e8f7e9">Geometrico (guardare anche dispensa)</h4>
<div class="outline-text-4" id="text-org1e8f7e9">
<p>
Modello discreto che parte da bernoulli.
Conta quante ripetizioni servono prima che avvenga un successo.
</p>
</div>
<div id="outline-container-orgc76f5cc" class="outline-5">
<h5 id="orgc76f5cc">Definizione e concetti base</h5>
<div class="outline-text-5" id="text-orgc76f5cc">
<p>
La distribuzione geometrica descrive il numero di insuccessi necessari affinché si verifichi il primo successo in una successione di esperimenti bernoulliani indipendenti e identicamente distribuiti.
</p>

<ul class="org-ul">
<li>Parametro: \(p\) = probabilità di successo in un singolo esperimento</li>
<li>Variabile aleatoria: \(X\) = numero di insuccessi prima del primo successo</li>
<li>Supporto: \(X \in \mathbb{N} \cup \{0\}\)</li>
</ul>

<p>
<b>Nota:</b> Esiste una definizione alternativa che conta il numero totale di esperimenti fino al primo successo, in quel caso \(Y = X + 1\) e \(Y \in \mathbb{N}^+\).
</p>
</div>
</div>
<div id="outline-container-orgf519898" class="outline-5">
<h5 id="orgf519898">Funzione di massa di probabilità</h5>
<div class="outline-text-5" id="text-orgf519898">
<p>
La funzione di massa di probabilità è data da:
</p>

<p>
\[f_X(i; p) = p(1-p)^i \mathbb{I}_{\mathbb{N} \cup \{0\}}(i)\]
</p>

<p>
Implementazione in Python:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">def</span> <span style="color: #0000ff;">geom_pdf</span>(x, p):
    <span style="color: #a020f0;">assert</span> p &gt; 0 <span style="color: #a020f0;">and</span> p &lt;= 1, <span style="color: #8b2252;">'{} is not a valid parameter for the geometric distribution.'</span>.<span style="color: #483d8b;">format</span>(p)
    <span style="color: #a020f0;">return</span> p * (1 - p)**x <span style="color: #a020f0;">if</span> x==<span style="color: #483d8b;">int</span>(x) <span style="color: #a020f0;">and</span> x &gt;= 0 <span style="color: #a020f0;">else</span> 0
</pre>
</div>
</div>
</div>
<div id="outline-container-orga4968ae" class="outline-5">
<h5 id="orga4968ae">Valore atteso</h5>
<div class="outline-text-5" id="text-orga4968ae">
<p>
Il valore atteso della distribuzione geometrica è:
</p>

<p>
\[E[X] = \frac{1-p}{p}\]
</p>

<p>
<b>Dimostrazione:</b> Utilizzando la serie geometrica \(\sum_{i=0}^{+\infty} \alpha^i = \frac{1}{1-\alpha}\) per \(|\alpha| < 1\) e le sue derivate.
</p>

\begin{align}
E[X] &= \sum_{i=0}^{+\infty} i \cdot p(1-p)^i \\
&= p \sum_{i=1}^{+\infty} i \cdot (1-p)^i \\
&= p(1-p) \sum_{i=1}^{+\infty} i \cdot (1-p)^{i-1} \\
&= p(1-p) \frac{d}{d(1-p)} \sum_{i=0}^{+\infty} (1-p)^i \\
&= p(1-p) \frac{d}{d(1-p)} \frac{1}{1-(1-p)} \\
&= p(1-p) \frac{d}{d(1-p)} \frac{1}{p} \\
&= p(1-p) \cdot \frac{1}{p^2} \\
&= \frac{1-p}{p}
\end{align}
</div>
</div>
<div id="outline-container-org732100c" class="outline-5">
<h5 id="org732100c">Varianza</h5>
<div class="outline-text-5" id="text-org732100c">
<p>
La varianza della distribuzione geometrica è:
</p>

<p>
\[\text{Var}(X) = \frac{1-p}{p^2}\]
</p>

<p>
<b>Nota:</b> All'aumentare di \(p\), sia il valore atteso che la varianza diminuiscono, coerentemente con l'interpretazione probabilistica (maggiore probabilità di successo = minor numero atteso di insuccessi).
</p>
</div>
</div>
<div id="outline-container-org316685e" class="outline-5">
<h5 id="org316685e">Funzione di ripartizione</h5>
<div class="outline-text-5" id="text-org316685e">
<p>
La funzione di ripartizione ha una forma analitica calcolabile:
</p>

<p>
\[F_X(x; p) = \left(1 - (1-p)^{\lfloor x \rfloor + 1}\right) \mathbb{I}_{[0, +\infty]}(x)\]
</p>

<p>
Implementazione in Python:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">def</span> <span style="color: #0000ff;">geom_cdf</span>(x, p):
    <span style="color: #a020f0;">assert</span> p &gt; 0 <span style="color: #a020f0;">and</span> p &lt;= 1, <span style="color: #8b2252;">'{} is not a valid parameter for the geometric distribution.'</span>.<span style="color: #483d8b;">format</span>(p)
    <span style="color: #a020f0;">return</span> 1 - (1-p)**(<span style="color: #483d8b;">int</span>(x) + 1) <span style="color: #a020f0;">if</span> x &gt;= 0 <span style="color: #a020f0;">else</span> 0
</pre>
</div>
</div>
</div>
<div id="outline-container-org698e5cf" class="outline-5">
<h5 id="org698e5cf">Proprietà di assenza di memoria</h5>
<div class="outline-text-5" id="text-org698e5cf">
<p>
Una caratteristica fondamentale della distribuzione geometrica è l'assenza di memoria:
</p>

<p>
\[P(X \geq x+y | X \geq x) = P(X \geq y)\]
</p>

<p>
Questo significa che la probabilità di dover attendere altri \(y\) insuccessi non dipende da quanti insuccessi si sono già verificati.
</p>

<p>
<b>Dimostrazione:</b>
</p>
\begin{align}
P(X \geq x+y | X \geq x) &= \frac{P(X \geq x+y \cap X \geq x)}{P(X \geq x)} \\
&= \frac{P(X \geq x+y)}{P(X \geq x)} \\
&= \frac{(1-p)^{x+y}}{(1-p)^x} \\
&= (1-p)^y \\
&= P(X \geq y)
\end{align}
</div>
</div>
<div id="outline-container-org2cc6ff4" class="outline-5">
<h5 id="org2cc6ff4">Relazione con altre distribuzioni</h5>
<div class="outline-text-5" id="text-org2cc6ff4">
<ul class="org-ul">
<li>La distribuzione geometrica è un caso particolare della distribuzione binomiale negativa</li>
<li>Se \(X \sim \text{Geom}(p)\), allora \(Y = X + 1\) segue la distribuzione del numero di prove fino al primo successo</li>
<li>La somma di \(n\) variabili geometriche indipendenti con lo stesso parametro \(p\) segue una distribuzione binomiale negativa</li>
</ul>
</div>
</div>
<div id="outline-container-org0942342" class="outline-5">
<h5 id="org0942342">Applicazioni</h5>
<div class="outline-text-5" id="text-org0942342">
<p>
La distribuzione geometrica modella efficacemente:
</p>
<ul class="org-ul">
<li>Il numero di tentativi prima di ottenere un successo</li>
<li>Il tempo di attesa (discreto) fino alla prima occorrenza di un evento raro</li>
<li>Fenomeni di affidabilità in cui si attende il primo guasto</li>
</ul>

<p>
<b>Esempio:</b> Se la probabilità di vincere alla lotteria è \(p = 0.001\), il numero atteso di biglietti da acquistare prima di vincere è \(E[X+1] = 1/p = 1000\).
</p>
</div>
</div>
<div id="outline-container-org3107891" class="outline-5">
<h5 id="org3107891">Python</h5>
<div class="outline-text-5" id="text-org3107891">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
<span style="color: #a020f0;">import</span> matplotlib.pyplot <span style="color: #a020f0;">as</span> plt
<span style="color: #a020f0;">import</span> scipy.stats <span style="color: #a020f0;">as</span> st
<span style="color: #a020f0;">from</span> ipywidgets <span style="color: #a020f0;">import</span> interact, FloatSlider
<span style="color: #a020f0;">import</span> matplotlib.patches <span style="color: #a020f0;">as</span> patches

<span style="color: #b22222;"># </span><span style="color: #b22222;">Funzione di massa di probabilit&#224; per la distribuzione geometrica</span>
<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">geom_pdf</span>(x, p):
    <span style="color: #a020f0;">assert</span> p &gt; 0 <span style="color: #a020f0;">and</span> p &lt;= 1, <span style="color: #8b2252;">'{} non &#232; un parametro valido per la distribuzione geometrica'</span>.<span style="color: #483d8b;">format</span>(p)
    <span style="color: #a020f0;">return</span> p * (1 - p)**x <span style="color: #a020f0;">if</span> x==<span style="color: #483d8b;">int</span>(x) <span style="color: #a020f0;">and</span> x &gt;= 0 <span style="color: #a020f0;">else</span> 0

<span style="color: #b22222;"># </span><span style="color: #b22222;">Funzione di ripartizione per la distribuzione geometrica</span>
<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">geom_cdf</span>(x, p):
    <span style="color: #a020f0;">assert</span> p &gt; 0 <span style="color: #a020f0;">and</span> p &lt;= 1, <span style="color: #8b2252;">'{} non &#232; un parametro valido per la distribuzione geometrica'</span>.<span style="color: #483d8b;">format</span>(p)
    <span style="color: #a020f0;">return</span> 1 - (1-p)**(<span style="color: #483d8b;">int</span>(x) + 1) <span style="color: #a020f0;">if</span> x &gt;= 0 <span style="color: #a020f0;">else</span> 0

<span style="color: #b22222;"># </span><span style="color: #b22222;">Creazione di una distribuzione geometrica usando scipy.stats</span>
<span style="color: #a0522d;">p</span> = 0.3
<span style="color: #a0522d;">geom_dist</span> = st.geom(p)  <span style="color: #b22222;"># </span><span style="color: #b22222;">Nota: scipy.stats usa la definizione alternativa (Y = X + 1)</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo valori PMF e CDF usando funzioni personalizzate e scipy</span>
<span style="color: #a0522d;">x_values</span> = np.arange(0, 10)
<span style="color: #a0522d;">pdf_custom</span> = [geom_pdf(x, p) <span style="color: #a020f0;">for</span> x <span style="color: #a020f0;">in</span> x_values]
<span style="color: #a0522d;">cdf_custom</span> = [geom_cdf(x, p) <span style="color: #a020f0;">for</span> x <span style="color: #a020f0;">in</span> x_values]

<span style="color: #b22222;"># </span><span style="color: #b22222;">Nota: Per confrontare con scipy, dobbiamo modificare i valori poich&#233;</span>
<span style="color: #b22222;"># </span><span style="color: #b22222;">scipy.stats.geom usa la definizione Y = X + 1</span>
<span style="color: #a0522d;">pdf_scipy</span> = geom_dist.pmf(x_values + 1)
<span style="color: #a0522d;">cdf_scipy</span> = geom_dist.cdf(x_values + 1) - geom_dist.pmf(0)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Verifica che le funzioni siano corrette</span>
<span style="color: #483d8b;">print</span>(<span style="color: #8b2252;">"Confronto tra PMF personalizzata e scipy.stats:"</span>)
<span style="color: #a020f0;">for</span> i, x <span style="color: #a020f0;">in</span> <span style="color: #483d8b;">enumerate</span>(x_values):
    <span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X=</span>{x}<span style="color: #8b2252;">) = </span>{pdf_custom[i]:.6f}<span style="color: #8b2252;"> (custom) vs </span>{pdf_scipy[i]:.6f}<span style="color: #8b2252;"> (scipy)"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Simulazione di valori di una geometrica</span>
<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">geom_sim</span>(p, size=1):
    <span style="color: #8b2252;">"""Simula valori da una distribuzione geometrica"""</span>
    <span style="color: #a0522d;">result</span> = []
    <span style="color: #a020f0;">for</span> _ <span style="color: #a020f0;">in</span> <span style="color: #483d8b;">range</span>(size):
        <span style="color: #a0522d;">count</span> = 0
        <span style="color: #a020f0;">while</span> np.random.random() &gt;= p:
            <span style="color: #a0522d;">count</span> += 1
        result.append(count)
    <span style="color: #a020f0;">return</span> np.array(result)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Simulazione usando la funzione personalizzata</span>
<span style="color: #a0522d;">sim_sample_custom</span> = geom_sim(p, 1000)
<span style="color: #a0522d;">sim_mean_custom</span> = sim_sample_custom.mean()
<span style="color: #a0522d;">sim_var_custom</span> = sim_sample_custom.var()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Simulazione usando scipy.stats (correggiamo per la definizione)</span>
<span style="color: #a0522d;">sim_sample_scipy</span> = geom_dist.rvs(size=1000) - 1
<span style="color: #a0522d;">sim_mean_scipy</span> = sim_sample_scipy.mean()
<span style="color: #a0522d;">sim_var_scipy</span> = sim_sample_scipy.var()

<span style="color: #483d8b;">print</span>(<span style="color: #8b2252;">"</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">Statistiche dalle simulazioni:"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Media (custom): </span>{sim_mean_custom:.4f}<span style="color: #8b2252;"> vs Media teorica: </span>{(1-p)/p:.4f}<span style="color: #8b2252;">"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Varianza (custom): </span>{sim_var_custom:.4f}<span style="color: #8b2252;"> vs Varianza teorica: </span>{(1-p)/p**2:.4f}<span style="color: #8b2252;">"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Media (scipy): </span>{sim_mean_scipy:.4f}<span style="color: #8b2252;">"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Varianza (scipy): </span>{sim_var_scipy:.4f}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Funzione per visualizzare la distribuzione con parametro variabile</span>
<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">visualizza_geom_pdf</span>(p=0.5):
    <span style="color: #a0522d;">x</span> = np.arange(0, 10, 1)
    <span style="color: #a0522d;">avg</span> = (1 - p) / p
    <span style="color: #a0522d;">stdev</span> = np.sqrt((1-p)/p**2)

    plt.figure(figsize=(10, 6))

    <span style="color: #b22222;"># </span><span style="color: #b22222;">Aggiungi intervallo di confidenza</span>
    plt.gca().add_patch(patches.Rectangle(
        (avg-stdev, 0.95), 2*stdev, 0.05, edgecolor=<span style="color: #8b2252;">'None'</span>, facecolor=<span style="color: #8b2252;">'green'</span>
    ))
    plt.plot([avg, avg], [0.9, 1], color=<span style="color: #8b2252;">'green'</span>, label=f<span style="color: #8b2252;">'Media: </span>{avg:.2f}<span style="color: #8b2252;">'</span>)

    <span style="color: #b22222;"># </span><span style="color: #b22222;">Grafico a bastoncini della PMF</span>
    plt.vlines(x, [0]*<span style="color: #483d8b;">len</span>(x), [geom_pdf(_, p) <span style="color: #a020f0;">for</span> _ <span style="color: #a020f0;">in</span> x], color=<span style="color: #8b2252;">'blue'</span>)
    plt.plot(x, [geom_pdf(_, p) <span style="color: #a020f0;">for</span> _ <span style="color: #a020f0;">in</span> x], <span style="color: #8b2252;">'o'</span>, color=<span style="color: #8b2252;">'blue'</span>, label=<span style="color: #8b2252;">'PMF'</span>)

    <span style="color: #b22222;"># </span><span style="color: #b22222;">Aggiungi CDF</span>
    plt.plot(x, [geom_cdf(_, p) <span style="color: #a020f0;">for</span> _ <span style="color: #a020f0;">in</span> x], <span style="color: #8b2252;">'s--'</span>, color=<span style="color: #8b2252;">'red'</span>, alpha=0.7, label=<span style="color: #8b2252;">'CDF'</span>)

    plt.ylim(ymax=1, ymin=0)
    plt.xlim(xmax=11, xmin=-1)
    plt.grid(alpha=0.3)
    plt.title(f<span style="color: #8b2252;">'Distribuzione Geometrica con p=</span>{p}<span style="color: #8b2252;">'</span>)
    plt.xlabel(<span style="color: #8b2252;">'x (numero di insuccessi)'</span>)
    plt.ylabel(<span style="color: #8b2252;">'Probabilit&#224;'</span>)
    plt.legend()
    plt.show()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Visualizza la distribuzione con un controllo interattivo per p</span>
interact(visualizza_geom_pdf, p=FloatSlider(<span style="color: #483d8b;">min</span>=0.1, <span style="color: #483d8b;">max</span>=1.0, step=0.1, value=0.5))
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-org7036508" class="outline-4">
<h4 id="org7036508">Uniforme discreto</h4>
<div class="outline-text-4" id="text-org7036508">
<p>
Si immagini di avere \(n\in \mathbb{N}\) esiti equiprobabili numerati. Allora \(X\sim U(n)\) e 
\[p_X(i) = P(X=i)=\frac{1}{n}\mathbb{I}_{\{1,...,n\}}(i)\]
</p>

<p>
\[F_X(x)=P(X\leq x)=\sum_{i=1}^{\lfloor x \rfloor} P(X=i)\]
che non dipende da \(i\) e quindi 
\[F_X(x) = \frac{\lfloor x \rfloor}{n}\mathbb{I}_{[1,n[}(x) + \mathbb{I}_{[n,\infty)}(x)\]
</p>

<p>
\[E(X) = \frac{n+1}{2}\]
\[E(X^2) = \frac{(n+1)(2n+1)}{6}\]
\[Var(X) = E(X^2)-E(X)^2 = \frac{n^2-1}{12}\]
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
<span style="color: #a020f0;">import</span> matplotlib.pyplot <span style="color: #a020f0;">as</span> plt
<span style="color: #a020f0;">import</span> scipy.stats <span style="color: #a020f0;">as</span> st

<span style="color: #b22222;"># </span><span style="color: #b22222;">Parametri della distribuzione uniforme discreta</span>
<span style="color: #a0522d;">n</span> = 6  <span style="color: #b22222;"># </span><span style="color: #b22222;">come un dado a sei facce</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Creazione dell'oggetto distribuzione uniforme discreta</span>
<span style="color: #a0522d;">unif_disc</span> = st.randint(1, n+1)  <span style="color: #b22222;"># </span><span style="color: #b22222;">scipy.stats.randint usa [low, high)</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo della funzione di massa di probabilit&#224;</span>
<span style="color: #a0522d;">x</span> = np.arange(1, n+1)
<span style="color: #a0522d;">pmf_values</span> = unif_disc.pmf(x)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Funzione personalizzata per calcolare la PMF</span>
<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">p_unif_disc</span>(k, n):
    <span style="color: #8b2252;">"""Calcola la PMF di una uniforme discreta"""</span>
    <span style="color: #a020f0;">if</span> 1 &lt;= k &lt;= n <span style="color: #a020f0;">and</span> k == <span style="color: #483d8b;">int</span>(k):
        <span style="color: #a020f0;">return</span> 1/n
    <span style="color: #a020f0;">return</span> 0

<span style="color: #b22222;"># </span><span style="color: #b22222;">Verifica</span>
<span style="color: #483d8b;">print</span>(<span style="color: #8b2252;">"PMF calcolata manualmente:"</span>)
<span style="color: #a020f0;">for</span> k <span style="color: #a020f0;">in</span> <span style="color: #483d8b;">range</span>(1, n+1):
    <span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X=</span>{k}<span style="color: #8b2252;">) = </span>{p_unif_disc(k, n)}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo della funzione di ripartizione</span>
<span style="color: #a0522d;">cdf_values</span> = unif_disc.cdf(x)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Funzione personalizzata per calcolare la CDF</span>
<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">F_unif_disc</span>(x, n):
    <span style="color: #8b2252;">"""Calcola la CDF di una uniforme discreta"""</span>
    <span style="color: #a020f0;">if</span> x &lt; 1:
        <span style="color: #a020f0;">return</span> 0
    <span style="color: #a020f0;">elif</span> x &gt;= n:
        <span style="color: #a020f0;">return</span> 1
    <span style="color: #a020f0;">else</span>:
        <span style="color: #a020f0;">return</span> np.floor(x)/n

<span style="color: #b22222;"># </span><span style="color: #b22222;">Verifica della CDF</span>
<span style="color: #483d8b;">print</span>(<span style="color: #8b2252;">"</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">CDF calcolata manualmente:"</span>)
<span style="color: #a0522d;">check_points</span> = [0.5, 1, 1.5, 2, 4.7, 6, 7]
<span style="color: #a020f0;">for</span> x <span style="color: #a020f0;">in</span> check_points:
    <span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X&#8804;</span>{x}<span style="color: #8b2252;">) = </span>{F_unif_disc(x, n)}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo dei momenti</span>
<span style="color: #a0522d;">mean</span> = unif_disc.mean()
<span style="color: #a0522d;">var</span> = unif_disc.var()
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">Valore atteso: </span>{mean}<span style="color: #8b2252;"> (teoricamente: </span>{(n+1)/2}<span style="color: #8b2252;">)"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Varianza: </span>{var}<span style="color: #8b2252;"> (teoricamente: </span>{(n**2-1)/12}<span style="color: #8b2252;">)"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Generazione di un campione di variabili aleatorie uniformi discrete</span>
<span style="color: #a0522d;">random_sample</span> = unif_disc.rvs(size=1000)
<span style="color: #a0522d;">sample_mean</span> = random_sample.mean()
<span style="color: #a0522d;">sample_var</span> = random_sample.var()
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">Media campionaria: </span>{sample_mean}<span style="color: #8b2252;">"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Varianza campionaria: </span>{sample_var}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Visualizzazione della distribuzione</span>
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.bar(x, pmf_values, width=0.4)
plt.xlabel(<span style="color: #8b2252;">'x'</span>)
plt.ylabel(<span style="color: #8b2252;">'P(X=x)'</span>)
plt.title(f<span style="color: #8b2252;">'PMF - Uniforme Discreta su </span>{1, 2, ..., n}<span style="color: #8b2252;">'</span>)
plt.grid(alpha=0.3)

plt.subplot(1, 2, 2)
<span style="color: #b22222;"># </span><span style="color: #b22222;">Per la CDF, aggiungiamo punti extra per mostrare i salti</span>
<span style="color: #a0522d;">x_minus</span> = np.array([val - 0.001 <span style="color: #a020f0;">for</span> val <span style="color: #a020f0;">in</span> x])
<span style="color: #a0522d;">x_plus</span> = np.array([val + 0.001 <span style="color: #a020f0;">for</span> val <span style="color: #a020f0;">in</span> x])
<span style="color: #a0522d;">x_cdf</span> = np.sort(np.concatenate([np.arange(0.8, n+1.2, 0.01), x_minus, x_plus]))
<span style="color: #a0522d;">cdf_custom</span> = [F_unif_disc(t, n) <span style="color: #a020f0;">for</span> t <span style="color: #a020f0;">in</span> x_cdf]
plt.plot(x_cdf, cdf_custom)
plt.xlabel(<span style="color: #8b2252;">'x'</span>)
plt.ylabel(<span style="color: #8b2252;">'P(X&#8804;x)'</span>)
plt.title(<span style="color: #8b2252;">'CDF - Uniforme Discreta'</span>)
plt.grid(alpha=0.3)

plt.tight_layout()
plt.show()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Istogramma di un campione grande per verificare la distribuzione</span>
plt.figure(figsize=(8, 5))
plt.hist(random_sample, bins=np.linspace(0.5, n+0.5, n+1), density=<span style="color: #008b8b;">True</span>)
plt.xlabel(<span style="color: #8b2252;">'Valore'</span>)
plt.ylabel(<span style="color: #8b2252;">'Frequenza relativa'</span>)
plt.title(<span style="color: #8b2252;">'Istogramma di 1000 campioni da Uniforme Discreta'</span>)
plt.xticks(<span style="color: #483d8b;">range</span>(1, n+1))
plt.grid(alpha=0.3)
plt.show()
</pre>
</div>
<p>
NON FUNZIONA!
</p>
</div>
</div>
<div id="outline-container-orgb2fb813" class="outline-4">
<h4 id="orgb2fb813">Poisson</h4>
<div class="outline-text-4" id="text-orgb2fb813">
<p>
<b>Definizione (Distribuzione di Poisson):</b> Una variabile aleatoria \(X\) segue una distribuzione di Poisson con parametro \(\lambda > 0\) (scriviamo \(X \sim \text{Pois}(\lambda)\)) se rappresenta il numero di eventi che si verificano in un intervallo fisso di tempo o spazio, quando questi eventi si verificano con un tasso medio costante e indipendentemente dall'istante dell'ultimo evento.
</p>

<p>
La funzione di massa di probabilità è:
\[p_X(i) = \frac{e^{-\lambda}\lambda^i}{i!} \mathbb{I}_{\mathbb{N} \cup \{0\}}(i)\]
</p>

<p>
<b>Proprietà principali:</b>
</p>
<ul class="org-ul">
<li><b>Supporto:</b> \(X \in \{0, 1, 2, ...\}\)</li>
<li><b>Valore atteso:</b> \(E[X] = \lambda\)</li>
<li><b>Varianza:</b> \(Var(X) = \lambda\)</li>
</ul>

<p>
<b>Dimostrazione del valore atteso:</b>
</p>
\begin{align}
E[X] &= \sum_{i=0}^{\infty} i \cdot \frac{e^{-\lambda}\lambda^i}{i!} \\
&= \sum_{i=1}^{\infty} i \cdot \frac{e^{-\lambda}\lambda^i}{i!} \\
&= \lambda e^{-\lambda} \sum_{i=1}^{\infty} \frac{\lambda^{i-1}}{(i-1)!} \\
&= \lambda e^{-\lambda} \sum_{j=0}^{\infty} \frac{\lambda^j}{j!} \\
&= \lambda e^{-\lambda} \cdot e^{\lambda} \\
&= \lambda
\end{align}

<p>
La funzione di ripartizione è:
\[F_X(x) = P(X \leq x) = \sum_{i=0}^{\lfloor x \rfloor} \frac{e^{-\lambda}\lambda^i}{i!} \mathbb{I}_{[0, \infty)}(x)\]
</p>

<p>
<b>Proprietà aggiuntive:</b>
</p>
<ul class="org-ul">
<li>Se \(X_1 \sim \text{Pois}(\lambda_1)\) e \(X_2 \sim \text{Pois}(\lambda_2)\) sono indipendenti, allora \(X_1 + X_2 \sim \text{Pois}(\lambda_1 + \lambda_2)\)</li>
<li>La distribuzione di Poisson può essere ottenuta come limite della distribuzione binomiale quando \(n \to \infty\), \(p \to 0\) e \(np = \lambda\) rimane costante</li>
</ul>

<p>
Un vantaggio significativo della distribuzione di Poisson rispetto a quella binomiale è di natura computazionale: quando si modellano eventi rari in campioni grandi, la binomiale richiederebbe il calcolo di un numero elevato di coefficienti binomiali e potenze, mentre la Poisson fornisce una buona approssimazione con una formula più semplice da calcolare, risparmiando così un considerevole numero di addendi e semplificando i calcoli.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
<span style="color: #a020f0;">import</span> matplotlib.pyplot <span style="color: #a020f0;">as</span> plt
<span style="color: #a020f0;">import</span> scipy.stats <span style="color: #a020f0;">as</span> st
<span style="color: #a020f0;">import</span> math
<span style="color: #a020f0;">from</span> ipywidgets <span style="color: #a020f0;">import</span> interact, FloatSlider

<span style="color: #b22222;"># </span><span style="color: #b22222;">Definizione di una funzione per calcolare la PMF della distribuzione di Poisson</span>
<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">p_poisson</span>(k, lambda_val):
    <span style="color: #8b2252;">"""Calcola la PMF della distribuzione di Poisson P(X=k)"""</span>
    <span style="color: #a020f0;">if</span> k &lt; 0 <span style="color: #a020f0;">or</span> k != <span style="color: #483d8b;">int</span>(k):
        <span style="color: #a020f0;">return</span> 0
    <span style="color: #a020f0;">return</span> np.exp(-lambda_val) * (lambda_val**k) / math.factorial(k)

<span style="color: #b22222;">#</span><span style="color: #b22222;">F</span>
<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">F_Pois</span>(l, k):
    <span style="color: #a020f0;">if</span> k &lt; 0:  <span style="color: #b22222;"># </span><span style="color: #b22222;">CDF is 0 for negative values</span>
        <span style="color: #a020f0;">return</span> 0
    <span style="color: #a0522d;">total</span> = 0
    <span style="color: #a020f0;">for</span> i <span style="color: #a020f0;">in</span> <span style="color: #483d8b;">range</span>(k + 1):  <span style="color: #b22222;"># </span><span style="color: #b22222;">Include k in the calculation</span>
        <span style="color: #a0522d;">total</span> += p_poisson(l, i)
    <span style="color: #a020f0;">return</span> total  <span style="color: #b22222;"># </span><span style="color: #b22222;">Return the calculated CDF value</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Parametro della distribuzione di Poisson</span>
<span style="color: #a0522d;">lambda_val</span> = 5

<span style="color: #b22222;"># </span><span style="color: #b22222;">Creazione dell'oggetto distribuzione di Poisson usando scipy.stats</span>
<span style="color: #a0522d;">poisson_dist</span> = st.poisson(lambda_val)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo della PMF per diversi valori</span>
<span style="color: #a0522d;">x_range</span> = np.arange(0, 15)
<span style="color: #a0522d;">pmf_custom</span> = [p_poisson(x, lambda_val) <span style="color: #a020f0;">for</span> x <span style="color: #a020f0;">in</span> x_range]
<span style="color: #a0522d;">pmf_scipy</span> = poisson_dist.pmf(x_range)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Verifica che le due implementazioni diano risultati identici</span>
<span style="color: #483d8b;">print</span>(<span style="color: #8b2252;">"Confronto tra implementazione personalizzata e scipy.stats:"</span>)
<span style="color: #a020f0;">for</span> i, x <span style="color: #a020f0;">in</span> <span style="color: #483d8b;">enumerate</span>(x_range):
    <span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X=</span>{x}<span style="color: #8b2252;">) = </span>{pmf_custom[i]:.6f}<span style="color: #8b2252;"> (custom) vs </span>{pmf_scipy[i]:.6f}<span style="color: #8b2252;"> (scipy)"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo della CDF</span>
<span style="color: #a0522d;">cdf_values</span> = poisson_dist.cdf(x_range)
<span style="color: #483d8b;">print</span>(<span style="color: #8b2252;">"</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">Valori della CDF:"</span>)
<span style="color: #a020f0;">for</span> i, x <span style="color: #a020f0;">in</span> <span style="color: #483d8b;">enumerate</span>(x_range):
    <span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X&#8804;</span>{x}<span style="color: #8b2252;">) = </span>{cdf_values[i]:.6f}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo di momenti e statistiche</span>
<span style="color: #a0522d;">mean</span> = poisson_dist.mean()
<span style="color: #a0522d;">var</span> = poisson_dist.var()
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">Valore atteso: </span>{mean}<span style="color: #8b2252;"> (teoricamente: </span>{lambda_val}<span style="color: #8b2252;">)"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Varianza: </span>{var}<span style="color: #8b2252;"> (teoricamente: </span>{lambda_val}<span style="color: #8b2252;">)"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Generazione di variabili aleatorie con distribuzione di Poisson</span>
<span style="color: #a0522d;">sample</span> = poisson_dist.rvs(size=1000)
<span style="color: #a0522d;">sample_mean</span> = sample.mean()
<span style="color: #a0522d;">sample_var</span> = sample.var()
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">Media campionaria: </span>{sample_mean}<span style="color: #8b2252;">"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Varianza campionaria: </span>{sample_var}<span style="color: #8b2252;">"</span>)

<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">visualizza_poisson</span>(lambda_val=5):
    <span style="color: #8b2252;">"""Funzione per visualizzare la distribuzione di Poisson con un dato parametro"""</span>
    <span style="color: #a0522d;">poisson_dist</span> = st.poisson(lambda_val)
    <span style="color: #a0522d;">x_range</span> = np.arange(0, <span style="color: #483d8b;">max</span>(20, <span style="color: #483d8b;">int</span>(lambda_val*3)))
    <span style="color: #a0522d;">pmf_values</span> = poisson_dist.pmf(x_range)
    <span style="color: #a0522d;">cdf_values</span> = poisson_dist.cdf(x_range)

    plt.figure(figsize=(12, 5))

    <span style="color: #b22222;"># </span><span style="color: #b22222;">Grafico della PMF con sticks invece che barre</span>
    plt.subplot(1, 2, 1)
    plt.vlines(x_range, [0]*<span style="color: #483d8b;">len</span>(x_range), pmf_values, colors=<span style="color: #8b2252;">'blue'</span>)  <span style="color: #b22222;"># </span><span style="color: #b22222;">Sticks verticali</span>
    plt.plot(x_range, pmf_values, <span style="color: #8b2252;">'o'</span>, color=<span style="color: #8b2252;">'blue'</span>)  <span style="color: #b22222;"># </span><span style="color: #b22222;">Punti alle estremit&#224;</span>
    plt.axvline(x=lambda_val, color=<span style="color: #8b2252;">'r'</span>, linestyle=<span style="color: #8b2252;">'--'</span>, 
                label=f<span style="color: #8b2252;">'Media = Varianza = </span>{lambda_val}<span style="color: #8b2252;">'</span>)
    plt.xlabel(<span style="color: #8b2252;">'k'</span>)
    plt.ylabel(<span style="color: #8b2252;">'P(X=k)'</span>)
    plt.title(f<span style="color: #8b2252;">'PMF - Poisson(&#955;=</span>{lambda_val}<span style="color: #8b2252;">)'</span>)
    plt.grid(alpha=0.3)
    plt.legend()

    <span style="color: #b22222;"># </span><span style="color: #b22222;">Grafico della CDF</span>
    plt.subplot(1, 2, 2)
    plt.step(x_range, cdf_values, where=<span style="color: #8b2252;">'post'</span>, color=<span style="color: #8b2252;">'green'</span>)
    plt.axvline(x=lambda_val, color=<span style="color: #8b2252;">'r'</span>, linestyle=<span style="color: #8b2252;">'--'</span>, 
                label=f<span style="color: #8b2252;">'Media = Varianza = </span>{lambda_val}<span style="color: #8b2252;">'</span>)
    plt.xlabel(<span style="color: #8b2252;">'x'</span>)
    plt.ylabel(<span style="color: #8b2252;">'P(X&#8804;x)'</span>)
    plt.title(f<span style="color: #8b2252;">'CDF - Poisson(&#955;=</span>{lambda_val}<span style="color: #8b2252;">)'</span>)
    plt.grid(alpha=0.3)
    plt.legend()

    plt.tight_layout()
    plt.show()

    <span style="color: #b22222;"># </span><span style="color: #b22222;">Istogramma di un campione grande</span>
    <span style="color: #a0522d;">sample</span> = poisson_dist.rvs(size=2000)
    plt.figure(figsize=(10, 4))
    plt.hist(sample, bins=np.arange(-0.5, <span style="color: #483d8b;">max</span>(sample)+1.5, 1), 
             density=<span style="color: #008b8b;">True</span>, alpha=0.7, label=<span style="color: #8b2252;">'Campione'</span>)
    plt.vlines(x_range, [0]*<span style="color: #483d8b;">len</span>(x_range), pmf_values, colors=<span style="color: #8b2252;">'red'</span>, alpha=0.7)
    plt.plot(x_range, pmf_values, <span style="color: #8b2252;">'o'</span>, color=<span style="color: #8b2252;">'red'</span>, label=<span style="color: #8b2252;">'PMF teorica'</span>)

    plt.axvline(x=lambda_val, color=<span style="color: #8b2252;">'k'</span>, linestyle=<span style="color: #8b2252;">'--'</span>, 
                label=f<span style="color: #8b2252;">'Media = Varianza = </span>{lambda_val}<span style="color: #8b2252;">'</span>)
    plt.xlabel(<span style="color: #8b2252;">'Valore'</span>)
    plt.ylabel(<span style="color: #8b2252;">'Frequenza relativa'</span>)
    plt.title(f<span style="color: #8b2252;">'Istogramma di 2000 campioni da Poisson(&#955;=</span>{lambda_val}<span style="color: #8b2252;">)'</span>)
    plt.grid(alpha=0.3)
    plt.legend()
    plt.show()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Eseguire per il valore di default</span>
visualizza_poisson(lambda_val)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Oppure creare un controllo interattivo</span>
<span style="color: #b22222;"># </span><span style="color: #b22222;">interact(visualizza_poisson, lambda_val=FloatSlider(min=0.5, max=15, step=0.5, value=5))</span>
</pre>
</div>
</div>
</div>
<div id="outline-container-org6b15bd9" class="outline-4">
<h4 id="org6b15bd9">Ipergeometrica</h4>
<div class="outline-text-4" id="text-org6b15bd9">
<p>
<b>Definizione (Distribuzione Ipergeometrica):</b> Una variabile aleatoria \(X\) segue una distribuzione ipergeometrica con parametri \(M\) (numero di oggetti funzionanti), \(N\) (numero di oggetti guasti) e \(n\) (numero di estrazioni), scriviamo \(X \sim \text{HGeom}(M,N,n)\), se rappresenta il numero di oggetti funzionanti in \(n\) estrazioni senza rimpiazzo da una popolazione di \(M+N\) elementi totali.
</p>

<p>
La funzione di massa di probabilità è:
\[p_X(i) = \frac{\binom{M}{i} \binom{N}{n-i}}{\binom{M+N}{n}} \mathbb{I}_{\{max(0,n-N), \ldots, min(n,M)\}}(i)\]
</p>

<p>
<b>Proprietà principali:</b>
</p>
<ul class="org-ul">
<li><b>Supporto:</b> \(X \in \{max(0,n-N), \ldots, min(n,M)\}\)</li>
</ul>

<p>
<b><b>Calcolo del valore atteso:</b></b>
</p>

<p>
Per determinare il valore atteso della variabile ipergeometrica, possiamo scomporla in variabili più semplici. Definiamo \(n\) variabili indicatrici \(X_1, X_2, \ldots, X_n\) dove:
\[X_i = \begin{cases}
1 & \text{se l'$i$-esimo oggetto estratto è funzionante} \\
0 & \text{altrimenti}
\end{cases}\]
</p>

<p>
Ciascuna \(X_i\) è una variabile bernoulliana, ma non con lo stesso parametro per ogni estrazione a causa dell'assenza di rimpiazzo. Tuttavia, per simmetria, la probabilità marginale che un singolo oggetto estratto sia funzionante è \(\frac{M}{M+N}\).
</p>

<p>
La variabile aleatoria ipergeometrica \(X\) rappresenta il numero totale di oggetti funzionanti estratti, quindi:
\[X = \sum_{i=1}^{n} X_i\]
</p>

<p>
Applicando la linearità del valore atteso:
\[E[X] = E\left[\sum_{i=1}^{n} X_i\right] = \sum_{i=1}^{n} E[X_i] = \sum_{i=1}^{n} \frac{M}{M+N} = n \cdot \frac{M}{M+N}\]
</p>

<p>
<b><b>Analogia con la distribuzione binomiale:</b></b>
</p>

<p>
La distribuzione ipergeometrica è simile alla distribuzione binomiale \(B(n, p)\) con \(p = \frac{M}{M+N}\), in quanto entrambe rappresentano il numero di "successi" in \(n\) prove. La differenza fondamentale è che:
</p>
<ul class="org-ul">
<li>Nella distribuzione binomiale, le prove sono indipendenti (con rimpiazzo)</li>
<li>Nella distribuzione ipergeometrica, le prove non sono indipendenti (senza rimpiazzo)</li>
</ul>

<p>
Quando la popolazione è molto grande rispetto al numero di estrazioni (cioè \(M+N \gg n\)), la probabilità di estrarre un oggetto funzionante rimane quasi costante durante il processo di estrazione, rendendo le prove quasi indipendenti. In questo caso, la distribuzione ipergeometrica può essere ben approssimata dalla distribuzione binomiale \(B(n, \frac{M}{M+N})\).
</p>

<ul class="org-ul">
<li><b>Valore atteso:</b> \(E[X] = n \cdot \frac{M}{M+N}\)</li>
<li><b>Varianza:</b> \(Var(X) = n \cdot \frac{M}{M+N} \cdot (1-\frac{M}{M+N}) \cdot \frac{M+N-n}{M+N-1}\)</li>
</ul>

<p>
La funzione di ripartizione è:
\[F_X(x) = P(X \leq x) = \sum_{i=max(0,n-N)}^{min(\lfloor x \rfloor, min(n,M))} \frac{\binom{M}{i} \binom{N}{n-i}}{\binom{M+N}{n}}\]
</p>

<p>
<b>Relazione con altre distribuzioni:</b>
</p>
<ul class="org-ul">
<li>Se \(n\) è piccolo rispetto a \(M+N\), la distribuzione ipergeometrica può essere approssimata da una distribuzione binomiale \(B(n, \frac{M}{M+N})\)</li>
<li>A differenza della binomiale, le estrazioni non sono indipendenti (senza rimpiazzo)</li>
<li>Il fattore \(\frac{M+N-n}{M+N-1}\) nella varianza è chiamato "fattore di correzione per popolazione finita"n (non visto a lezione)</li>
</ul>

<p>
<b><b>Domande per approfondire:</b></b>
</p>
<ol class="org-ol">
<li><p>
Come si comporta la varianza della distribuzione ipergeometrica quando \(n\) si avvicina a \(M+N\)?
</p>

<p>
La varianza tende a 0, infatti nel caso limite di \(n=M+N\), il numero di oggetti funzionanti sarà sempre \(M\) e quindi tale probabilità è \(1\) e tutto il resto \(0\).
</p></li>
<li>Perché il fattore di correzione per popolazione finita è sempre minore o uguale a 1?</li>
<li><p>
Come cambierebbe il modello se fosse possibile rimettere gli oggetti nella popolazione dopo ogni estrazione?
</p>

<div class="org-src-container">
<pre class="src src-python"> <span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
<span style="color: #a020f0;">import</span> matplotlib.pyplot <span style="color: #a020f0;">as</span> plt
<span style="color: #a020f0;">import</span> scipy.stats <span style="color: #a020f0;">as</span> st
<span style="color: #a020f0;">from</span> scipy.special <span style="color: #a020f0;">import</span> comb

<span style="color: #b22222;"># </span><span style="color: #b22222;">Parametri della distribuzione ipergeometrica</span>
<span style="color: #a0522d;">M</span> = 20  <span style="color: #b22222;"># </span><span style="color: #b22222;">numero di oggetti funzionanti</span>
<span style="color: #a0522d;">N</span> = 30  <span style="color: #b22222;"># </span><span style="color: #b22222;">numero di oggetti guasti</span>
<span style="color: #a0522d;">n</span> = 10  <span style="color: #b22222;"># </span><span style="color: #b22222;">numero di estrazioni</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Creazione dell'oggetto distribuzione ipergeometrica</span>
<span style="color: #b22222;"># </span><span style="color: #b22222;">scipy.stats.hypergeom usa parametri (M+N, M, n)</span>
<span style="color: #a0522d;">hypergeom_dist</span> = st.hypergeom(M=M+N, n=M, N=n)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo del supporto</span>
<span style="color: #a0522d;">lower_bound</span> = <span style="color: #483d8b;">max</span>(0, n-N)
<span style="color: #a0522d;">upper_bound</span> = <span style="color: #483d8b;">min</span>(n, M)
<span style="color: #a0522d;">support</span> = np.arange(lower_bound, upper_bound+1)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Funzione personalizzata per calcolare la PMF</span>
<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">p_hypergeom</span>(k, M, N, n):
    <span style="color: #8b2252;">"""Calcola la PMF di una distribuzione ipergeometrica"""</span>
    <span style="color: #a020f0;">if</span> k &lt; <span style="color: #483d8b;">max</span>(0, n-N) <span style="color: #a020f0;">or</span> k &gt; <span style="color: #483d8b;">min</span>(n, M):
        <span style="color: #a020f0;">return</span> 0
    <span style="color: #a020f0;">return</span> (comb(M, k) * comb(N, n-k)) / comb(M+N, n)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo della PMF</span>
<span style="color: #a0522d;">pmf_custom</span> = [p_hypergeom(k, M, N, n) <span style="color: #a020f0;">for</span> k <span style="color: #a020f0;">in</span> support]
<span style="color: #a0522d;">pmf_scipy</span> = hypergeom_dist.pmf(support)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Verifica che le due implementazioni diano risultati simili</span>
<span style="color: #483d8b;">print</span>(<span style="color: #8b2252;">"Confronto tra implementazione personalizzata e scipy.stats:"</span>)
<span style="color: #a020f0;">for</span> i, k <span style="color: #a020f0;">in</span> <span style="color: #483d8b;">enumerate</span>(support):
    <span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X=</span>{k}<span style="color: #8b2252;">) = </span>{pmf_custom[i]:.6f}<span style="color: #8b2252;"> (custom) vs </span>{pmf_scipy[i]:.6f}<span style="color: #8b2252;"> (scipy)"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo della CDF</span>
<span style="color: #a0522d;">cdf_values</span> = hypergeom_dist.cdf(support)
<span style="color: #483d8b;">print</span>(<span style="color: #8b2252;">"</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">Valori della CDF:"</span>)
<span style="color: #a020f0;">for</span> i, k <span style="color: #a020f0;">in</span> <span style="color: #483d8b;">enumerate</span>(support):
    <span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X&#8804;</span>{k}<span style="color: #8b2252;">) = </span>{cdf_values[i]:.6f}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo dei momenti</span>
<span style="color: #a0522d;">mean</span> = hypergeom_dist.mean()
<span style="color: #a0522d;">var</span> = hypergeom_dist.var()
<span style="color: #a0522d;">theo_mean</span> = n * M / (M+N)
<span style="color: #a0522d;">theo_var</span> = n * (M/(M+N)) * (1-M/(M+N)) * ((M+N-n)/(M+N-1))

<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">Valore atteso: </span>{mean:.6f}<span style="color: #8b2252;"> (teoricamente: </span>{theo_mean:.6f}<span style="color: #8b2252;">)"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Varianza: </span>{var:.6f}<span style="color: #8b2252;"> (teoricamente: </span>{theo_var:.6f}<span style="color: #8b2252;">)"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Generazione di variabili aleatorie con distribuzione ipergeometrica</span>
<span style="color: #a0522d;">sample</span> = hypergeom_dist.rvs(size=1000)
<span style="color: #a0522d;">sample_mean</span> = sample.mean()
<span style="color: #a0522d;">sample_var</span> = sample.var()
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">Media campionaria: </span>{sample_mean:.6f}<span style="color: #8b2252;">"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Varianza campionaria: </span>{sample_var:.6f}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Visualizzazione della distribuzione ipergeometrica</span>
plt.figure(figsize=(10, 5))

<span style="color: #b22222;"># </span><span style="color: #b22222;">Grafico della PMF</span>
plt.subplot(1, 2, 1)
plt.bar(support, pmf_scipy, width=0.4, alpha=0.7)
plt.axvline(x=mean, color=<span style="color: #8b2252;">'r'</span>, linestyle=<span style="color: #8b2252;">'--'</span>, 
            label=f<span style="color: #8b2252;">'Media = </span>{mean:.2f}<span style="color: #8b2252;">'</span>)
plt.xlabel(<span style="color: #8b2252;">'Numero di oggetti funzionanti estratti'</span>)
plt.ylabel(<span style="color: #8b2252;">'Probabilit&#224;'</span>)
plt.title(f<span style="color: #8b2252;">'PMF - Ipergeometrica(M=</span>{M}<span style="color: #8b2252;">, N=</span>{N}<span style="color: #8b2252;">, n=</span>{n}<span style="color: #8b2252;">)'</span>)
plt.grid(alpha=0.3)
plt.legend()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Grafico della CDF</span>
plt.subplot(1, 2, 2)
plt.step(support, cdf_values, where=<span style="color: #8b2252;">'post'</span>, color=<span style="color: #8b2252;">'green'</span>)
plt.axvline(x=mean, color=<span style="color: #8b2252;">'r'</span>, linestyle=<span style="color: #8b2252;">'--'</span>, 
            label=f<span style="color: #8b2252;">'Media = </span>{mean:.2f}<span style="color: #8b2252;">'</span>)
plt.xlabel(<span style="color: #8b2252;">'x'</span>)
plt.ylabel(<span style="color: #8b2252;">'P(X&#8804;x)'</span>)
plt.title(<span style="color: #8b2252;">'CDF - Ipergeometrica'</span>)
plt.grid(alpha=0.3)
plt.legend()

plt.tight_layout()
plt.show()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Confronto con la distribuzione binomiale approssimante</span>
<span style="color: #a0522d;">p_approx</span> = M / (M+N)
<span style="color: #a0522d;">binom_approx</span> = st.binom(n, p_approx)
<span style="color: #a0522d;">x_binom</span> = np.arange(0, n+1)
<span style="color: #a0522d;">pmf_binom</span> = binom_approx.pmf(x_binom)

plt.figure(figsize=(12, 5))
plt.bar(support, pmf_scipy, width=0.4, alpha=0.7, label=<span style="color: #8b2252;">'Ipergeometrica'</span>)
plt.plot(x_binom, pmf_binom, <span style="color: #8b2252;">'ro-'</span>, label=<span style="color: #8b2252;">'Binomiale approssimante'</span>)
plt.axvline(x=mean, color=<span style="color: #8b2252;">'k'</span>, linestyle=<span style="color: #8b2252;">'--'</span>, 
            label=f<span style="color: #8b2252;">'Media = </span>{mean:.2f}<span style="color: #8b2252;">'</span>)
plt.xlabel(<span style="color: #8b2252;">'Numero di oggetti funzionanti estratti'</span>)
plt.ylabel(<span style="color: #8b2252;">'Probabilit&#224;'</span>)
plt.title(<span style="color: #8b2252;">'Confronto tra Ipergeometrica e Binomiale approssimante'</span>)
plt.grid(alpha=0.3)
plt.legend()
plt.show()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Visualizzazione dell'effetto del rapporto n/(M+N) sull'approssimazione binomiale</span>
<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">compare_hypergeom_binom</span>(M, N, n):
    <span style="color: #8b2252;">"""Confronta la distribuzione ipergeometrica con la sua approssimazione binomiale"""</span>
    <span style="color: #a0522d;">hypergeom_dist</span> = st.hypergeom(M=M+N, n=M, N=n)
    <span style="color: #a0522d;">p_approx</span> = M / (M+N)
    <span style="color: #a0522d;">binom_approx</span> = st.binom(n, p_approx)

    <span style="color: #a0522d;">support</span> = np.arange(<span style="color: #483d8b;">max</span>(0, n-N), <span style="color: #483d8b;">min</span>(n, M)+1)
    <span style="color: #a0522d;">pmf_hypergeom</span> = hypergeom_dist.pmf(support)

    <span style="color: #a0522d;">x_binom</span> = np.arange(0, n+1)
    <span style="color: #a0522d;">pmf_binom</span> = binom_approx.pmf(x_binom)

    plt.figure(figsize=(10, 5))
    plt.bar(support, pmf_hypergeom, width=0.4, alpha=0.7, label=<span style="color: #8b2252;">'Ipergeometrica'</span>)
    plt.plot(x_binom, pmf_binom, <span style="color: #8b2252;">'ro-'</span>, label=<span style="color: #8b2252;">'Binomiale approssimante'</span>)
    plt.xlabel(<span style="color: #8b2252;">'Numero di oggetti funzionanti estratti'</span>)
    plt.ylabel(<span style="color: #8b2252;">'Probabilit&#224;'</span>)
    plt.title(f<span style="color: #8b2252;">'Confronto: M=</span>{M}<span style="color: #8b2252;">, N=</span>{N}<span style="color: #8b2252;">, n=</span>{n}<span style="color: #8b2252;">, rapporto n/(M+N)=</span>{n/(M+N):.4f}<span style="color: #8b2252;">'</span>)
    plt.grid(alpha=0.3)
    plt.legend()
    plt.show()

    <span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo della divergenza di Kullback-Leibler (solo per i valori nel supporto dell'ipergeometrica)</span>
    <span style="color: #a0522d;">kl_div</span> = 0
    <span style="color: #a020f0;">for</span> i, k <span style="color: #a020f0;">in</span> <span style="color: #483d8b;">enumerate</span>(support):
        <span style="color: #a0522d;">p_hyper</span> = pmf_hypergeom[i]
        <span style="color: #a0522d;">p_binom</span> = binom_approx.pmf(k)
        <span style="color: #a020f0;">if</span> p_hyper &gt; 0 <span style="color: #a020f0;">and</span> p_binom &gt; 0:
            <span style="color: #a0522d;">kl_div</span> += p_hyper * np.log(p_hyper / p_binom)

    <span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Divergenza KL: </span>{kl_div:.6f}<span style="color: #8b2252;"> (pi&#249; vicino a 0 = migliore approssimazione)"</span>)
    <span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Rapporto n/(M+N): </span>{n/(M+N):.6f}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Esempio con alto rapporto n/(M+N)</span>
compare_hypergeom_binom(M=20, N=30, n=40)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Esempio con basso rapporto n/(M+N)</span>
compare_hypergeom_binom(M=200, N=300, n=40)
</pre>
</div></li>
</ol>
</div>
</div>
<div id="outline-container-orge9c02b0" class="outline-4">
<h4 id="orge9c02b0">Continuo uniforme</h4>
<div class="outline-text-4" id="text-orge9c02b0">
<p>
<b>Definizione (Distribuzione Uniforme Continua):</b> Una variabile aleatoria \(X\) segue una distribuzione uniforme continua sull'intervallo \([a,b]\) (scriviamo \(X \sim \text{Unif}(a,b)\)) se la sua densità di probabilità è costante sull'intervallo \([a,b]\).
</p>

<p>
La funzione di densità di probabilità è:
\[f_X(x) = \frac{1}{b-a} \mathbb{I}_{[a,b]}(x)\]
</p>

<p>
<b>Calcolo delle probabilità mediante integrazione:</b>
La probabilità che \(X\) appartenga a un intervallo \(I \subseteq [a,b]\) si calcola mediante l'integrale:
\[P(X \in I) = \int_I f_X(x) \, dx = \int_I \frac{1}{b-a} \, dx = \frac{1}{b-a} \int_I \, dx = \frac{|I|}{b-a}\]
</p>

<p>
dove \(|I|\) rappresenta la misura (lunghezza) dell'intervallo \(I\). Questo dimostra che la probabilità dipende solo dalla lunghezza dell'intervallo considerato, non dal valore specifico di \(x\) all'interno di esso - una caratteristica fondamentale della distribuzione uniforme.
</p>

<p>
<b>Verifica della normalizzazione:</b>
Integrando la densità su tutto lo spazio campionario:
\[\int_{-\infty}^{+\infty} f_X(x) \, dx = \int_{-\infty}^{a} f_X(x) \, dx + \int_{a}^{b} f_X(x) \, dx + \int_{b}^{+\infty} f_X(x) \, dx\]
</p>

<p>
Poiché \(f_X(x) = 0\) per \(x < a\) e \(x > b\):
\[\int_{-\infty}^{+\infty} f_X(x) \, dx = \int_{a}^{b} \frac{1}{b-a} \, dx = \frac{1}{b-a} \cdot (b-a) = 1\]
</p>

<p>
Questo risultato è coerente con l'interpretazione geometrica: la densità forma un rettangolo di altezza \(\frac{1}{b-a}\) e base \((b-a)\), con area totale pari a 1.
</p>

<p>
<b>Proprietà principali:</b>
</p>
<ul class="org-ul">
<li><b>Supporto:</b> \(X \in [a,b]\)</li>
<li><b>Valore atteso:</b> \(E[X] = \frac{a+b}{2}\)</li>
<li><b>Varianza:</b> \(Var(X) = \frac{(b-a)^2}{12}\)</li>
</ul>

<p>
La funzione di ripartizione è:
\[F_X(x) = \frac{x-a}{b-a} \mathbb{I}_{[a,b]}(x) + \mathbb{I}_{(b,\infty)}(x)\]
</p>

<p>
che equivale a:
\[F_X(x) = \begin{cases}
0 & \text{se } x < a \\
\frac{x-a}{b-a} & \text{se } a \leq x \leq b \\
1 & \text{se } x > b
\end{cases}\]
</p>

<p>
<b>Proprietà aggiuntive:</b>
</p>
<ul class="org-ul">
<li>Se \(X \sim \text{Unif}(a,b)\), allora \(Y = c \cdot X + d \sim \text{Unif}(c \cdot a + d, c \cdot b + d)\) per \(c > 0\)</li>
<li>La distribuzione uniforme rappresenta la situazione di massima incertezza all'interno di un intervallo limitato</li>
<li>Il valore atteso coincide con il punto medio dell'intervallo</li>
</ul>

<p>
<b>Applicazioni:</b>
</p>
<ul class="org-ul">
<li>Modellazione di errori di arrotondamento</li>
<li>Generazione di numeri pseudo-casuali</li>
<li>Modellazione di situazioni in cui qualsiasi valore in un intervallo ha uguale probabilità di verificarsi</li>
</ul>


<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
<span style="color: #a020f0;">import</span> matplotlib.pyplot <span style="color: #a020f0;">as</span> plt
<span style="color: #a020f0;">import</span> scipy.stats <span style="color: #a020f0;">as</span> st
<span style="color: #a020f0;">from</span> ipywidgets <span style="color: #a020f0;">import</span> interact, FloatSlider

<span style="color: #b22222;"># </span><span style="color: #b22222;">Parametri della distribuzione uniforme continua</span>
<span style="color: #a0522d;">a</span> = 2
<span style="color: #a0522d;">b</span> = 5

<span style="color: #b22222;"># </span><span style="color: #b22222;">Creazione dell'oggetto distribuzione uniforme continua</span>
<span style="color: #a0522d;">unif_cont</span> = st.uniform(loc=a, scale=b-a)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Funzioni personalizzate per calcolare PDF e CDF</span>
<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">f_unif</span>(x, a, b):
    <span style="color: #8b2252;">"""Calcola la PDF di una uniforme continua"""</span>
    <span style="color: #a020f0;">if</span> a &lt;= x &lt;= b:
        <span style="color: #a020f0;">return</span> 1/(b-a)
    <span style="color: #a020f0;">else</span>:
        <span style="color: #a020f0;">return</span> 0

<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">F_unif</span>(x, a, b):
    <span style="color: #8b2252;">"""Calcola la CDF di una uniforme continua"""</span>
    <span style="color: #a020f0;">if</span> x &lt; a:
        <span style="color: #a020f0;">return</span> 0
    <span style="color: #a020f0;">elif</span> x &gt; b:
        <span style="color: #a020f0;">return</span> 1
    <span style="color: #a020f0;">else</span>:
        <span style="color: #a020f0;">return</span> (x-a)/(b-a)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo della PDF e CDF per diversi valori</span>
<span style="color: #a0522d;">x_values</span> = np.linspace(a-1, b+1, 1000)
<span style="color: #a0522d;">pdf_custom</span> = [f_unif(x, a, b) <span style="color: #a020f0;">for</span> x <span style="color: #a020f0;">in</span> x_values]
<span style="color: #a0522d;">pdf_scipy</span> = unif_cont.pdf(x_values)
<span style="color: #a0522d;">cdf_custom</span> = [F_unif(x, a, b) <span style="color: #a020f0;">for</span> x <span style="color: #a020f0;">in</span> x_values]
<span style="color: #a0522d;">cdf_scipy</span> = unif_cont.cdf(x_values)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo di momenti e statistiche</span>
<span style="color: #a0522d;">mean</span> = unif_cont.mean()
<span style="color: #a0522d;">var</span> = unif_cont.var()
<span style="color: #a0522d;">theo_mean</span> = (a+b)/2
<span style="color: #a0522d;">theo_var</span> = (b-a)**2/12

<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Valore atteso: </span>{mean}<span style="color: #8b2252;"> (teoricamente: </span>{theo_mean}<span style="color: #8b2252;">)"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Varianza: </span>{var}<span style="color: #8b2252;"> (teoricamente: </span>{theo_var}<span style="color: #8b2252;">)"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Generazione di variabili aleatorie con distribuzione uniforme continua</span>
<span style="color: #a0522d;">sample</span> = unif_cont.rvs(size=1000)
<span style="color: #a0522d;">sample_mean</span> = sample.mean()
<span style="color: #a0522d;">sample_var</span> = sample.var()
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Media campionaria: </span>{sample_mean}<span style="color: #8b2252;">"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Varianza campionaria: </span>{sample_var}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo di probabilit&#224; specifiche</span>
<span style="color: #a0522d;">point1</span> = (a + b)/4
<span style="color: #a0522d;">point2</span> = 3*(a + b)/4
<span style="color: #a0522d;">prob_between</span> = F_unif(point2, a, b) - F_unif(point1, a, b)
<span style="color: #a0522d;">prob_between_scipy</span> = unif_cont.cdf(point2) - unif_cont.cdf(point1)

<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(</span>{point1}<span style="color: #8b2252;"> &lt; X &lt; </span>{point2}<span style="color: #8b2252;">) = </span>{prob_between}<span style="color: #8b2252;"> (custom)"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(</span>{point1}<span style="color: #8b2252;"> &lt; X &lt; </span>{point2}<span style="color: #8b2252;">) = </span>{prob_between_scipy}<span style="color: #8b2252;"> (scipy)"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Visualizzazione della distribuzione uniforme continua</span>
plt.figure(figsize=(12, 5))

<span style="color: #b22222;"># </span><span style="color: #b22222;">Grafico della PDF</span>
plt.subplot(1, 2, 1)
plt.plot(x_values, pdf_scipy, <span style="color: #8b2252;">'b-'</span>, label=<span style="color: #8b2252;">'PDF'</span>)
plt.axvline(x=mean, color=<span style="color: #8b2252;">'r'</span>, linestyle=<span style="color: #8b2252;">'--'</span>, label=f<span style="color: #8b2252;">'Media = </span>{mean}<span style="color: #8b2252;">'</span>)
plt.axhline(y=1/(b-a), color=<span style="color: #8b2252;">'g'</span>, linestyle=<span style="color: #8b2252;">'-.'</span>)
plt.fill_between(x_values, pdf_scipy, alpha=0.2)
plt.xlim(a-0.5, b+0.5)
plt.ylim(-0.05, 1/(b-a)*1.2)
plt.xlabel(<span style="color: #8b2252;">'x'</span>)
plt.ylabel(<span style="color: #8b2252;">'f(x)'</span>)
plt.title(f<span style="color: #8b2252;">'PDF - Uniforme Continua [</span>{a}<span style="color: #8b2252;">, </span>{b}<span style="color: #8b2252;">]'</span>)
plt.grid(alpha=0.3)
plt.legend()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Grafico della CDF</span>
plt.subplot(1, 2, 2)
plt.plot(x_values, cdf_scipy, <span style="color: #8b2252;">'g-'</span>, label=<span style="color: #8b2252;">'CDF'</span>)
plt.axvline(x=mean, color=<span style="color: #8b2252;">'r'</span>, linestyle=<span style="color: #8b2252;">'--'</span>, label=f<span style="color: #8b2252;">'Media = </span>{mean}<span style="color: #8b2252;">'</span>)
plt.xlim(a-0.5, b+0.5)
plt.ylim(-0.05, 1.05)
plt.xlabel(<span style="color: #8b2252;">'x'</span>)
plt.ylabel(<span style="color: #8b2252;">'F(x)'</span>)
plt.title(f<span style="color: #8b2252;">'CDF - Uniforme Continua [</span>{a}<span style="color: #8b2252;">, </span>{b}<span style="color: #8b2252;">]'</span>)
plt.grid(alpha=0.3)
plt.legend()

plt.tight_layout()
plt.show()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Visualizzazione di un istogramma del campione generato</span>
plt.figure(figsize=(10, 5))
plt.hist(sample, bins=30, density=<span style="color: #008b8b;">True</span>, alpha=0.7, label=<span style="color: #8b2252;">'Campione'</span>)
plt.plot(x_values, pdf_scipy, <span style="color: #8b2252;">'r-'</span>, label=<span style="color: #8b2252;">'PDF teorica'</span>)
plt.axvline(x=mean, color=<span style="color: #8b2252;">'k'</span>, linestyle=<span style="color: #8b2252;">'--'</span>, label=f<span style="color: #8b2252;">'Media = </span>{mean}<span style="color: #8b2252;">'</span>)
plt.xlim(a-0.5, b+0.5)
plt.xlabel(<span style="color: #8b2252;">'Valore'</span>)
plt.ylabel(<span style="color: #8b2252;">'Densit&#224;'</span>)
plt.title(f<span style="color: #8b2252;">'Istogramma di 1000 campioni da Uniforme Continua [</span>{a}<span style="color: #8b2252;">, </span>{b}<span style="color: #8b2252;">]'</span>)
plt.grid(alpha=0.3)
plt.legend()
plt.show()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Funzione per visualizzare la distribuzione con parametri variabili</span>
<span style="color: #a020f0;">def</span> <span style="color: #0000ff;">visualizza_unif_cont</span>(a=0, b=1):
    <span style="color: #8b2252;">"""Funzione per visualizzare la distribuzione uniforme continua con parametri variabili"""</span>
    <span style="color: #a020f0;">if</span> a &gt;= b:
        <span style="color: #483d8b;">print</span>(<span style="color: #8b2252;">"Errore: deve essere a &lt; b"</span>)
        <span style="color: #a020f0;">return</span>

    <span style="color: #a0522d;">unif_cont</span> = st.uniform(loc=a, scale=b-a)
    <span style="color: #a0522d;">x_values</span> = np.linspace(a-1, b+1, 1000)
    <span style="color: #a0522d;">pdf_values</span> = unif_cont.pdf(x_values)
    <span style="color: #a0522d;">cdf_values</span> = unif_cont.cdf(x_values)
    <span style="color: #a0522d;">mean</span> = unif_cont.mean()
    <span style="color: #a0522d;">std</span> = np.sqrt(unif_cont.var())

    plt.figure(figsize=(12, 5))

    <span style="color: #b22222;"># </span><span style="color: #b22222;">Grafico della PDF</span>
    plt.subplot(1, 2, 1)
    plt.plot(x_values, pdf_values, <span style="color: #8b2252;">'b-'</span>, label=<span style="color: #8b2252;">'PDF'</span>)
    plt.axvline(x=mean, color=<span style="color: #8b2252;">'r'</span>, linestyle=<span style="color: #8b2252;">'--'</span>, label=f<span style="color: #8b2252;">'Media = </span>{mean:.2f}<span style="color: #8b2252;">'</span>)
    plt.fill_between(x_values, pdf_values, alpha=0.2)
    plt.xlim(a-0.5, b+0.5)
    plt.ylim(-0.05, 1/(b-a)*1.2)
    plt.xlabel(<span style="color: #8b2252;">'x'</span>)
    plt.ylabel(<span style="color: #8b2252;">'f(x)'</span>)
    plt.title(f<span style="color: #8b2252;">'PDF - Uniforme Continua [</span>{a}<span style="color: #8b2252;">, </span>{b}<span style="color: #8b2252;">]'</span>)
    plt.grid(alpha=0.3)
    plt.legend()

    <span style="color: #b22222;"># </span><span style="color: #b22222;">Grafico della CDF</span>
    plt.subplot(1, 2, 2)
    plt.plot(x_values, cdf_values, <span style="color: #8b2252;">'g-'</span>, label=<span style="color: #8b2252;">'CDF'</span>)
    plt.axvline(x=mean, color=<span style="color: #8b2252;">'r'</span>, linestyle=<span style="color: #8b2252;">'--'</span>, label=f<span style="color: #8b2252;">'Media = </span>{mean:.2f}<span style="color: #8b2252;">'</span>)
    plt.xlim(a-0.5, b+0.5)
    plt.ylim(-0.05, 1.05)
    plt.xlabel(<span style="color: #8b2252;">'x'</span>)
    plt.ylabel(<span style="color: #8b2252;">'F(x)'</span>)
    plt.title(f<span style="color: #8b2252;">'CDF - Uniforme Continua [</span>{a}<span style="color: #8b2252;">, </span>{b}<span style="color: #8b2252;">]'</span>)
    plt.grid(alpha=0.3)
    plt.legend()

    plt.tight_layout()
    plt.show()

    <span style="color: #b22222;"># </span><span style="color: #b22222;">Statistiche</span>
    <span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Valore atteso: </span>{mean:.4f}<span style="color: #8b2252;">"</span>)
    <span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Deviazione standard: </span>{std:.4f}<span style="color: #8b2252;">"</span>)
    <span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Varianza: </span>{unif_cont.var():.4f}<span style="color: #8b2252;">"</span>)
    <span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"PDF costante: </span>{1/(b-a):.4f}<span style="color: #8b2252;">"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Esempio di utilizzo della funzione</span>
visualizza_unif_cont(a=2, b=5)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Implementazione dell'interfaccia interattiva (decommentare per utilizzarla)</span>
<span style="color: #b22222;"># </span><span style="color: #b22222;">interact(visualizza_unif_cont, a=FloatSlider(min=-5, max=5, step=0.5, value=0), </span>
<span style="color: #b22222;">#          </span><span style="color: #b22222;">b=FloatSlider(min=-4, max=10, step=0.5, value=1))</span>
</pre>
</div>
</div>
</div>
<div id="outline-container-org59e2742" class="outline-4">
<h4 id="org59e2742">Esponenziale</h4>
<div class="outline-text-4" id="text-org59e2742">
<p>
<b>Definizione (Distribuzione Esponenziale):</b> Una variabile aleatoria \(X\) segue una distribuzione esponenziale con parametro \(\lambda > 0\) (scriviamo \(X \sim \text{Exp}(\lambda)\)) se rappresenta il tempo di attesa fino al verificarsi del primo evento in un processo di Poisson con tasso \(\lambda\). Il supporto della distribuzione è l'insieme dei numeri reali positivi, \(\mathbb{R}^+\).
</p>

<p>
La funzione di densità di probabilità è:
\[f_X(x) = \lambda e^{-\lambda x} \mathbb{I}_{[0,\infty)}(x)\]
</p>

<p>
<b>Verifica della normalizzazione:</b>
Per verificare che la densità integri a 1, calcoliamo:
</p>
\begin{align}
\int_{0}^{\infty} f_X(x) \, dx &= \int_{0}^{\infty} \lambda e^{-\lambda x} \, dx
\end{align}

<p>
Utilizzando la sostituzione \(y = \lambda x\) (quindi \(dx = \frac{dy}{\lambda}\)), otteniamo:
</p>
\begin{align}
\int_{0}^{\infty} \lambda e^{-\lambda x} \, dx &= \int_{0}^{\infty} \lambda e^{-y} \frac{dy}{\lambda} \\
&= \int_{0}^{\infty} e^{-y} \, dy \\
&= [-e^{-y}]_{0}^{\infty} \\
&= 0 - (-1) = 1
\end{align}

<p>
<b>Proprietà principali:</b>
</p>
<ul class="org-ul">
<li><b>Supporto:</b> \(X \in [0,\infty)\)</li>
<li><b>Valore atteso:</b> \(E[X] = \frac{1}{\lambda}\)</li>
<li><b>Varianza:</b> \(Var(X) = \frac{1}{\lambda^2}\)</li>
</ul>

<p>
La funzione di ripartizione è:
\[F_X(x) = \begin{cases}
0 & \text{se } x < 0 \\
1 - e^{-\lambda x} & \text{se } x \geq 0
\end{cases}\]
</p>

<p>
<b>Proprietà del massimo e minimo di variabili esponenziali:</b>
</p>

<p>
Siano \(X_1, X_2, \ldots, X_n\) variabili aleatorie indipendenti e identicamente distribuite (i.i.d.) con distribuzione \(\text{Exp}(\lambda)\).
</p>

<ol class="org-ol">
<li><p>
<b>Distribuzione del massimo:</b> 
</p>

<p>
Definiamo \(Y = \max(X_1, X_2, \ldots, X_n)\). La funzione di ripartizione di \(Y\) è:
</p>
\begin{align}
F_Y(x) &= P(Y \leq x) \\
&= P(\max(X_1, X_2, \ldots, X_n) \leq x) \\
&= P(X_1 \leq x, X_2 \leq x, \ldots, X_n \leq x)
\end{align}

<p>
Poiché le variabili sono indipendenti, possiamo scrivere:
</p>
\begin{align}
F_Y(x) &= P(X_1 \leq x) \cdot P(X_2 \leq x) \cdot \ldots \cdot P(X_n \leq x) \\
&= F_X(x)^n \\
&= (1 - e^{-\lambda x})^n \quad \text{per } x \geq 0
\end{align}</li>

<li><p>
<b>Distribuzione del minimo:</b>
</p>

<p>
Definiamo \(Z = \min(X_1, X_2, \ldots, X_n)\). La funzione di ripartizione di \(Z\) è:
</p>
\begin{align}
F_Z(x) &= P(Z \leq x) \\
&= P(\min(X_1, X_2, \ldots, X_n) \leq x) \\
&= 1 - P(\min(X_1, X_2, \ldots, X_n) > x) \\
&= 1 - P(X_1 > x, X_2 > x, \ldots, X_n > x)
\end{align}

<p>
Sfruttando l'indipendenza:
</p>
\begin{align}
F_Z(x) &= 1 - P(X_1 > x) \cdot P(X_2 > x) \cdot \ldots \cdot P(X_n > x) \\
&= 1 - (1 - F_X(x))^n \\
&= 1 - (1 - (1 - e^{-\lambda x}))^n \\
&= 1 - (e^{-\lambda x})^n \\
&= 1 - e^{-n\lambda x} \quad \text{per } x \geq 0
\end{align}

<p>
Questo dimostra che \(Z \sim \text{Exp}(n\lambda)\). Un risultato notevole: il minimo di \(n\) variabili esponenziali i.i.d. è ancora una variabile esponenziale con parametro moltiplicato per \(n\).
</p></li>
</ol>

<p>
(Un po' diverso dai fogli, da controllare)
</p>

<p>
<b>Proprietà di scala:</b>
</p>

<p>
Se \(X \sim \text{Exp}(\lambda)\) e \(Y = cX\) con \(c > 0\), allora \(Y \sim \text{Exp}(\lambda/c)\).
</p>

<p>
<b>Dimostrazione:</b>
</p>
\begin{align}
F_Y(x) &= P(Y \leq x) \\
&= P(cX \leq x) \\
&= P\left(X \leq \frac{x}{c}\right) \\
&= F_X\left(\frac{x}{c}\right) \\
&= 1 - e^{-\lambda \frac{x}{c}} \\
&= 1 - e^{-\frac{\lambda}{c}x}
\end{align}

<p>
Questo corrisponde alla funzione di ripartizione di una variabile esponenziale con parametro \(\frac{\lambda}{c}\).
</p>

<p>
<b>Proprietà di assenza di memoria:</b> Come per la distribuzione geometrica (suo analogo discreto), anche la distribuzione esponenziale gode della proprietà di assenza di memoria:
\[P(X > s+t | X > s) = P(X > t)\]
</p>

<p>
<b>Dimostrazione:</b>
</p>
\begin{align}
P(X > s+t | X > s) &= \frac{P(X > s+t)}{P(X > s)} \\
&= \frac{e^{-\lambda(s+t)}}{e^{-\lambda s}} \\
&= e^{-\lambda t} \\
&= P(X > t)
\end{align}

<p>
<b>Relazione con altre distribuzioni:</b>
</p>
<ul class="org-ul">
<li>Se \(X_1, X_2, \ldots, X_n\) sono variabili aleatorie esponenziali indipendenti con lo stesso parametro \(\lambda\), allora la loro somma segue una distribuzione Gamma con parametri \(n\) e \(\lambda\)</li>
<li>La distribuzione esponenziale è un caso particolare della distribuzione Gamma con parametro di forma uguale a 1</li>
</ul>

<p>
<b><b>Domande di approfondimento:</b></b>
</p>
<ol class="org-ol">
<li>Perché la distribuzione esponenziale è l'unica distribuzione continua con la proprietà di assenza di memoria?</li>
<li>Come cambierebbe il risultato sul minimo se le variabili esponenziali avessero parametri diversi?</li>
<li>Qual è l'interpretazione del parametro \(\lambda\) in termini di tasso di occorrenza degli eventi?</li>
</ol>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
<span style="color: #a020f0;">import</span> matplotlib.pyplot <span style="color: #a020f0;">as</span> plt
<span style="color: #a020f0;">import</span> scipy.stats <span style="color: #a020f0;">as</span> st

<span style="color: #b22222;"># </span><span style="color: #b22222;">Parametri della distribuzione esponenziale</span>
<span style="color: #a0522d;">lambda_val</span> = 0.5  <span style="color: #b22222;"># </span><span style="color: #b22222;">parametro della distribuzione</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Creazione dell'oggetto distribuzione esponenziale</span>
<span style="color: #a0522d;">exp_dist</span> = st.expon(scale=1/lambda_val)  <span style="color: #b22222;"># </span><span style="color: #b22222;">scipy.stats.expon usa scale=1/lambda</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo della PDF e CDF per diversi valori</span>
<span style="color: #a0522d;">x_values</span> = np.linspace(-1, 10, 1000)
<span style="color: #a0522d;">pdf_values</span> = exp_dist.pdf(x_values)
<span style="color: #a0522d;">cdf_values</span> = exp_dist.cdf(x_values)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo di momenti e statistiche</span>
<span style="color: #a0522d;">mean</span> = exp_dist.mean()
<span style="color: #a0522d;">var</span> = exp_dist.var()
<span style="color: #a0522d;">std</span> = np.sqrt(var)
<span style="color: #a0522d;">theo_mean</span> = 1/lambda_val
<span style="color: #a0522d;">theo_var</span> = 1/(lambda_val**2)

<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Valore atteso: </span>{mean}<span style="color: #8b2252;"> (teoricamente: </span>{theo_mean}<span style="color: #8b2252;">)"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Varianza: </span>{var}<span style="color: #8b2252;"> (teoricamente: </span>{theo_var}<span style="color: #8b2252;">)"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Generazione di variabili aleatorie</span>
<span style="color: #a0522d;">sample</span> = exp_dist.rvs(size=1000)
<span style="color: #a0522d;">sample_mean</span> = sample.mean()
<span style="color: #a0522d;">sample_var</span> = sample.var()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo di probabilit&#224; specifiche</span>
<span style="color: #a0522d;">point1</span> = 1/lambda_val  <span style="color: #b22222;"># </span><span style="color: #b22222;">media</span>
<span style="color: #a0522d;">prob_less_than_mean</span> = 1 - np.exp(-1)  <span style="color: #b22222;"># </span><span style="color: #b22222;">P(X &lt; 1/&#955;) = 1 - e^(-1)</span>
<span style="color: #a0522d;">prob_greater_than_mean</span> = np.exp(-1)  <span style="color: #b22222;"># </span><span style="color: #b22222;">P(X &gt; 1/&#955;) = e^(-1)</span>

<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">P(X &lt; media) = </span>{prob_less_than_mean:.4f}<span style="color: #8b2252;"> (esattamente 1-1/e &#8776; 0.6321)"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X &gt; media) = </span>{prob_greater_than_mean:.4f}<span style="color: #8b2252;"> (esattamente 1/e &#8776; 0.3679)"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Visualizzazione della distribuzione</span>
plt.figure(figsize=(12, 5))

<span style="color: #b22222;"># </span><span style="color: #b22222;">Grafico della PDF</span>
plt.subplot(1, 2, 1)
plt.plot(x_values, pdf_values, <span style="color: #8b2252;">'b-'</span>, label=<span style="color: #8b2252;">'PDF'</span>)
plt.axvline(x=mean, color=<span style="color: #8b2252;">'r'</span>, linestyle=<span style="color: #8b2252;">'--'</span>, label=f<span style="color: #8b2252;">'Media = </span>{mean}<span style="color: #8b2252;">'</span>)
plt.fill_between(x_values, pdf_values, alpha=0.2)
plt.xlabel(<span style="color: #8b2252;">'x'</span>)
plt.ylabel(<span style="color: #8b2252;">'f(x)'</span>)
plt.title(f<span style="color: #8b2252;">'PDF - Esponenziale con &#955;=</span>{lambda_val}<span style="color: #8b2252;">'</span>)
plt.grid(alpha=0.3)
plt.legend()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Grafico della CDF</span>
plt.subplot(1, 2, 2)
plt.plot(x_values, cdf_values, <span style="color: #8b2252;">'g-'</span>, label=<span style="color: #8b2252;">'CDF'</span>)
plt.axvline(x=mean, color=<span style="color: #8b2252;">'r'</span>, linestyle=<span style="color: #8b2252;">'--'</span>, label=f<span style="color: #8b2252;">'Media = </span>{mean}<span style="color: #8b2252;">'</span>)
plt.axhline(y=1-np.exp(-1), color=<span style="color: #8b2252;">'orange'</span>, linestyle=<span style="color: #8b2252;">'-.'</span>, 
            label=f<span style="color: #8b2252;">'1-1/e &#8776; </span>{1-np.exp(-1):.4f}<span style="color: #8b2252;">'</span>)
plt.xlabel(<span style="color: #8b2252;">'x'</span>)
plt.ylabel(<span style="color: #8b2252;">'F(x)'</span>)
plt.title(f<span style="color: #8b2252;">'CDF - Esponenziale con &#955;=</span>{lambda_val}<span style="color: #8b2252;">'</span>)
plt.grid(alpha=0.3)
plt.legend()

plt.tight_layout()
plt.show()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Dimostrazione della propriet&#224; di assenza di memoria</span>
<span style="color: #a0522d;">s</span>, <span style="color: #a0522d;">t</span> = 2, 3
<span style="color: #a0522d;">p_greater_t</span> = np.exp(-lambda_val * t)
<span style="color: #a0522d;">p_greater_s</span> = np.exp(-lambda_val * s)
<span style="color: #a0522d;">p_greater_s_plus_t</span> = np.exp(-lambda_val * (s + t))
<span style="color: #a0522d;">p_conditional</span> = p_greater_s_plus_t / p_greater_s

<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">Dimostrazione della propriet&#224; di assenza di memoria:"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X &gt; </span>{t}<span style="color: #8b2252;">) = e^(-&#955;&#183;</span>{t}<span style="color: #8b2252;">) = </span>{p_greater_t:.6f}<span style="color: #8b2252;">"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(X &gt; </span>{s+t}<span style="color: #8b2252;"> | X &gt; </span>{s}<span style="color: #8b2252;">) = </span>{p_conditional:.6f}<span style="color: #8b2252;">"</span>)
</pre>
</div>
</div>
</div>
<div id="outline-container-org451db8e" class="outline-4">
<h4 id="org451db8e">Gaussiano</h4>
<div class="outline-text-4" id="text-org451db8e">
<p>
<b>Definizione (Distribuzione Gaussiana o Normale):</b> Una variabile aleatoria \(X\) segue una distribuzione gaussiana con parametri \(\mu\) (media) e \(\sigma > 0\) (<a href="#org6db3d4f">deviazione standard</a>), scriviamo \(X \sim \mathcal{G}(\mu, \sigma)\), se la sua funzione di densità di probabilità è:
</p>

<p>
\[f_X(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}\]
</p>

<p>
Il codominio della funzione di densità è \(\mathbb{R}^+\). Si noti che \(\lim_{x \to \pm\infty} f_X(x) = 0\), quindi la distribuzione è asintotica rispetto all'asse delle ascisse.
</p>

<p>
<b>Analisi dei massimi e minimi:</b>
Calcoliamo la derivata prima della funzione di densità:
</p>
\begin{align}
f'_X(x) &= \frac{d}{dx}\left[\frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}\right] \\
&= \frac{1}{\sigma\sqrt{2\pi}} \cdot e^{-\frac{(x-\mu)^2}{2\sigma^2}} \cdot \left(-\frac{2(x-\mu)}{2\sigma^2}\right) \\
&= -\frac{x-\mu}{\sigma^2} \cdot \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
\end{align}

<p>
Ponendo \(f'_X(x) = 0\):
\[-\frac{x-\mu}{\sigma^2} \cdot \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}} = 0\]
</p>

<p>
Poiché il fattore esponenziale è sempre positivo, l'equazione è soddisfatta solo quando \(x-\mu = 0\), quindi \(x = \mu\).
</p>

<p>
Inoltre:
</p>
<ul class="org-ul">
<li>\(f'_X(x) > 0\) quando \(x < \mu\) (funzione crescente)</li>
<li>\(f'_X(x) < 0\) quando \(x > \mu\) (funzione decrescente)</li>
</ul>

<p>
Quindi \(x = \mu\) è punto di massimo della funzione di densità.
</p>

<p>
<b>Analisi della convessità:</b>
Calcoliamo la derivata seconda:
</p>
\begin{align}
f''_X(x) &= \frac{d}{dx}\left[-\frac{x-\mu}{\sigma^2} \cdot \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}\right] \\
&= -\frac{1}{\sigma^2} \cdot \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}} + \frac{x-\mu}{\sigma^2} \cdot \frac{x-\mu}{\sigma^2} \cdot \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}} \\
&= \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}} \cdot \left(\frac{(x-\mu)^2}{\sigma^4} - \frac{1}{\sigma^2}\right)
\end{align}

<p>
Ponendo \(f''_X(x) = 0\):
\[\frac{(x-\mu)^2}{\sigma^4} - \frac{1}{\sigma^2} = 0\]
</p>

<p>
Risolvendo:
\[(x-\mu)^2 = \sigma^2\]
\[x = \mu \pm \sigma\]
</p>

<p>
Quindi abbiamo due punti di flesso in \(x = \mu - \sigma\) e \(x = \mu + \sigma\).
</p>

<p>
Inoltre:
</p>
<ul class="org-ul">
<li>\(f''_X(x) < 0\) quando \(\mu - \sigma < x < \mu + \sigma\) (funzione concava)</li>
<li>\(f''_X(x) > 0\) quando \(x < \mu - \sigma\) o \(x > \mu + \sigma\) (funzione convessa)</li>
</ul>

<p>
<b>Effetti dei parametri sul grafico:</b>
</p>
<ul class="org-ul">
<li>Cambiare \(\mu\) trasla orizzontalmente il grafico: aumentare \(\mu\) sposta la curva verso destra, diminuire \(\mu\) la sposta verso sinistra.</li>
<li>Cambiare \(\sigma\) modifica la forma della curva: 
<ul class="org-ul">
<li>Aumentando \(\sigma\), i punti di flesso \(\mu \pm \sigma\) si allontanano da \(\mu\)</li>
<li>Aumentando \(\sigma\), la curva diventa più bassa al centro (in \(\mu\)) e più alta nelle code, per mantenere l'area totale uguale a 1</li>
<li>Diminuendo \(\sigma\), la curva diventa più concentrata attorno a \(\mu\) e più ripida</li>
</ul></li>
</ul>

<p>
<b>Normalizzazione della densità:</b>
La dimostrazione che l'area totale sotto la curva di densità è uguale a 1, ovvero \(\int_{-\infty}^{+\infty} f_X(x) \, dx = 1\), è un calcolo complesso. L'approccio classico utilizza un integrale doppio e il passaggio alle coordinate polari:
\[\int_{-\infty}^{+\infty} e^{-\frac{x^2}{2}} \, dx = \sqrt{2\pi}\]
</p>

<p>
Questo risultato garantisce che la funzione di densità sia correttamente normalizzata.
</p>

<p>
<b>Trasformazioni lineari:</b>
Se \(X \sim \mathcal{G}(\mu, \sigma)\) e \(Y = aX + b\) con \(a \neq 0\), allora \(Y \sim \mathcal{G}(a\mu + b, |a|\sigma)\).
</p>

<p>
<b>Somma di variabili gaussiane indipendenti:</b>
Se \(X_1 \sim \mathcal{G}(\mu_1, \sigma_1)\) e \(X_2 \sim \mathcal{G}(\mu_2, \sigma_2)\) sono indipendenti, allora:
\[X_1 + X_2 \sim \mathcal{G}(\mu_1 + \mu_2, \sqrt{\sigma_1^2 + \sigma_2^2})\]
</p>

<p>
Questo risultato si estende alla somma di un numero arbitrario di variabili gaussiane indipendenti:
\[\sum_{i=1}^n X_i \sim \mathcal{G}\left(\sum_{i=1}^n \mu_i, \sqrt{\sum_{i=1}^n \sigma_i^2}\right)\]
</p>

<p>
È importante sottolineare che la linearità del valore atteso e l'additività delle varianze per variabili indipendenti valgono per qualsiasi distribuzione, non solo per le gaussiane. Tuttavia, la peculiarità della distribuzione gaussiana è la <b>riproducibilità</b>: la somma di variabili gaussiane indipendenti è ancora una variabile gaussiana. Questa proprietà non è generalmente vera per altre distribuzioni.
</p>

<p>
<b>Standardizzazione:</b>
Se \(X \sim \mathcal{G}(\mu, \sigma)\), allora la trasformazione:
\[Z = \frac{X-\mu}{\sigma}\]
produce una variabile aleatoria \(Z \sim \mathcal{G}(0, 1)\), chiamata normale standard.
</p>

<p>
<b>Proprietà principali:</b>
</p>
<ul class="org-ul">
<li><b>Supporto:</b> \(X \in (-\infty, \infty)\)</li>
<li><b>Valore atteso:</b> \(E[X] = \mu\)</li>
<li><b>Varianza:</b> \(Var(X) = \sigma^2\)</li>
</ul>

<p>
La funzione di ripartizione non ha una forma chiusa esprimibile in termini di funzioni elementari, ma si può scrivere come:
\[F_X(x) = \Phi\left(\frac{x-\mu}{\sigma}\right)\]
dove \(\Phi\) è la funzione di ripartizione della distribuzione normale standard \(\mathcal{G}(0,1)\).
</p>

<p>
<b>Proprietà di simmetria della funzione di ripartizione:</b>
Per la normale standard, vale la relazione:
\[\Phi(-x) = 1 - \Phi(x)\]
</p>

<p>
Questa proprietà deriva dalla simmetria della densità gaussiana rispetto all'origine.
</p>

<p>
<b>Calcolo degli intervalli di confidenza e relazione con la regola empirica:</b>
</p>

<p>
Possiamo trasformare un campione statistico in una variabile aleatoria e porci domande in funzione della sua probabilità. Ad esempio, possiamo chiederci qual è la probabilità che una variabile gaussiana si trovi entro \(n\) deviazioni standard dalla media:
\[P(|X-\mu| \leq n\sigma)\]
</p>

<p>
Questo ci indica quale percentuale della popolazione si trova nell'intervallo \([\mu-n\sigma, \mu+n\sigma]\).
</p>

<p>
Standardizzando la variabile:
\[P(|X-\mu| \leq n\sigma) = P\left(\left|\frac{X-\mu}{\sigma}\right| \leq n\right) = P(-n \leq Z \leq n)\]
</p>

<p>
dove \(Z\) è una normale standard. Quindi:
\[P(-n \leq Z \leq n) = \Phi(n) - \Phi(-n) = \Phi(n) - (1 - \Phi(n)) = 2\Phi(n) - 1\]
</p>

<p>
Calcolando questo valore per \(n = 1, 2, 3\), otteniamo i risultati della regola empirica:
</p>
<ul class="org-ul">
<li>Per \(n = 1\): \(2\Phi(1) - 1 \approx 0.68\) (68% dei valori)</li>
<li>Per \(n = 2\): \(2\Phi(2) - 1 \approx 0.95\) (95% dei valori)</li>
<li>Per \(n = 3\): \(2\Phi(3) - 1 \approx 0.997\) (99.7% dei valori)</li>
</ul>

<p>
<b>Regola empirica (regola 68-95-99.7):</b> Per una variabile aleatoria normale:
</p>
<ul class="org-ul">
<li>Circa il 68% dei valori si trova entro 1 <a href="#org6db3d4f">deviazione standard</a> dalla media</li>
<li>Circa il 95% dei valori si trova entro 2 deviazioni standard dalla media</li>
<li>Circa il 99.7% dei valori si trova entro 3 deviazioni standard dalla media</li>
</ul>

<p>
<b>Legame con la distribuzione binomiale</b>: dato che
$X&sim; B(n,p) = &sum;<sub>i=1</sub><sup>n</sup> (X<sub>i&sim;</sub> B(p)) \dot&sim; N(np,\sqrt{np(1-p)}) e in particolare frac{X-np}{sqrt(np(1-p) approx N(0,1)
Quindi, se ho a che fare con binomiali abbastanza grandi, posso approssimare con una gaussiana in virtù del TCL, evitando una lunga catena di addendi contenenti binomiali.
Potrei anche approssimare con Poisson ma solo quando np rimane costante.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a020f0;">import</span> numpy <span style="color: #a020f0;">as</span> np
<span style="color: #a020f0;">import</span> matplotlib.pyplot <span style="color: #a020f0;">as</span> plt
<span style="color: #a020f0;">import</span> scipy.stats <span style="color: #a020f0;">as</span> st

<span style="color: #b22222;"># </span><span style="color: #b22222;">Parametri della distribuzione gaussiana</span>
<span style="color: #a0522d;">mu</span> = 0      <span style="color: #b22222;"># </span><span style="color: #b22222;">media</span>
<span style="color: #a0522d;">sigma</span> = 1   <span style="color: #b22222;"># </span><span style="color: #b22222;">deviazione standard</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Creazione dell'oggetto distribuzione gaussiana</span>
<span style="color: #a0522d;">norm_dist</span> = st.norm(loc=mu, scale=sigma)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo della PDF e CDF per diversi valori</span>
<span style="color: #a0522d;">x_values</span> = np.linspace(mu-4*sigma, mu+4*sigma, 1000)
<span style="color: #a0522d;">pdf_values</span> = norm_dist.pdf(x_values)
<span style="color: #a0522d;">cdf_values</span> = norm_dist.cdf(x_values)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Calcolo di probabilit&#224; per la regola empirica</span>
<span style="color: #a0522d;">p_1sigma</span> = 2 * norm_dist.cdf(mu + sigma) - 1
<span style="color: #a0522d;">p_2sigma</span> = 2 * norm_dist.cdf(mu + 2*sigma) - 1
<span style="color: #a0522d;">p_3sigma</span> = 2 * norm_dist.cdf(mu + 3*sigma) - 1

<span style="color: #483d8b;">print</span>(<span style="color: #8b2252;">"Regola empirica (68-95-99.7):"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(&#956;-&#963; &lt; X &lt; &#956;+&#963;) = </span>{p_1sigma:.6f}<span style="color: #8b2252;"> &#8776; 68.27%"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(&#956;-2&#963; &lt; X &lt; &#956;+2&#963;) = </span>{p_2sigma:.6f}<span style="color: #8b2252;"> &#8776; 95.45%"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"P(&#956;-3&#963; &lt; X &lt; &#956;+3&#963;) = </span>{p_3sigma:.6f}<span style="color: #8b2252;"> &#8776; 99.73%"</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Visualizzazione della distribuzione gaussiana</span>
plt.figure(figsize=(12, 5))

<span style="color: #b22222;"># </span><span style="color: #b22222;">Grafico della PDF</span>
plt.subplot(1, 2, 1)
plt.plot(x_values, pdf_values, <span style="color: #8b2252;">'b-'</span>, label=<span style="color: #8b2252;">'PDF'</span>)
plt.axvline(x=mean, color=<span style="color: #8b2252;">'r'</span>, linestyle=<span style="color: #8b2252;">'--'</span>, label=f<span style="color: #8b2252;">'Media (&#956;) = </span>{mu}<span style="color: #8b2252;">'</span>)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Aggiungiamo le aree per la regola empirica</span>
plt.fill_between(x_values[(x_values &gt;= mu-sigma) &amp; (x_values &lt;= mu+sigma)], 
                 pdf_values[(x_values &gt;= mu-sigma) &amp; (x_values &lt;= mu+sigma)], 
                 alpha=0.3, color=<span style="color: #8b2252;">'green'</span>, label=<span style="color: #8b2252;">'68.27% (&#177;1&#963;)'</span>)
plt.fill_between(x_values[(x_values &gt;= mu-2*sigma) &amp; (x_values &lt;= mu-sigma) | 
                          (x_values &gt;= mu+sigma) &amp; (x_values &lt;= mu+2*sigma)], 
                 pdf_values[(x_values &gt;= mu-2*sigma) &amp; (x_values &lt;= mu-sigma) | 
                            (x_values &gt;= mu+sigma) &amp; (x_values &lt;= mu+2*sigma)], 
                 alpha=0.2, color=<span style="color: #8b2252;">'blue'</span>, label=<span style="color: #8b2252;">'27.18% (&#177;2&#963;)'</span>)
plt.fill_between(x_values[(x_values &gt;= mu-3*sigma) &amp; (x_values &lt;= mu-2*sigma) | 
                          (x_values &gt;= mu+2*sigma) &amp; (x_values &lt;= mu+3*sigma)], 
                 pdf_values[(x_values &gt;= mu-3*sigma) &amp; (x_values &lt;= mu-2*sigma) | 
                            (x_values &gt;= mu+2*sigma) &amp; (x_values &lt;= mu+3*sigma)], 
                 alpha=0.1, color=<span style="color: #8b2252;">'purple'</span>, label=<span style="color: #8b2252;">'4.28% (&#177;3&#963;)'</span>)

plt.axvline(x=mu-sigma, color=<span style="color: #8b2252;">'green'</span>, linestyle=<span style="color: #8b2252;">':'</span>, label=<span style="color: #8b2252;">'&#956;&#177;&#963;'</span>)
plt.axvline(x=mu+sigma, color=<span style="color: #8b2252;">'green'</span>, linestyle=<span style="color: #8b2252;">':'</span>)
plt.xlabel(<span style="color: #8b2252;">'x'</span>)
plt.ylabel(<span style="color: #8b2252;">'f(x)'</span>)
plt.title(f<span style="color: #8b2252;">'PDF - Gaussiana con &#956;=</span>{mu}<span style="color: #8b2252;">, &#963;=</span>{sigma}<span style="color: #8b2252;">'</span>)
plt.grid(alpha=0.3)
plt.legend()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Grafico della CDF</span>
plt.subplot(1, 2, 2)
plt.plot(x_values, cdf_values, <span style="color: #8b2252;">'g-'</span>, label=<span style="color: #8b2252;">'CDF'</span>)
plt.axvline(x=mean, color=<span style="color: #8b2252;">'r'</span>, linestyle=<span style="color: #8b2252;">'--'</span>, label=f<span style="color: #8b2252;">'Media (&#956;) = </span>{mu}<span style="color: #8b2252;">'</span>)
plt.axhline(y=0.5, color=<span style="color: #8b2252;">'r'</span>, linestyle=<span style="color: #8b2252;">':'</span>)
plt.axvline(x=mu-sigma, color=<span style="color: #8b2252;">'green'</span>, linestyle=<span style="color: #8b2252;">':'</span>, label=<span style="color: #8b2252;">'&#956;&#177;&#963;'</span>)
plt.axvline(x=mu+sigma, color=<span style="color: #8b2252;">'green'</span>, linestyle=<span style="color: #8b2252;">':'</span>)
plt.xlabel(<span style="color: #8b2252;">'x'</span>)
plt.ylabel(<span style="color: #8b2252;">'F(x)'</span>)
plt.title(f<span style="color: #8b2252;">'CDF - Gaussiana con &#956;=</span>{mu}<span style="color: #8b2252;">, &#963;=</span>{sigma}<span style="color: #8b2252;">'</span>)
plt.grid(alpha=0.3)
plt.legend()

plt.tight_layout()
plt.show()

<span style="color: #b22222;"># </span><span style="color: #b22222;">Esempio di somma di variabili gaussiane indipendenti</span>
<span style="color: #a0522d;">mu1</span>, <span style="color: #a0522d;">sigma1</span> = 0, 1
<span style="color: #a0522d;">mu2</span>, <span style="color: #a0522d;">sigma2</span> = 2, 1.5
<span style="color: #a0522d;">n_samples</span> = 1000

<span style="color: #b22222;"># </span><span style="color: #b22222;">Creazione delle distribuzioni</span>
<span style="color: #a0522d;">norm_X1</span> = st.norm(loc=mu1, scale=sigma1)
<span style="color: #a0522d;">norm_X2</span> = st.norm(loc=mu2, scale=sigma2)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Risultati teorici per la somma</span>
<span style="color: #a0522d;">mu_sum</span> = mu1 + mu2
<span style="color: #a0522d;">sigma_sum</span> = np.sqrt(sigma1**2 + sigma2**2)

<span style="color: #b22222;"># </span><span style="color: #b22222;">Generazione di campioni</span>
<span style="color: #a0522d;">sample_X1</span> = norm_X1.rvs(size=n_samples)
<span style="color: #a0522d;">sample_X2</span> = norm_X2.rvs(size=n_samples)
<span style="color: #a0522d;">sample_sum</span> = sample_X1 + sample_X2  <span style="color: #b22222;"># </span><span style="color: #b22222;">somma delle variabili</span>

<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"</span><span style="color: #008b8b;">\n</span><span style="color: #8b2252;">Somma di variabili gaussiane indipendenti:"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"X&#8321; ~ N(</span>{mu1}<span style="color: #8b2252;">, </span>{sigma1}<span style="color: #8b2252;">&#178;), X&#8322; ~ N(</span>{mu2}<span style="color: #8b2252;">, </span>{sigma2}<span style="color: #8b2252;">&#178;)"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Media della somma: </span>{sample_sum.mean():.4f}<span style="color: #8b2252;"> (teoricamente: </span>{mu_sum}<span style="color: #8b2252;">)"</span>)
<span style="color: #483d8b;">print</span>(f<span style="color: #8b2252;">"Deviazione standard della somma: </span>{sample_sum.std():.4f}<span style="color: #8b2252;"> (teoricamente: </span>{sigma_sum}<span style="color: #8b2252;">)"</span>)
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-org5f6d451" class="outline-3">
<h3 id="org5f6d451">Riproducibilità (da integrare)</h3>
<div class="outline-text-3" id="text-org5f6d451">
<p>
La riproducibilità è una proprietà fondamentale di alcune distribuzioni di probabilità che si mantengono "della stessa famiglia" sotto certe operazioni, come la somma di variabili aleatorie indipendenti.
</p>

<p>
## Definizione formale di riproducibilità per la binomiale
</p>

<p>
<b>Teorema (Riproducibilità della distribuzione binomiale):</b> Se \(X_1 \sim \text{Bin}(n_1, p)\) e \(X_2 \sim \text{Bin}(n_2, p)\) sono variabili aleatorie binomiali indipendenti con lo stesso parametro di probabilità \(p\), allora la loro somma segue ancora una distribuzione binomiale:
</p>

<p>
\[X_1 + X_2 \sim \text{Bin}(n_1 + n_2, p)\]
</p>

<p>
## Dimostrazione intuitiva
</p>

<p>
La dimostrazione concettuale di questa proprietà è abbastanza intuitiva:
</p>

<ol class="org-ol">
<li>\(X_1\) rappresenta il numero di successi in \(n_1\) prove indipendenti, ciascuna con probabilità \(p\)</li>
<li>\(X_2\) rappresenta il numero di successi in \(n_2\) prove indipendenti, ciascuna con probabilità \(p\)</li>
<li>Quindi \(X_1 + X_2\) rappresenta il numero totale di successi in \(n_1 + n_2\) prove indipendenti, ciascuna con probabilità \(p\)</li>
</ol>

<p>
Questo corrisponde esattamente alla definizione di una variabile binomiale \(\text{Bin}(n_1 + n_2, p)\).
</p>

<p>
## Dimostrazione formale
</p>

<p>
Possiamo anche dimostrarlo usando le funzioni generatrici di probabilità (PGF):
</p>
<ul class="org-ul">
<li>La PGF di \(X_1 \sim \text{Bin}(n_1, p)\) è \(G_{X_1}(t) = (1-p+pt)^{n_1}\)</li>
<li>La PGF di \(X_2 \sim \text{Bin}(n_2, p)\) è \(G_{X_2}(t) = (1-p+pt)^{n_2}\)</li>
<li>La PGF della somma di variabili indipendenti è il prodotto delle PGF:
\[G_{X_1+X_2}(t) = G_{X_1}(t) \cdot G_{X_2}(t) = (1-p+pt)^{n_1} \cdot (1-p+pt)^{n_2} = (1-p+pt)^{n_1+n_2}\]</li>
</ul>

<p>
Questa è la PGF di una \(\text{Bin}(n_1 + n_2, p)\).
</p>

<p>
## Condizioni necessarie e limitazioni
</p>

<p>
È importante notare che la riproducibilità della binomiale richiede due condizioni fondamentali:
</p>

<ol class="org-ol">
<li><b><b>Stesso parametro \(p\)</b></b>: Le variabili devono avere la stessa probabilità di successo</li>
<li><b><b>Indipendenza</b></b>: Le variabili aleatorie devono essere indipendenti</li>
</ol>

<p>
Se \(X_1 \sim \text{Bin}(n_1, p_1)\) e \(X_2 \sim \text{Bin}(n_2, p_2)\) con \(p_1 \neq p_2\), la loro somma <b><b>non</b></b> segue una distribuzione binomiale.
</p>

<p>
## Confronto con altre distribuzioni
</p>

<p>
La riproducibilità è una caratteristica che la binomiale condivide con altre distribuzioni importanti:
</p>

<ul class="org-ul">
<li><b><b>Gaussiana</b></b>: \(X_1 \sim \mathcal{G}(\mu_1, \sigma_1)\) e \(X_2 \sim \mathcal{G}(\mu_2, \sigma_2)\) indipendenti ⟹ \(X_1 + X_2 \sim \mathcal{G}(\mu_1 + \mu_2, \sqrt{\sigma_1^2 + \sigma_2^2})\)</li>
<li><b><b>Poisson</b></b>: \(X_1 \sim \text{Pois}(\lambda_1)\) e \(X_2 \sim \text{Pois}(\lambda_2)\) indipendenti ⟹ \(X_1 + X_2 \sim \text{Pois}(\lambda_1 + \lambda_2)\)</li>
<li><b><b>Gamma</b></b>: Con stesso parametro di scala</li>
</ul>

<p>
## Applicazioni pratiche
</p>

<p>
Questa proprietà ha importanti applicazioni pratiche:
</p>

<ol class="org-ol">
<li>Semplifica il calcolo di distribuzioni di variabili somma</li>
<li>Permette di analizzare processi composti da più fasi binomiali indipendenti</li>
<li>Facilita la modellazione di conteggi totali in esperimenti di gruppo</li>
</ol>

<p>
## Domande di approfondimento
</p>

<ol class="org-ol">
<li>Come cambierebbe il risultato se le variabili binomiali avessero parametri \(p\) diversi?</li>
<li>Quali altre distribuzioni di probabilità godono della proprietà di riproducibilità?</li>
<li>Come si collegano la riproducibilità della binomiale e il teorema del limite centrale?</li>
</ol>
</div>
</div>
<div id="outline-container-orgbd29f40" class="outline-3">
<h3 id="orgbd29f40">Teorema centrale del limite (da integrare)</h3>
<div class="outline-text-3" id="text-orgbd29f40">
<p>
Data una successione di \(n\) v.a. X<sub>1,&#x2026;,X</sub><sub>n</sub> i.i.d (ovvero essendo indipendenti e distribuite secondo lo stesso parametro), allora forall i vale che \(\mathcal{E}(X_i) = \mu\) e $Var(X<sub>i</sub>) = &sigma;<sup>2</sup>. Allora &sum;<sub>i</sub>=1<sup>n</sup> X<sub>i</sub> è approssimamente distribuito come una normale N(nmu, \sqrt(n) sigma). Standardizzando, ottengo &sum;<sub>i=1</sub><sup>n</sup> (\frac{X_i-n\mu}{\sqrt{n}&sigma; \dot\sym N(0,1)
</p>

<p>
Quando il numero \(n\) aumenta, ovvero lim n-&gt; infty che la probabilità che la var standardizzata sia controllata da un certo errore \(x\) allora ci si avvicina alla normale standard Phi (X)
</p>
</div>
</div>
<div id="outline-container-orge1dd96e" class="outline-3">
<h3 id="orge1dd96e">Assenza di memoria</h3>
</div>
</div>
<div id="outline-container-org117121d" class="outline-2">
<h2 id="org117121d">Statistica inferenziale</h2>
<div class="outline-text-2" id="text-org117121d">
<p>
Estrarre informazioni in senso <b>induttivo</b>.
Conosco il modello di una v.a. ma non conosco alcuni dei parametri.
I concetti fondamentali sono:
</p>
<dl class="org-dl">
<dt>popolazione</dt><dd>una v.a. \(X\sim F(\theta)\), con &theta; ignoto e &tau;(theta è quello che vogliamo stimare</dd>
<dt>campione</dt><dd>X<sub>1</sub>, &#x2026;, X<sub>n</sub> i.i.d fra loro e rispetto ad \(X\), con le lettere minuscole si intende i loro singoli valori calcolati</dd>
<dt>statistica (stimatore)</dt><dd>\(t:D_X^n -> R\), ovvero una funzione che prende una ennupla di \(n\) campionamenti e restituisce una stima</dd>
<dt>stima</dt><dd>t cappello = t(x<sub>1,&#x2026;,x</sub><sub>n</sub>) &asymp; &tau;(&theta;)</dd>
</dl>
</div>
<div id="outline-container-org8673dd5" class="outline-3">
<h3 id="org8673dd5">Stimatore non deviato</h3>
<div class="outline-text-3" id="text-org8673dd5">
<p>
Uno stimatore è detto non deviato per &tau;(&theta;) quando il valore atteso della statistica su un certo campione di \(n\) osservazioni è uguale al valore da stimare, ovvero
\[\mathcal{E}(t(X_1,...,X_n))=\tau(\theta)\]
</p>
</div>
<div id="outline-container-org43be099" class="outline-4">
<h4 id="org43be099">Media campionaria come stimatore</h4>
<div class="outline-text-4" id="text-org43be099">
<p>
Se considero lo stimatore \(\overline X = t(x_1, ...,x_n) = \frac{1}{n}\sum_i X_i\), ovvero quello che stima \(\tau(\theta)\) come la media campionaria delle osservazioni, allora questo è sempre <b>non deviato</b> per la stima del valore atteso di \(X\), indipendentemente dal modello di X. Infatti E(\frac{1}{n}&sum;<sub>i</sub> X<sub>i</sub>) = \frac{1}{n}&sum;<sub>i</sub> E(X<sub>i</sub>) usando la linearità di E = \frac{1}{n}&sum;<sub>i</sub> E(X) usando che X<sub>i</sub> i.i.d X = E(X).
Notare inoltre che \overline X è una v.a. e possiamo calcolarci anche la varianza:
Var(\overline X) = \frac{Var(x)}{n}, seguendo lo stesso procedimento del valore atteso ma tirando fuori un \(n^2\)
</p>

<p>
Si dimostra anche che non è possibile pesare i campioni in modo diverso per ottenere una stima più accurata, in quanto quella che minimizza il MSE è la scelta uniforme dei pesi
</p>
</div>
</div>
<div id="outline-container-orgc7bb3f2" class="outline-4">
<h4 id="orgc7bb3f2">Varianza campionaria come stimatore</h4>
<div class="outline-text-4" id="text-orgc7bb3f2">
<p>
Lo stimatore \(S^2 = \frac{1}{n-1}\sum_{i=1}^n(X_i-\overline X)^2\), questo è sempre non distorto per \(Var(X)\), come la media campionaria.
</p>

<p>
La dimostrazione procede in questo modo:
</p>
<ul class="org-ul">
<li>sviluppa il binomio della definizione e trasforma \(\sum X_i\) in \(n\overline X\).</li>
<li>porta \((n-1)\) a sinistra e calcola il valore atteso dei due membri</li>
<li>nel calcolo del valore atteso, trasformare ogni caso di \(\mathcal{E}(X^2\) usando la formula della varianza.</li>
<li>considerare anche che la varianza di \(\overline X\) è \(\frac{\sigma^2}{n}\) e che il suo valore atteso è \(\mu\)</li>
<li>i \(\mu\) scompaiono, rimangono solo i \(\sigma\)</li>
<li>dividere entrambi i membri per \((n-1)\)</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orga719f4c" class="outline-3">
<h3 id="orga719f4c">Errore quadratico medio</h3>
<div class="outline-text-3" id="text-orga719f4c">
<p>
Per la stessa quantità &tau;(&theta;) possono esistere vari stimatori non deviati. L'errore quadratico medio serve per capire quanto è buono uno stimatore, calcolando il valore atteso dell'errore commesso dallo stimatore, ovvero la distanza fra esso e il valore che vuole stimare.
MSE<sub>&tau;(&theta;)</sub>(T)= E((T-&tau;(&theta;))<sup>2</sup>), dove \(T=t(X_1,...,X_n\) è uno stimatore.
</p>

<p>
Espandendo nella definizione il quadrato di binomio e applicando la linearità del valore atteso, si ottiene anche che $$MSE<sub>&tau;(&theta;)</sub>(T) = Var(T)+(E(T)-&tau;(&theta;))<sup>2</sup>.
Il secondo addendo è chiamato <b>bias</b>, \(b_{\tau(\theta)=E(T)-\tau(\theta)\)
</p>

<p>
In uno stimatore non deviato ,b<sub>&tau;(&theta;)</sub>=0
</p>
</div>
</div>
<div id="outline-container-orge69270b" class="outline-3">
<h3 id="orge69270b">Consistenza in media quadratica</h3>
<div class="outline-text-3" id="text-orge69270b">
<p>
Uno stimatore T è consistente in media quadratica per &tau;(&theta;) se lim<sub>n-&gt;&infin;</sub> MSE<sub>&tau;(&theta;)</sub>(T) = 0
</p>

<p>
Va specificato che in realtà lo stimatore è una funzione diversa per ogni valore di \(n\), si parla quindi di una famiglia di stimatori {T<sub>n</sub>} e qui si studia come si comporta lo stimatore di questa famiglia con n molto alto
</p>
</div>
</div>
<div id="outline-container-orgf4bb999" class="outline-3">
<h3 id="orgf4bb999">Consistenza debole</h3>
<div class="outline-text-3" id="text-orgf4bb999">
<p>
Un altro modo di vedere la "precisione" di uno stimatore rispetto alla media quadratica.
Uno stimatore T è consistente in media quadratica per &tau;(&theta;) se &forall; &epsilon; &gt; 0 lim<sub>n-&gt;&infin;</sub> P(&tau;(&theta;)-&epsilon; &le; T<sub>n</sub> &le; &tau;(&theta;)+&epsilon; = 1
</p>

<p>
La consistenza in media quadratica implica quella debole. La dimostrazione consiste nel
</p>
<ul class="org-ul">
<li>trasformare la probabilità nella definizione in P(|T<sub>n</sub>-&tau;(&theta;)|&le; e)</li>
<li>elevare al quadrato.</li>
<li>applicare la disuguaglianza di Markov, prima complementando a 1</li>
<li>ottenere una formula che contiene MSE che per ipotesi tende a 0</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgb225ee8" class="outline-2">
<h2 id="orgb225ee8"><span class="todo TODO">TODO</span> Creare una panoramica con tutti i grafici delle distribuzioni che abbiamo guardato fatte in python</h2>
</div>
<div id="outline-container-org7b7f9ea" class="outline-2">
<h2 id="org7b7f9ea"><span class="todo TODO">TODO</span> Creare boxplot di tutte le distribuzioni in python</h2>
</div>
<div id="outline-container-org4e12c02" class="outline-2">
<h2 id="org4e12c02"><span class="todo TODO">TODO</span> Creare ANKI usando copilot</h2>
</div>
<div id="outline-container-org98baca8" class="outline-2">
<h2 id="org98baca8"><span class="todo TODO">TODO</span> grafici interattivi (usare possibilmente all'esame)</h2>
</div>
<div id="outline-container-org9e2228d" class="outline-2">
<h2 id="org9e2228d"><span class="todo TODO">TODO</span> mathcal per E</h2>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Created: 2025-06-08 Sun 22:00</p>
</div>
</body>
</html>
